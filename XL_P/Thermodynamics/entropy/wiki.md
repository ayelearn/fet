The following text has been accessed from https://en.wikipedia.org/wiki/Entropy at Fri Aug 9 02:58:56 IST 2019
Creative_Commons_Attribution-ShareAlike_License





















****** Entropy ******
From Wikipedia, the free encyclopedia
Jump_to_navigation Jump_to_search
This article is about entropy in thermodynamics. For other uses, see Entropy_
(disambiguation).
Not to be confused with Enthalpy.
For a more accessible and less technical introduction to this topic, see
Introduction_to_entropy.
Entropy
Common symbols   S
SI unit         joules per kelvin (JâKâ1)
In SI base_unitskgâm2âsâ2âKâ1
Thermodynamics
[Carnot_heat_engine_2.svg]
The classical Carnot_heat_engine
Branches
    * Classical
    * Statistical
    * Chemical
    * Quantum_thermodynamics
    * Equilibrium / Non-equilibrium
Laws
    * Zeroth
    * First
    * Second
    * Third
Systems
State
    * Equation_of_state
    * Ideal_gas
    * Real_gas
    * State_of_matter
    * Equilibrium
    * Control_volume
    * Instruments
Processes
    * Isobaric
    * Isochoric
    * Isothermal
    * Adiabatic
    * Isentropic
    * Isenthalpic
    * Quasistatic
    * Polytropic
    * Free_expansion
    * Reversibility
    * Irreversibility
    * Endoreversibility
Cycles
    * Heat_engines
    * Heat_pumps
    * Thermal_efficiency
System_properties
Note: Conjugate_variables in italics
    * Property_diagrams
    * Intensive_and_extensive_properties
Process_functions
    * Work
    * Heat
Functions_of_state
    * Temperature / Entropy (introduction)
    * Pressure / Volume
    * Chemical_potential / Particle_number
    * Vapor_quality
    * Reduced_properties
Material_properties
    * Property_databases
                                                     T              &#x2202; S
                                                  {\displaystyle {\displaystyle
                                                  T}  [T]        \partial S}
Specific_heat_capacity    c =   {\displaystyle                  [\partial S]
                        c=}  [c=]                    N              &#x2202; T
                                                  {\displaystyle {\displaystyle
                                                  N}  [N]        \partial T}
                                                                 [\partial T]
                                                     1              &#x2202; V
                                                  {\displaystyle {\displaystyle
                           &#x03B2; = &#x2212;    1}  [1]        \partial V}
Compressibility        {\displaystyle \beta =-                  [\partial V]
                        }  [\beta =-]                V              &#x2202; p
                                                  {\displaystyle {\displaystyle
                                                  V}  [V]        \partial p}
                                                                 [\partial p]
                                                     1              &#x2202; V
                                                  {\displaystyle {\displaystyle
                           &#x03B1; =             1}  [1]        \partial V}
Thermal_expansion      {\displaystyle \alpha =}                 [\partial V]
                        [\alpha =]                   V              &#x2202; T
                                                  {\displaystyle {\displaystyle
                                                  V}  [V]        \partial T}
                                                                 [\partial T]
Equations
    * Carnot's_theorem
    * Clausius_theorem
    * Fundamental_relation
    * Ideal_gas_law
    * Maxwell_relations
    * Onsager_reciprocal_relations
    * Bridgman's_equations
    * Table_of_thermodynamic_equations
Potentials
    * Free_energy
    * Free_entropy
    * Internal_energy
         U ( S , V )   {\displaystyle U(S,V)}  [U(S,V)]
    * Enthalpy
         H ( S , p ) = U + p V   {\displaystyle H(S,p)=U+pV}  [H(S,p)=U+pV]
    * Helmholtz_free_energy
         A ( T , V ) = U &#x2212; T S   {\displaystyle A(T,V)=U-TS}  [A(T,V)=U-
      TS]
    * Gibbs_free_energy
         G ( T , p ) = H &#x2212; T S   {\displaystyle G(T,p)=H-TS}  [G(T,p)=H-
      TS]
    * History
    * Culture
History
    * General
    * Entropy
    * Gas_laws
    * "Perpetual_motion"_machines
Philosophy
    * Entropy_and_time
    * Entropy_and_life
    * Brownian_ratchet
    * Maxwell's_demon
    * Heat_death_paradox
    * Loschmidt's_paradox
    * Synergetics
Theories
    * Caloric_theory
    * Theory_of_heat
    * Vis_viva_("living_force")
    * Mechanical_equivalent_of_heat
    * Motive_power
Key_publications
    * "An_Experimental_Enquiry
      Concerning_..._Heat"
    * "On_the_Equilibrium_of
      Heterogeneous_Substances"
    * "Reflections_on_the
      Motive_Power_of_Fire"
Timelines
    * Thermodynamics
    * Heat_engines
    * Art
    * Education
    * Maxwell's_thermodynamic_surface
    * Entropy_as_energy_dispersal
Scientists
    * Bernoulli
    * Boltzmann
    * Carnot
    * Clapeyron
    * Clausius
    * CarathÃ©odory
    * Duhem
    * Gibbs
    * von_Helmholtz
    * Joule
    * Maxwell
    * von_Mayer
    * Onsager
    * Rankine
    * Smeaton
    * Stahl
    * Thompson
    * Thomson
    * van_der_Waals
    * Waterston
    * [Wikipedia book] Book
    * [Category] Category
    * v
    * t
    * e
Entropy articles
    * Introduction
    * History
    * Classical_entropy
    * Statistical_entropy
    * v
    * t
    * e
In statistical_mechanics, entropy is an extensive_property of a thermodynamic
system. It is closely related to the number Î© of microscopic configurations
(known as microstates) that are consistent with the macroscopic quantities that
characterize the system (such as its volume, pressure and temperature). Under
the assumption that each microstate is equally probable, the entropy     S
{\displaystyle S}  [S] is the natural_logarithm of the number of microstates,
multiplied by the Boltzmann_constant kB. Formally (assuming equiprobable
microstates),
         S =  k   B    ln &#x2061; &#x03A9; .   {\displaystyle S=k_{\mathrm {B}
      }\ln \Omega .}  [{\displaystyle S=k_{\mathrm {B} }\ln \Omega .}]
Macroscopic systems typically have a very large number Î© of possible
microscopic configurations. For example, the entropy of an ideal_gas is
proportional to the number of gas molecules N. The number of molecules in
twenty liters of gas at room_temperature_and_atmospheric_pressure is roughly N
â 6Ã1023 (the Avogadro_number). At equilibrium, each of the Î© â eN
configurations can be regarded as random and equally likely.[citation_needed]
The second_law_of_thermodynamics states that the entropy of an isolated system
never decreases over time. Such systems spontaneously evolve towards
thermodynamic_equilibrium, the state with maximum entropy. Non-isolated systems
may lose entropy, provided their environment's entropy increases by at least
that amount so that the total entropy increases. Entropy is a function of the
state_of_the_system, so the change in entropy of a system is determined by its
initial and final states. In the idealization that a process is reversible, the
entropy does not change, while irreversible processes always increase the total
entropy.
Because it is determined by the number of random microstates, entropy is
related to the amount of additional information needed to specify the exact
physical state of a system, given its macroscopic specification. For this
reason, it is often said that entropy is an expression of the disorder, or
randomness of a system, or of the lack of information about it[citation
needed]. The concept of entropy plays a central role in information_theory.
Boltzmann's constant, and therefore entropy, have dimensions of energy divided
by temperature, which has a unit of joules per kelvin (JâKâ1) in the
International_System_of_Units (or kgâm2âsâ2âKâ1 in terms of base
units). The entropy of a substance is usually given as an intensive_property –
either entropy per unit mass (SI unit: JâKâ1âkgâ1) or entropy per unit
amount_of_substance (SI unit: JâKâ1âmolâ1).
⁰
***** Contents *****
    * 1_History
    * 2_Definitions_and_descriptions
          o 2.1_Function_of_state
          o 2.2_Reversible_process
          o 2.3_Carnot_cycle
          o 2.4_Classical_thermodynamics
          o 2.5_Statistical_mechanics
          o 2.6_Entropy_of_a_system
          o 2.7_Equivalence_of_definitions
    * 3_Second_law_of_thermodynamics
    * 4_Applications
          o 4.1_The_fundamental_thermodynamic_relation
          o 4.2_Entropy_in_chemical_thermodynamics
          o 4.3_Entropy_balance_equation_for_open_systems
    * 5_Entropy_change_formulas_for_simple_processes
          o 5.1_Isothermal_expansion_or_compression_of_an_ideal_gas
          o 5.2_Cooling_and_heating
          o 5.3_Phase_transitions
    * 6_Approaches_to_understanding_entropy
          o 6.1_Standard_textbook_definitions
          o 6.2_Order_and_disorder
          o 6.3_Energy_dispersal
          o 6.4_Relating_entropy_to_energy_usefulness
          o 6.5_Entropy_and_adiabatic_accessibility
          o 6.6_Entropy_in_quantum_mechanics
          o 6.7_Information_theory
          o 6.8_Experimental_measurement_of_entropy
    * 7_Interdisciplinary_applications_of_entropy
          o 7.1_Thermodynamic_and_statistical_mechanics_concepts
          o 7.2_The_arrow_of_time
          o 7.3_Entropy_in_DNA_sequences
          o 7.4_Cosmology
          o 7.5_Economics
          o 7.6_Hermeneutics
    * 8_See_also
    * 9_Notes
    * 10_References
    * 11_Further_reading
    * 12_External_links
***** History[edit] *****
Rudolf_Clausius (1822â1888), originator of the concept of entropy
Main article: History_of_entropy
The French mathematician Lazare_Carnot proposed in his 1803 paper Fundamental
Principles of Equilibrium and Movement that in any machine the accelerations
and shocks of the moving parts represent losses of moment of activity. In other
words, in any natural process there exists an inherent tendency towards the
dissipation of useful energy. Building on this work, in 1824 Lazare's son Sadi
Carnot published Reflections_on_the_Motive_Power_of_Fire which posited that in
all heat-engines, whenever "caloric" (what is now known as heat) falls through
a temperature difference, work or motive_power can be produced from the actions
of its fall from a hot to cold body. He made the analogy with that of how water
falls in a water_wheel. This was an early insight into the second_law_of
thermodynamics.[1] Carnot based his views of heat partially on the early 18th
century "Newtonian hypothesis" that both heat and light were types of
indestructible forms of matter, which are attracted and repelled by other
matter, and partially on the contemporary views of Count_Rumford who showed
(1789) that heat could be created by friction as when cannon bores are
machined.[2] Carnot reasoned that if the body of the working substance, such as
a body of steam, is returned to its original state at the end of a complete
engine_cycle, that "no change occurs in the condition of the working body".
The first_law_of_thermodynamics, deduced from the heat-friction experiments of
James_Joule in 1843, expresses the concept of energy, and its conservation in
all processes; the first law, however, is unable to quantify the effects of
friction and dissipation.
In the 1850s and 1860s, German physicist Rudolf_Clausius objected to the
supposition that no change occurs in the working body, and gave this "change" a
mathematical interpretation by questioning the nature of the inherent loss of
usable heat when work is done, e.g. heat produced by friction.[3] Clausius
described entropy as the transformation-content, i.e. dissipative energy use,
of a thermodynamic_system or working_body of chemical_species during a change
of state.[3] This was in contrast to earlier views, based on the theories of
Isaac_Newton, that heat was an indestructible particle that had mass.
Later, scientists such as Ludwig_Boltzmann, Josiah_Willard_Gibbs, and James
Clerk_Maxwell gave entropy a statistical basis. In 1877 Boltzmann visualized a
probabilistic way to measure the entropy of an ensemble of ideal_gas particles,
in which he defined entropy to be proportional to the natural logarithm of the
number of microstates such a gas could occupy. Henceforth, the essential
problem in statistical_thermodynamics has been to determine the distribution of
a given amount of energy E over N identical systems. CarathÃ©odory linked
entropy with a mathematical definition of irreversibility, in terms of
trajectories and integrability.
***** Definitions and descriptions[edit] *****
Any method involving the notion of entropy, the very existence of which depends
on the second law of thermodynamics, will doubtless seem to many far-fetched,
and may repel beginners as obscure and difficult of comprehension.
Willard_Gibbs, Graphical Methods in the Thermodynamics of Fluids[4]
There are two equivalent definitions of entropy: the thermodynamic definition
and the statistical_mechanics definition. Historically, the classical
thermodynamics definition developed first. In the classical_thermodynamics
viewpoint, the microscopic details of a system are not considered. Instead, the
behavior of a system is described in terms of a set of empirically defined
thermodynamic variables, such as temperature, pressure, entropy, and heat
capacity. The classical thermodynamics description assumes a state of
equilibrium, although more recently attempts have been made to develop useful
definitions of entropy in nonequilibrium systems as well.
The statistical_definition of entropy and other thermodynamic properties were
developed later. In this viewpoint, thermodynamic properties are defined in
terms of the statistics of the motions of the microscopic constituents of a
system â modeled at first classically, e.g. Newtonian particles constituting
a gas, and later quantum-mechanically (photons, phonons, spins, etc.).
**** Function of state[edit] ****
There are many thermodynamic_properties that are functions_of_state. This means
that at a particular thermodynamic state (which should not be confused with the
microscopic state of a system), these properties have a certain value. Often,
if two properties of the system are determined, then the state is determined
and the other properties' values can also be determined. For instance, a
quantity of gas at a particular temperature and pressure has its state fixed by
those values and thus has a specific volume that is determined by those values.
As another instance, a system composed of a pure substance of a single phase at
a particular uniform temperature and pressure is determined (and is thus a
particular state) and is at not only a particular volume but also at a
particular entropy.[5] The fact that entropy is a function of state is one
reason it is useful. In the Carnot cycle, the working fluid returns to the same
state it had at the start of the cycle, hence the line_integral of any state
function, such as entropy, over this reversible cycle is zero.
**** Reversible process[edit] ****
Entropy is conserved for a reversible_process. A reversible process is one that
does not deviate from thermodynamic equilibrium, while producing the maximum
work. Any process which happens quickly enough to deviate from thermal
equilibrium cannot be reversible. In these cases energy is lost to heat, total
entropy increases, and the potential for maximum work to be done in the
transition is also lost. More specifically, total entropy is conserved in a
reversible process and not conserved in an irreversible process.[6] For
example, in the Carnot cycle, while the heat flow from the hot reservoir to the
cold reservoir represents an increase in entropy, the work output, if
reversibly and perfectly stored in some energy storage mechanism, represents a
decrease in entropy that could be used to operate the heat engine in reverse
and return to the previous state, thus the total entropy change is still zero
at all times if the entire process is reversible. An irreversible process
increases entropy.[7]
**** Carnot cycle[edit] ****
The concept of entropy arose from Rudolf_Clausius's study of the Carnot_cycle.
[8] In a Carnot cycle, heat QH is absorbed isothermally at temperature TH from
a 'hot' reservoir and given up isothermally as heat QC to a 'cold' reservoir at
TC. According to Carnot's principle, work can only be produced by the system
when there is a temperature difference, and the work should be some function of
the difference in temperature and the heat absorbed (QH). Carnot did not
distinguish between QH and QC, since he was using the incorrect hypothesis that
caloric_theory was valid, and hence heat was conserved (the incorrect
assumption that QH and QC were equal) when, in fact, QH is greater than QC.[9]
[10] Through the efforts of Clausius and Kelvin, it is now known that the
maximum work that a heat engine can produce is the product of the Carnot
efficiency and the heat absorbed from the hot reservoir:
         W =  (     T  H   &#x2212;  T  C     T  H     )   Q  H   =
      (  1 &#x2212;    T  C    T  H      )   Q  H     {\displaystyle
      W=\left({\frac {T_{\text{H}}-T_{\text{C}}}{T_{\text
      {H}}}}\right)Q_{\text{H}}=\left(1-{\frac {T_{\text{C}}}{T_         (1)
      {\text{H}}}}\right)Q_{\text{H}}}  [{\displaystyle W=\left(         
      {\frac {T_{\text{H}}-T_{\text{C}}}{T_{\text{H}}}}\right)Q_
      {\text{H}}=\left(1-{\frac {T_{\text{C}}}{T_{\text
      {H}}}}\right)Q_{\text{H}}}]
To derive the Carnot efficiency, which is 1 â TC/TH (a number less than one),
Kelvin had to evaluate the ratio of the work output to the heat absorbed during
the isothermal expansion with the help of the CarnotâClapeyron equation which
contained an unknown function, known as the Carnot function. The possibility
that the Carnot function could be the temperature as measured from a zero
temperature, was suggested by Joule in a letter to Kelvin. This allowed Kelvin
to establish his absolute temperature scale.[11] It is also known that the work
produced by the system is the difference between the heat absorbed from the hot
reservoir and the heat given up to the cold reservoir:
         W =  Q  H   &#x2212;  Q  C     {\displaystyle W=Q_{\text    
      {H}}-Q_{\text{C}}}  [{\displaystyle W=Q_{\text{H}}-Q_{\text      (2)
      {C}}}]
Since the latter is valid over the entire cycle, this gave Clausius the hint
that at each stage of the cycle, work and heat would not be equal, but rather
their difference would be a state function that would vanish upon completion of
the cycle. The state function was called the internal energy and it became the
first_law_of_thermodynamics.[12]
Now equating (1) and (2) gives
            Q  H    T  H     &#x2212;    Q  C    T  C     = 0   {\displaystyle
      {\frac {Q_{\text{H}}}{T_{\text{H}}}}-{\frac {Q_{\text{C}}}{T_{\text
      {C}}}}=0}  [{\displaystyle {\frac {Q_{\text{H}}}{T_{\text{H}}}}-{\frac
      {Q_{\text{C}}}{T_{\text{C}}}}=0}]
or
            Q  H    T  H     =    Q  C    T  C       {\displaystyle {\frac {Q_
      {\text{H}}}{T_{\text{H}}}}={\frac {Q_{\text{C}}}{T_{\text{C}}}}}  [
      {\displaystyle {\frac {Q_{\text{H}}}{T_{\text{H}}}}={\frac {Q_{\text{C}}}
      {T_{\text{C}}}}}]
This implies that there is a function of state which is conserved over a
complete cycle of the Carnot cycle. Clausius called this state function
entropy. One can see that entropy was discovered through mathematics rather
than through laboratory results. It is a mathematical construct and has no easy
physical analogy. This makes the concept somewhat obscure or abstract, akin to
how the concept of energy arose.
Clausius then asked what would happen if there should be less work produced by
the system than that predicted by Carnot's principle. The right-hand side of
the first equation would be the upper bound of the work output by the system,
which would now be converted into an inequality
         W <  (  1 &#x2212;    T  C    T  H      )   Q  H     {\displaystyle
      W<\left(1-{\frac {T_{\text{C}}}{T_{\text{H}}}}\right)Q_{\text{H}}}  [
      {\displaystyle W<\left(1-{\frac {T_{\text{C}}}{T_{\text{H}}}}\right)Q_
      {\text{H}}}]
When the second equation is used to express the work as a difference in heats,
we get
          Q  H   &#x2212;  Q  C   <  (  1 &#x2212;    T  C    T  H      )   Q
      H     {\displaystyle Q_{\text{H}}-Q_{\text{C}}<\left(1-{\frac {T_{\text
      {C}}}{T_{\text{H}}}}\right)Q_{\text{H}}}  [{\displaystyle Q_{\text{H}}-Q_
      {\text{C}}<\left(1-{\frac {T_{\text{C}}}{T_{\text{H}}}}\right)Q_{\text
      {H}}}]
      or
          Q  C   >    T  C    T  H      Q  H     {\displaystyle Q_{\text{C}}>
      {\frac {T_{\text{C}}}{T_{\text{H}}}}Q_{\text{H}}}  [{\displaystyle Q_
      {\text{C}}>{\frac {T_{\text{C}}}{T_{\text{H}}}}Q_{\text{H}}}]
So more heat is given up to the cold reservoir than in the Carnot cycle. If we
denote the entropies by Si = Qi/Ti for the two states, then the above
inequality can be written as a decrease in the entropy
          S  H   &#x2212;  S  C   < 0   {\displaystyle S_{\text{H}}-S_{\text
      {C}}<0}  [{\displaystyle S_{\text{H}}-S_{\text{C}}<0}]
      or
          S  H   <  S  C     {\displaystyle S_{\text{H}}<S_{\text{C}}}  [
      {\displaystyle S_{\text{H}}<S_{\text{C}}}]
The entropy that leaves the system is greater than the entropy that enters the
system, implying that some irreversible process prevents the cycle from
producing the maximum amount of work predicted by the Carnot equation.
The Carnot cycle and efficiency are useful because they define the upper bound
of the possible work output and the efficiency of any classical thermodynamic
system. Other cycles, such as the Otto_cycle, Diesel_cycle and Brayton_cycle,
can be analyzed from the standpoint of the Carnot cycle. Any machine or process
that converts heat to work and is claimed to produce an efficiency greater than
the Carnot efficiency is not viable because it violates the second law of
thermodynamics. For very small numbers of particles in the system, statistical
thermodynamics must be used. The efficiency of devices such as photovoltaic
cells requires an analysis from the standpoint of quantum mechanics.
**** Classical thermodynamics[edit] ****
Main article: Entropy_(classical_thermodynamics)
Conjugate_variables
of_thermodynamics
Pressure           Volume
(Stress)           (Strain)
Temperature        Entropy
Chemical_potential Particle_number
The thermodynamic definition of entropy was developed in the early 1850s by
Rudolf_Clausius and essentially describes how to measure the entropy of an
isolated_system in thermodynamic_equilibrium with its parts. Clausius created
the term entropy as an extensive thermodynamic variable that was shown to be
useful in characterizing the Carnot_cycle. Heat transfer along the isotherm
steps of the Carnot cycle was found to be proportional to the temperature of a
system (known as its absolute_temperature). This relationship was expressed in
increments of entropy equal to the ratio of incremental heat transfer divided
by temperature, which was found to vary in the thermodynamic cycle but
eventually return to the same value at the end of every cycle. Thus it was
found to be a function_of_state, specifically a thermodynamic state of the
system.
While Clausius based his definition on a reversible process, there are also
irreversible processes that change entropy. Following the second_law_of
thermodynamics, entropy of an isolated system always increases for irreversible
processes. The difference between an isolated system and closed system is that
heat may not flow to and from an isolated system, but heat flow to and from a
closed system is possible. Nevertheless, for both closed and isolated systems,
and indeed, also in open systems, irreversible thermodynamics processes may
occur.
According to the Clausius_equality, for a reversible cyclic process:
&#x222E;      &#x2061;    &#x03B4;  Q  rev    T   = 0.   {\displaystyle \oint
{\frac {\delta Q_{\text{rev}}}{T}}=0.}  [\oint {\frac {\delta Q_{\text{rev}}}
{T}}=0.] This means the line integral      &#x222B;  L      &#x03B4;  Q  rev
T     {\displaystyle \int _{L}{\frac {\delta Q_{\text{rev}}}{T}}}  [\int _{L}
{\frac {\delta Q_{\text{rev}}}{T}}] is path-independent.
So we can define a state function S called entropy, which satisfies     d S =
&#x03B4;  Q  rev    T   .   {\displaystyle dS={\frac {\delta Q_{\text{rev}}}
{T}}.}  [dS={\frac {\delta Q_{\text{rev}}}{T}}.]
Clausius coined the name entropy (Entropie) for S in 1865. He gives
"transformational content" (Verwandlungsinhalt) as a synonym, paralleling his
"thermal and ergonal content" (WÃ¤rme- und Werkinhalt) as the name of U, but
preferring the term entropy as a close parallel of energy, formed by replacing
the root of á¼ÏÎ³Î¿Î½ "work" by that of ÏÏÎ¿ÏÎ® "transformation".[13]
To find the entropy difference between any two states of a system, the integral
must be evaluated for some reversible path between the initial and final
states.[14] Since entropy is a state function, the entropy change of the system
for an irreversible path is the same as for a reversible path between the same
two states.[15] However, the entropy change of the surroundings will be
different.
We can only obtain the change of entropy by integrating the above formula. To
obtain the absolute value of the entropy, we need the third_law_of
thermodynamics, which states that S = 0 at absolute_zero for perfect crystals.
From a macroscopic perspective, in classical_thermodynamics the entropy is
interpreted as a state_function of a thermodynamic_system: that is, a property
depending only on the current state of the system, independent of how that
state came to be achieved. In any process where the system gives up energy ÎE,
and its entropy falls by ÎS, a quantity at least TR ÎS of that energy must be
given up to the system's surroundings as unusable heat (TR is the temperature
of the system's external surroundings). Otherwise the process cannot go
forward. In classical thermodynamics, the entropy of a system is defined only
if it is in thermodynamic_equilibrium.
**** Statistical mechanics[edit] ****
The statistical definition was developed by Ludwig_Boltzmann in the 1870s by
analyzing the statistical behavior of the microscopic components of the system.
Boltzmann showed that this definition of entropy was equivalent to the
thermodynamic entropy to within a constant factor which has since been known as
Boltzmann's_constant. In summary, the thermodynamic definition of entropy
provides the experimental definition of entropy, while the statistical
definition of entropy extends the concept, providing an explanation and a
deeper understanding of its nature.
The interpretation_of_entropy_in_statistical_mechanics is the measure of
uncertainty, or mixedupness in the phrase of Gibbs, which remains about a
system after its observable macroscopic properties, such as temperature,
pressure and volume, have been taken into account. For a given set of
macroscopic variables, the entropy measures the degree to which the probability
of the system is spread out over different possible microstates. In contrast to
the macrostate, which characterizes plainly observable average quantities, a
microstate specifies all molecular details about the system including the
position and velocity of every molecule. The more such states available to the
system with appreciable probability, the greater the entropy. In statistical
mechanics, entropy is a measure of the number of ways in which a system may be
arranged, often taken to be a measure of "disorder" (the higher the entropy,
the higher the disorder).[16][17][18] This definition describes the entropy as
being proportional to the natural logarithm of the number of possible
microscopic configurations of the individual atoms and molecules of the system
(microstates) which could give rise to the observed macroscopic state
(macrostate) of the system. The constant of proportionality is the Boltzmann
constant.
Specifically, entropy is a logarithmic measure of the number of states with
significant probability of being occupied:
         S = &#x2212;  k   B     &#x2211;  i    p  i   log &#x2061;  p  i   ,
      {\displaystyle S=-k_{\mathrm {B} }\sum _{i}p_{i}\log p_{i},}  [
      {\displaystyle S=-k_{\mathrm {B} }\sum _{i}p_{i}\log p_{i},}]
or, equivalently, the expected value of the_logarithm_of_the_probability that a
microstate will be occupied
         S = &#x2212;  k   B     E  i   &#x2061; ( log &#x2061;  p  i   )
      {\displaystyle S=-k_{\mathrm {B} }\operatorname {E} _{i}(\log p_{i})}  [
      {\displaystyle S=-k_{\mathrm {B} }\operatorname {E} _{i}(\log p_{i})}]
where kB is the Boltzmann_constant, equal to 1.38065Ã10â23 J/K. The
summation is over all the possible microstates of the system, and pi is the
probability that the system is in the i-th microstate.[19] This definition
assumes that the basis set of states has been picked so that there is no
information on their relative phases. In a different basis set, the more
general expression is
         S = &#x2212;  k   B    Tr &#x2061; (    &#x03C1; &#x005E;    log
      &#x2061; (    &#x03C1; &#x005E;    ) ) ,   {\displaystyle S=-k_{\mathrm
      {B} }\operatorname {Tr} ({\widehat {\rho }}\log({\widehat {\rho }})),}  [
      {\displaystyle S=-k_{\mathrm {B} }\operatorname {Tr} ({\widehat {\rho
      }}\log({\widehat {\rho }})),}]
where        &#x03C1; &#x005E;      {\displaystyle {\widehat {\rho }}}  [
{\widehat {\rho }}] is the density_matrix,     Tr   {\displaystyle
\operatorname {Tr} }  [\operatorname {Tr}] is trace and     log
{\displaystyle \log }  [\log ] is the matrix_logarithm. This density matrix
formulation is not needed in cases of thermal equilibrium so long as the basis
states are chosen to be energy eigenstates. For most practical purposes, this
can be taken as the fundamental definition of entropy since all other formulas
for S can be mathematically derived from it, but not vice versa.
In what has been called the fundamental assumption of statistical
thermodynamics or the_fundamental_postulate_in_statistical_mechanics, the
occupation of any microstate is assumed to be equally probable (i.e. pi = 1/Î©,
where Î© is the number of microstates); this assumption is usually justified
for an isolated system in equilibrium.[20] Then the previous equation reduces
to
         S =  k   B    log &#x2061; &#x03A9; .   {\displaystyle S=k_{\mathrm
      {B} }\log \Omega .}  [{\displaystyle S=k_{\mathrm {B} }\log \Omega .}]
In thermodynamics, such a system is one in which the volume, number of
molecules, and internal energy are fixed (the microcanonical_ensemble).
The most general interpretation of entropy is as a measure of our uncertainty
about a system. The equilibrium_state of a system maximizes the entropy because
we have lost all information about the initial conditions except for the
conserved variables; maximizing the entropy maximizes our ignorance about the
details of the system.[21] This uncertainty is not of the everyday subjective
kind, but rather the uncertainty inherent to the experimental method and
interpretative model.
The interpretative model has a central role in determining entropy. The
qualifier "for a given set of macroscopic variables" above has deep
implications: if two observers use different sets of macroscopic variables,
they see different entropies. For example, if observer A uses the variables U,
V and W, and observer B uses U, V, W, X, then, by changing X, observer B can
cause an effect that looks like a violation of the second law of thermodynamics
to observer A. In other words: the set of macroscopic variables one chooses
must include everything that may change in the experiment, otherwise one might
see decreasing entropy![22]
Entropy can be defined for any Markov_processes with reversible_dynamics and
the detailed_balance property.
In Boltzmann's 1896 Lectures on Gas Theory, he showed that this expression
gives a measure of entropy for systems of atoms and molecules in the gas phase,
thus providing a measure for the entropy of classical thermodynamics.
**** Entropy of a system[edit] ****
A thermodynamic_system
A temperatureâentropy_diagram for steam. The vertical axis represents uniform
temperature, and the horizontal axis represents specific entropy. Each dark
line on the graph represents constant pressure, and these form a mesh with
light gray lines of constant volume. (Dark-blue is liquid water, light-blue is
liquid-steam mixture, and faint-blue is steam. Grey-blue represents
supercritical liquid water.)
Entropy arises directly from the Carnot_cycle. It can also be described as the
reversible heat divided by temperature. Entropy is a fundamental function of
state.
In a thermodynamic_system, pressure, density, and temperature tend to become
uniform over time because the equilibrium_state has higher probability (more
possible combinations of microstates) than any other state.
As an example, for a glass of ice water in air at room_temperature, the
difference in temperature between a warm room (the surroundings) and cold glass
of ice and water (the system and not part of the room), begins to equalize as
portions of the thermal_energy from the warm surroundings spread to the cooler
system of ice and water. Over time the temperature of the glass and its
contents and the temperature of the room become equal. In other words, the
entropy of the room has decreased as some of its energy has been dispersed to
the ice and water.
However, as calculated in the example, the entropy of the system of ice and
water has increased more than the entropy of the surrounding room has
decreased. In an isolated_system such as the room and ice water taken together,
the dispersal of energy from warmer to cooler always results in a net increase
in entropy. Thus, when the "universe" of the room and ice water system has
reached a temperature equilibrium, the entropy change from the initial state is
at a maximum. The entropy of the thermodynamic_system is a measure of how far
the equalization has progressed.
Thermodynamic entropy is a non-conserved state_function that is of great
importance in the sciences of physics and chemistry.[16][23] Historically, the
concept of entropy evolved to explain why some processes (permitted by
conservation laws) occur spontaneously while their time_reversals (also
permitted by conservation laws) do not; systems tend to progress in the
direction of increasing entropy.[24][25] For isolated_systems, entropy never
decreases.[23] This fact has several important consequences in science: first,
it prohibits "perpetual_motion" machines; and second, it implies the arrow_of
entropy has the same direction as the arrow_of_time. Increases in entropy
correspond to irreversible changes in a system, because some energy is expended
as waste heat, limiting the amount of work a system can do.[16][17][26][27]
Unlike many other functions of state, entropy cannot be directly observed but
must be calculated. Entropy can be calculated for a substance as the standard
molar_entropy from absolute_zero (also known as absolute entropy) or as a
difference in entropy from some other reference state which is defined as zero
entropy. Entropy has the dimension of energy divided by temperature, which has
a unit of joules per kelvin (J/K) in the International_System_of_Units. While
these are the same units as heat_capacity, the two concepts are distinct.[28]
Entropy is not a conserved quantity: for example, in an isolated system with
non-uniform temperature, heat might irreversibly flow and the temperature
become more uniform such that entropy increases. The second_law_of
thermodynamics states that a closed system has entropy which may increase or
otherwise remain constant. Chemical reactions cause changes in entropy and
entropy plays an important role in determining in which direction a chemical
reaction spontaneously proceeds.
One dictionary definition of entropy is that it is "a measure of thermal energy
per unit temperature that is not available for useful work". For instance, a
substance at uniform temperature is at maximum entropy and cannot drive a heat
engine. A substance at non-uniform temperature is at a lower entropy (than if
the heat distribution is allowed to even out) and some of the thermal energy
can drive a heat engine.
A special case of entropy increase, the entropy_of_mixing, occurs when two or
more different substances are mixed. If the substances are at the same
temperature and pressure, there is no net exchange of heat or work â the
entropy change is entirely due to the mixing of the different substances. At a
statistical mechanical level, this results due to the change in available
volume per particle with mixing.[29]
**** Equivalence of definitions[edit] ****
Proofs of equivalence between the definition of entropy in statistical
mechanics (the Gibbs_entropy_formula     S = &#x2212;  k   B     &#x2211;  i
p  i   log &#x2061;  p  i     {\displaystyle S=-k_{\mathrm {B} }\sum _{i}p_
{i}\log p_{i}}  [{\displaystyle S=-k_{\mathrm {B} }\sum _{i}p_{i}\log p_{i}}])
and in classical thermodynamics (    d S =    &#x03B4;  Q  rev    T
{\displaystyle dS={\frac {\delta Q_{\text{rev}}}{T}}}  [{\displaystyle dS=
{\frac {\delta Q_{\text{rev}}}{T}}}] together with the fundamental
thermodynamic_relation) are known for the microcanonical_ensemble, the
canonical_ensemble, the grand_canonical_ensemble, and the isothermalâisobaric
ensemble. These proofs are based on the probability density of microstates of
the generalized Boltzmann_distribution and the identification of the
thermodynamic internal energy as the ensemble average     U =  &#x27E8;  E  i
&#x27E9;    {\displaystyle U=\left\langle E_{i}\right\rangle }  [{\displaystyle
U=\left\langle E_{i}\right\rangle }][30]. Thermodynamic relations are then
emplyed to derive the well-known Gibbs_entropy_formula. However, the
equivalence between the Gibbs_entropy_formula and the thermodynamic definition
of entropy is not a fundamental thermodynamic relation but rather a consequence
of the form of the generalized Boltzmann_distribution[31].
***** Second law of thermodynamics[edit] *****
Main article: Second_law_of_thermodynamics
See also: Thermodynamic_Equilibrium and Non-equilibrium_thermodynamics
The second law of thermodynamics requires that, in general, the total entropy
of any system can't decrease other than by increasing the entropy of some other
system. Hence, in a system isolated from its environment, the entropy of that
system tends not to decrease. It follows that heat can't flow from a colder
body to a hotter body without the application of work (the imposition of order)
to the colder body. Secondly, it is impossible for any device operating on a
cycle to produce net work from a single temperature reservoir; the production
of net work requires flow of heat from a hotter reservoir to a colder
reservoir, or a single expanding reservoir undergoing adiabatic_cooling, which
performs adiabatic_work. As a result, there is no possibility of a perpetual
motion system. It follows that a reduction in the increase of entropy in a
specified process, such as a chemical_reaction, means that it is energetically
more efficient.
It follows from the second law of thermodynamics that the entropy of a system
that is not isolated may decrease. An air_conditioner, for example, may cool
the air in a room, thus reducing the entropy of the air of that system. The
heat expelled from the room (the system), which the air conditioner transports
and discharges to the outside air, always makes a bigger contribution to the
entropy of the environment than the decrease of the entropy of the air of that
system. Thus, the total of entropy of the room plus the entropy of the
environment increases, in agreement with the second law of thermodynamics.
In mechanics, the second law in conjunction with the fundamental_thermodynamic
relation places limits on a system's ability to do useful_work.[32] The entropy
change of a system at temperature T absorbing an infinitesimal amount of heat
Î´q in a reversible way, is given by Î´q/T. More explicitly, an energy TR S is
not available to do useful work, where TR is the temperature of the coldest
accessible reservoir or heat sink external to the system. For further
discussion, see Exergy.
Statistical mechanics demonstrates that entropy is governed by probability,
thus allowing for a decrease in disorder even in an isolated system. Although
this is possible, such an event has a small probability of occurring, making it
unlikely.[33]
The applicability of a second law of thermodynamics is limited to systems which
are near or in equilibrium_state.[34] At the same time, laws governing systems
which are far from equilibrium are still debatable. One of the guiding
principles for such systems is the maximum entropy production principle.[35]
[36] It claims that non-equilibrium systems evolve such as to maximize its
entropy production.[37][38]
***** Applications[edit] *****
**** The fundamental thermodynamic relation[edit] ****
Main article: Fundamental_thermodynamic_relation
The entropy of a system depends on its internal energy and its external
parameters, such as its volume. In the thermodynamic limit, this fact leads to
an equation relating the change in the internal energy U to changes in the
entropy and the external parameters. This relation is known as the fundamental
thermodynamic relation. If external pressure p bears on the volume V as the
only external parameter, this relation is:
         d U = T  d S &#x2212; p  d V   {\displaystyle dU=T\,dS-p\,dV}  [
      {\displaystyle dU=T\,dS-p\,dV}]
Since both internal energy and entropy are monotonic functions of temperature
T, implying that the internal energy is fixed when one specifies the entropy
and the volume, this relation is valid even if the change from one state of
thermal equilibrium to another with infinitesimally larger entropy and volume
happens in a non-quasistatic way (so during this change the system may be very
far out of thermal equilibrium and then the entropy, pressure and temperature
may not exist).
The fundamental thermodynamic relation implies many thermodynamic identities
that are valid in general, independent of the microscopic details of the
system. Important examples are the Maxwell_relations and the relations_between
heat_capacities.
**** Entropy in chemical thermodynamics[edit] ****
Thermodynamic entropy is central in chemical_thermodynamics, enabling changes
to be quantified and the outcome of reactions predicted. The second_law_of
thermodynamics states that entropy in an isolated_system â the combination of
a subsystem under study and its surroundings â increases during all
spontaneous chemical and physical processes. The Clausius equation of Î´qrev/
T = ÎS introduces the measurement of entropy change, ÎS. Entropy change
describes the direction and quantifies the magnitude of simple changes such as
heat transfer between systems â always from hotter to cooler spontaneously.
The thermodynamic entropy therefore has the dimension of energy divided by
temperature, and the unit joule per kelvin (J/K) in the International System of
Units (SI).
Thermodynamic entropy is an extensive property, meaning that it scales with the
size or extent of a system. In many processes it is useful to specify the
entropy as an intensive_property independent of the size, as a specific entropy
characteristic of the type of system studied. Specific entropy may be expressed
relative to a unit of mass, typically the kilogram (unit: Jâkgâ1âKâ1).
Alternatively, in chemistry, it is also referred to one mole of substance, in
which case it is called the molar entropy with a unit of Jâmolâ1âKâ1.
Thus, when one mole of substance at about 0 K is warmed by its surroundings to
298 K, the sum of the incremental values of qrev/T constitute each element's or
compound's standard molar entropy, an indicator of the amount of energy stored
by a substance at 298 K.[39][40] Entropy change also measures the mixing of
substances as a summation of their relative quantities in the final mixture.
[41]
Entropy is equally essential in predicting the extent and direction of complex
chemical reactions. For such applications, ÎS must be incorporated in an
expression that includes both the system and its surroundings, ÎSuniverse =
ÎSsurroundings + ÎS system. This expression becomes, via some steps, the
Gibbs_free_energy equation for reactants and products in the system: ÎG [the
Gibbs free energy change of the system] = ÎH [the enthalpy change] â T ÎS
[the entropy change].[39]
**** Entropy balance equation for open systems[edit] ****
During steady-state continuous operation, an entropy balance applied to an open
system accounts for system entropy changes related to heat flow and mass flow
across the system boundary.
In chemical_engineering, the principles of thermodynamics are commonly applied
to "open_systems", i.e. those in which heat, work, and mass flow across the
system boundary. Flows of both heat (       Q &#x02D9;      {\displaystyle
{\dot {Q}}}  [{\dot {Q}}]) and work, i.e.         W &#x02D9;     S
{\displaystyle {\dot {W}}_{\text{S}}}  [{\displaystyle {\dot {W}}_{\text{S}}}]
(shaft_work) and P(dV/dt) (pressure-volume work), across the system boundaries,
in general cause changes in the entropy of the system. Transfer as heat entails
entropy transfer        Q &#x02D9;     /  T ,   {\displaystyle {\dot {Q}}/T,}
[{\dot {Q}}/T,] where T is the absolute thermodynamic_temperature of the system
at the point of the heat flow. If there are mass flows across the system
boundaries, they also influence the total entropy of the system. This account,
in terms of heat and work, is valid only for cases in which the work and heat
transfers are by paths physically distinct from the paths of entry and exit of
matter from the system.[42][43]
To derive a generalized entropy balanced equation, we start with the general
balance equation for the change in any extensive_quantity Î in a thermodynamic
system, a quantity that may be either conserved, such as energy, or non-
conserved, such as entropy. The basic generic balance expression states that
dÎ/dt, i.e. the rate of change of Î in the system, equals the rate at which
Î enters the system at the boundaries, minus the rate at which Î leaves the
system across the system boundaries, plus the rate at which Î is generated
within the system. For an open thermodynamic system in which heat and work are
transferred by paths separate from the paths for transfer of matter, using this
generic balance equation, with respect to the rate of change with time t of the
extensive quantity entropy S, the entropy balance equation is:[44][note_1]
            d S   d t    =  &#x2211;  k = 1   K       M &#x02D9;     k       S
      &#x005E;     k   +     Q &#x02D9;   T   +     S &#x02D9;     gen
      {\displaystyle {\frac {dS}{dt}}=\sum _{k=1}^{K}{\dot {M}}_{k}{\hat {S}}_
      {k}+{\frac {\dot {Q}}{T}}+{\dot {S}}_{\text{gen}}}  [{\displaystyle
      {\frac {dS}{dt}}=\sum _{k=1}^{K}{\dot {M}}_{k}{\hat {S}}_{k}+{\frac {\dot
      {Q}}{T}}+{\dot {S}}_{\text{gen}}}]
where
          &#x2211;  k = 1   K       M &#x02D9;     k       S &#x005E;     k   =
      {\displaystyle \sum _{k=1}^{K}{\dot {M}}_{k}{\hat {S}}_{k}={}}  [
      {\displaystyle \sum _{k=1}^{K}{\dot {M}}_{k}{\hat {S}}_{k}={}}] the net
      rate of entropy flow due to the flows of mass into and out of the system
      (where        S &#x005E;    =     {\displaystyle {\hat {S}}={}}  [
      {\displaystyle {\hat {S}}={}}] entropy per unit mass).
             Q &#x02D9;   T   =     {\displaystyle {\frac {\dot {Q}}{T}}={}}  [
      {\displaystyle {\frac {\dot {Q}}{T}}={}}] the rate of entropy flow due to
      the flow of heat across the system boundary.
             S &#x02D9;     gen   =     {\displaystyle {\dot {S}}_{\text{gen}}=
      {}}  [{\displaystyle {\dot {S}}_{\text{gen}}={}}] the rate of entropy
      production within the system. This entropy production arises from
      processes within the system, including chemical reactions, internal
      matter diffusion, internal heat transfer, and frictional effects such as
      viscosity occurring within the system from mechanical work transfer to or
      from the system.
Note, also, that if there are multiple heat flows, the term        Q &#x02D9;
/  T   {\displaystyle {\dot {Q}}/T}  [{\dot {Q}}/T] is replaced by     &#x2211;
Q &#x02D9;     j    /   T  j   ,   {\displaystyle \sum {\dot {Q}}_{j}/T_{j},}
[\sum {\dot {Q}}_{j}/T_{j},] where         Q &#x02D9;     j     {\displaystyle
{\dot {Q}}_{j}}  [{\dot {Q}}_{j}] is the heat flow and      T  j
{\displaystyle T_{j}}  [T_{j}] is the temperature at the jth heat flow port
into the system.
***** Entropy change formulas for simple processes[edit] *****
For certain simple transformations in systems of constant composition, the
entropy changes are given by simple formulas.[45]
**** Isothermal expansion or compression of an ideal gas[edit] ****
For the expansion (or compression) of an ideal_gas from an initial volume
V  0     {\displaystyle V_{0}}  [V_{0}] and pressure      P  0
{\displaystyle P_{0}}  [P_{0}] to a final volume     V   {\displaystyle V}  [V]
and pressure     P   {\displaystyle P}  [P] at any constant temperature, the
change in entropy is given by:
         &#x0394; S = n R ln &#x2061;   V  V  0     = &#x2212; n R ln &#x2061;
      P  P  0     .   {\displaystyle \Delta S=nR\ln {\frac {V}{V_{0}}}=-nR\ln
      {\frac {P}{P_{0}}}.}  [\Delta S=nR\ln {\frac {V}{V_{0}}}=-nR\ln {\frac
      {P}{P_{0}}}.]
Here     n   {\displaystyle n}  [n] is the number of moles of gas and     R
{\displaystyle R}  [R] is the ideal_gas_constant. These equations also apply
for expansion into a finite vacuum or a throttling_process, where the
temperature, internal energy and enthalpy for an ideal gas remain constant.
**** Cooling and heating[edit] ****
For heating or cooling of any system (gas, liquid or solid) at constant
pressure from an initial temperature      T  0     {\displaystyle T_{0}}  [T_
{0}] to a final temperature     T   {\displaystyle T}  [T], the entropy change
is
         &#x0394; S = n  C  P   ln &#x2061;   T  T  0     .   {\displaystyle
      \Delta S=nC_{P}\ln {\frac {T}{T_{0}}}.}  [{\displaystyle \Delta S=nC_
      {P}\ln {\frac {T}{T_{0}}}.}]
provided that the constant-pressure molar heat_capacity (or specific heat) CP
is constant and that no phase_transition occurs in this temperature interval.
Similarly at constant volume, the entropy change is
         &#x0394; S = n  C  v   ln &#x2061;   T  T  0     ,   {\displaystyle
      \Delta S=nC_{v}\ln {\frac {T}{T_{0}}},}  [{\displaystyle \Delta S=nC_
      {v}\ln {\frac {T}{T_{0}}},}]
where the constant-volume heat capacity Cv is constant and there is no phase
change.
At low temperatures near absolute zero, heat capacities of solids quickly drop
off to near zero, so the assumption of constant heat capacity does not apply.
[46]
Since entropy is a state_function, the entropy change of any process in which
temperature and volume both vary is the same as for a path divided into two
steps â heating at constant volume and expansion at constant temperature. For
an ideal gas, the total entropy change is[47]
         &#x0394; S = n  C  v   ln &#x2061;   T  T  0     + n R ln &#x2061;   V
      V  0     .   {\displaystyle \Delta S=nC_{v}\ln {\frac {T}{T_{0}}}+nR\ln
      {\frac {V}{V_{0}}}.}  [{\displaystyle \Delta S=nC_{v}\ln {\frac {T}{T_
      {0}}}+nR\ln {\frac {V}{V_{0}}}.}]
Similarly if the temperature and pressure of an ideal gas both vary,
         &#x0394; S = n  C  P   ln &#x2061;   T  T  0     &#x2212; n R ln
      &#x2061;   P  P  0     .   {\displaystyle \Delta S=nC_{P}\ln {\frac {T}
      {T_{0}}}-nR\ln {\frac {P}{P_{0}}}.}  [{\displaystyle \Delta S=nC_{P}\ln
      {\frac {T}{T_{0}}}-nR\ln {\frac {P}{P_{0}}}.}]
**** Phase transitions[edit] ****
Reversible phase_transitions occur at constant temperature and pressure. The
reversible heat is the enthalpy change for the transition, and the entropy
change is the enthalpy change divided by the thermodynamic temperature. For
fusion (melting) of a solid to a liquid at the melting point Tm, the entropy_of
fusion is
         &#x0394;  S  fus   =    &#x0394;  H  fus     T  m     .
      {\displaystyle \Delta S_{\text{fus}}={\frac {\Delta H_{\text{fus}}}{T_
      {\text{m}}}}.}  [\Delta S_{\text{fus}}={\frac {\Delta H_{\text{fus}}}{T_
      {\text{m}}}}.]
Similarly, for vaporization of a liquid to a gas at the boiling point Tb, the
entropy_of_vaporization is
         &#x0394;  S  vap   =    &#x0394;  H  vap     T  b     .
      {\displaystyle \Delta S_{\text{vap}}={\frac {\Delta H_{\text{vap}}}{T_
      {\text{b}}}}.}  [\Delta S_{\text{vap}}={\frac {\Delta H_{\text{vap}}}{T_
      {\text{b}}}}.]
***** Approaches to understanding entropy[edit] *****
As a fundamental aspect of thermodynamics and physics, several different
approaches to entropy beyond that of Clausius and Boltzmann are valid.
**** Standard textbook definitions[edit] ****
The following is a list of additional definitions of entropy from a collection
of textbooks:
    * a measure of energy_dispersal at a specific temperature.
    * a measure of disorder in the universe or of the availability of the
      energy in a system to do work.[48]
    * a measure of a system's thermal_energy per unit temperature that is
      unavailable for doing useful work.[49]
In Boltzmann's definition, entropy is a measure of the number of possible
microscopic states (or microstates) of a system in thermodynamic equilibrium.
Consistent with the Boltzmann definition, the second law of thermodynamics
needs to be re-worded as such that entropy increases over time, though the
underlying principle remains the same.
**** Order and disorder[edit] ****
Main article: Entropy_(order_and_disorder)
Entropy has often been loosely associated with the amount of order or disorder,
or of chaos, in a thermodynamic_system. The traditional qualitative description
of entropy is that it refers to changes in the status quo of the system and is
a measure of "molecular disorder" and the amount of wasted energy in a
dynamical energy transformation from one state or form to another. In this
direction, several recent authors have derived exact entropy formulas to
account for and measure disorder and order in atomic and molecular assemblies.
[50][51][52] One of the simpler entropy order/disorder formulas is that derived
in 1984 by thermodynamic physicist Peter Landsberg, based on a combination of
thermodynamics and information_theory arguments. He argues that when
constraints operate on a system, such that it is prevented from entering one or
more of its possible or permitted states, as contrasted with its forbidden
states, the measure of the total amount of "disorder" in the system is given
by:[51][52]
          Disorder  =    C  D    C  I     .    {\displaystyle {\text
      {Disorder}}={C_{\text{D}} \over C_{\text{I}}}.\,}  [{\displaystyle {\text
      {Disorder}}={C_{\text{D}} \over C_{\text{I}}}.\,}]
Similarly, the total amount of "order" in the system is given by:
          Order  = 1 &#x2212;    C  O    C  I     .    {\displaystyle {\text
      {Order}}=1-{C_{\text{O}} \over C_{\text{I}}}.\,}  [{\displaystyle {\text
      {Order}}=1-{C_{\text{O}} \over C_{\text{I}}}.\,}]
In which CD is the "disorder" capacity of the system, which is the entropy of
the parts contained in the permitted ensemble, CI is the "information" capacity
of the system, an expression similar to Shannon's channel_capacity, and CO is
the "order" capacity of the system.[50]
**** Energy dispersal[edit] ****
Main article: Entropy_(energy_dispersal)
The concept of entropy can be described qualitatively as a measure of energy
dispersal at a specific temperature.[53] Similar terms have been in use from
early in the history of classical_thermodynamics, and with the development of
statistical_thermodynamics and quantum_theory, entropy changes have been
described in terms of the mixing or "spreading" of the total energy of each
constituent of a system over its particular quantized energy levels.
Ambiguities in the terms disorder and chaos, which usually have meanings
directly opposed to equilibrium, contribute to widespread confusion and hamper
comprehension of entropy for most students.[54] As the second_law_of
thermodynamics shows, in an isolated_system internal portions at different
temperatures tend to adjust to a single uniform temperature and thus produce
equilibrium. A recently developed educational approach avoids ambiguous terms
and describes such spreading out of energy as dispersal, which leads to loss of
the differentials required for work even though the total energy remains
constant in accordance with the first_law_of_thermodynamics[55] (compare
discussion in next section). Physical chemist Peter_Atkins, for example, who
previously wrote of dispersal leading to a disordered state, now writes that
"spontaneous changes are always accompanied by a dispersal of energy".[56]
**** Relating entropy to energy usefulness[edit] ****
Following on from the above, it is possible (in a thermal context) to regard
lower entropy as an indicator or measure of the effectiveness or usefulness of
a particular quantity of energy.[57] This is because energy supplied at a
higher temperature (i.e. with low entropy) tends to be more useful than the
same amount of energy available at a lower temperature. Mixing a hot parcel of
a fluid with a cold one produces a parcel of intermediate temperature, in which
the overall increase in entropy represents a "loss" which can never be
replaced.
Thus, the fact that the entropy of the universe is steadily increasing, means
that its total energy is becoming less useful: eventually, this will lead to
the "heat_death_of_the_Universe".[58]
**** Entropy and adiabatic accessibility[edit] ****
A definition of entropy based entirely on the relation of adiabatic
accessibility between equilibrium states was given by E.H.Lieb and J._Yngvason
in 1999.[59] This approach has several predecessors, including the pioneering
work of Constantin_CarathÃ©odory from 1909[60] and the monograph by R. Giles.
[61] In the setting of Lieb and Yngvason one starts by picking, for a unit
amount of the substance under consideration, two reference states      X  0
{\displaystyle X_{0}}  [X_{0}] and      X  1     {\displaystyle X_{1}}  [X_{1}]
such that the latter is adiabatically accessible from the former but not vice
versa. Defining the entropies of the reference states to be 0 and 1
respectively the entropy of a state     X   {\displaystyle X}  [X] is defined
as the largest number     &#x03BB;   {\displaystyle \lambda }  [\lambda ] such
that     X   {\displaystyle X}  [X] is adiabatically accessible from a
composite state consisting of an amount     &#x03BB;   {\displaystyle \lambda }
[\lambda ] in the state      X  1     {\displaystyle X_{1}}  [X_{1}] and a
complementary amount,     ( 1 &#x2212; &#x03BB; )   {\displaystyle (1-\lambda
)}  [(1-\lambda )], in the state      X  0     {\displaystyle X_{0}}  [X_{0}].
A simple but important result within this setting is that entropy is uniquely
determined, apart from a choice of unit and an additive constant for each
chemical element, by the following properties: It is monotonic with respect to
the relation of adiabatic accessibility, additive on composite systems, and
extensive under scaling.
**** Entropy in quantum mechanics[edit] ****
Main article: von_Neumann_entropy
In quantum_statistical_mechanics, the concept of entropy was developed by John
von_Neumann and is generally referred to as "von_Neumann_entropy",
         S = &#x2212;  k   B    Tr &#x2061; ( &#x03C1; log &#x2061; &#x03C1; )
      {\displaystyle S=-k_{\mathrm {B} }\operatorname {Tr} (\rho \log \rho )\!}
      [{\displaystyle S=-k_{\mathrm {B} }\operatorname {Tr} (\rho \log \rho
      )\!}]
where Ï is the density_matrix and Tr is the trace operator.
This upholds the correspondence_principle, because in the classical_limit, when
the phases between the basis states used for the classical probabilities are
purely random, this expression is equivalent to the familiar classical
definition of entropy,
         S = &#x2212;  k   B     &#x2211;  i    p  i    log   p  i   ,
      {\displaystyle S=-k_{\mathrm {B} }\sum _{i}p_{i}\,\log \,p_{i},}  [
      {\displaystyle S=-k_{\mathrm {B} }\sum _{i}p_{i}\,\log \,p_{i},}]
i.e. in such a basis the density matrix is diagonal.
Von Neumann established a rigorous mathematical framework for quantum mechanics
with his work Mathematische Grundlagen der Quantenmechanik. He provided in this
work a theory of measurement, where the usual notion of wave_function_collapse
is described as an irreversible process (the so-called von Neumann or
projective measurement). Using this concept, in conjunction with the density
matrix he extended the classical concept of entropy into the quantum domain.
**** Information theory[edit] ****
I thought of calling it "information", but the word was overly used, so I
decided to call it "uncertainty". [...] Von Neumann told me, "You should call
it entropy, for two reasons. In the first place your uncertainty function has
been used in statistical mechanics under that name, so it already has a name.
In the second place, and more important, nobody knows what entropy really is,
so in a debate you will always have the advantage."
Conversation between Claude_Shannon and John_von_Neumann regarding what name to
give to the attenuation in phone-line signals[62]
Main articles: Entropy_(information_theory), Entropy_in_thermodynamics_and
information_theory, and Entropic_uncertainty
When viewed in terms of information theory, the entropy state function is
simply the amount of information (in the Shannon sense) that would be needed to
specify the full microstate of the system. This is left unspecified by the
macroscopic description.
In information_theory, entropy is the measure of the amount of information that
is missing before reception and is sometimes referred to as Shannon entropy.
[63] Shannon entropy is a broad and general concept which finds applications in
information theory as well as thermodynamics. It was originally devised by
Claude_Shannon in 1948 to study the amount of information in a transmitted
message. The definition of the information entropy is, however, quite general,
and is expressed in terms of a discrete set of probabilities pi so that
         H ( X ) = &#x2212;  &#x2211;  i = 1   n   p (  x  i   ) log &#x2061; p
      (  x  i   ) .   {\displaystyle H(X)=-\sum _{i=1}^{n}p(x_{i})\log p(x_
      {i}).}  [H(X)=-\sum _{i=1}^{n}p(x_{i})\log p(x_{i}).]
In the case of transmitted messages, these probabilities were the probabilities
that a particular message was actually transmitted, and the entropy of the
message system was a measure of the average amount of information in a message.
For the case of equal probabilities (i.e. each message is equally probable),
the Shannon entropy (in bits) is just the number of yes/no questions needed to
determine the content of the message.[19]
The question of the link between information entropy and thermodynamic entropy
is a debated topic. While most authors argue that there is a link between the
two,[64][65][66][67][68] a few argue that they have nothing to do with each
other.[citation_needed] The expressions for the two entropies are similar. If W
is the number of microstates that can yield a given macrostate, and each
microstate has the same a_priori probability, then that probability is p = 1/W.
The Shannon entropy (in nats) is:
         H = &#x2212;  &#x2211;  i = 1   W   p log &#x2061; ( p ) = log
      &#x2061; ( W )   {\displaystyle H=-\sum _{i=1}^{W}p\log(p)=\log(W)}  [H=-
      \sum _{i=1}^{W}p\log(p)=\log(W)]
and if entropy is measured in units of k per nat, then the entropy is given[69]
by:
         H = k log &#x2061; ( W )   {\displaystyle H=k\log(W)}  [{\displaystyle
      H=k\log(W)}]
which is the famous Boltzmann_entropy_formula when k is Boltzmann's constant,
which may be interpreted as the thermodynamic entropy per nat. There are many
ways of demonstrating the equivalence of "information entropy" and "physics
entropy", that is, the equivalence of "Shannon entropy" and "Boltzmann
entropy". Nevertheless, some authors argue for dropping the word entropy for
the H function of information theory and using Shannon's other term
"uncertainty" instead.[70]
**** Experimental measurement of entropy[edit] ****
Entropy of a substance can be measured, although in an indirect way. The
measurement uses the definition of temperature[71] in terms of entropy, while
limiting energy exchange to heat (    d U &#x2192; d Q   {\displaystyle
dU\rightarrow dQ}  [{\displaystyle dU\rightarrow dQ}]).
         T :=   (    &#x2202; U   &#x2202; S    )   V , N   &#x21D2; &#x22EF;
      &#x21D2;  d S = d Q  /  T   {\displaystyle T:=\left({\frac {\partial U}
      {\partial S}}\right)_{V,N}\Rightarrow \cdots \Rightarrow \;dS=dQ/T}  [
      {\displaystyle T:=\left({\frac {\partial U}{\partial S}}\right)_
      {V,N}\Rightarrow \cdots \Rightarrow \;dS=dQ/T}]
The resulting relation describes how entropy changes     d S   {\displaystyle
dS}  [dS] when a small amount of energy     d Q   {\displaystyle dQ}  [dQ] is
introduced into the system at a certain temperature     T   {\displaystyle T}
[T].
The process of measurement goes as follows. First, a sample of the substance is
cooled as close to absolute zero as possible. At such temperatures, the entropy
approaches zero – due to the definition of temperature. Then, small amounts of
heat are introduced into the sample and the change in temperature is recorded,
until the temperature reaches a desired value (usually 25Â°C). The obtained
data allows the user to integrate the equation above, yielding the absolute
value of entropy of the substance at the final temperature. This value of
entropy is called calorimetric entropy.[72]
***** Interdisciplinary applications of entropy[edit] *****
Although the concept of entropy was originally a thermodynamic construct, it
has been adapted in other fields of study, including information_theory,
psychodynamics, thermoeconomics/ecological_economics, and evolution.[73][74]
[75][76][77] For instance, an entropic argument has been recently proposed for
explaining the preference of cave spiders in choosing a suitable area for
laying their eggs.[78]
**** Thermodynamic and statistical mechanics concepts[edit] ****
    * Entropy unit â a non-S.I. unit of thermodynamic entropy, usually
      denoted "e.u." and equal to one calorie per kelvin per mole, or 4.184
      joules per kelvin per mole.[79]
    * Gibbs_entropy â the usual statistical mechanical entropy of a
      thermodynamic system.
    * Boltzmann_entropy â a type of Gibbs entropy, which neglects internal
      statistical correlations in the overall particle distribution.
    * Tsallis_entropy â a generalization of the standard BoltzmannâGibbs
      entropy.
    * Standard_molar_entropy â is the entropy content of one mole of
      substance, under conditions of standard temperature and pressure.
    * Residual_entropy â the entropy present after a substance is cooled
      arbitrarily close to absolute_zero.
    * Entropy_of_mixing â the change in the entropy when two different
      chemical_substances or components are mixed.
    * Loop_entropy â is the entropy lost upon bringing together two residues
      of a polymer within a prescribed distance.
    * Conformational_entropy â is the entropy associated with the physical
      arrangement of a polymer chain that assumes a compact or globular state
      in solution.
    * Entropic_force â a microscopic force or reaction tendency related to
      system organization changes, molecular frictional considerations, and
      statistical variations.
    * Free_entropy â an entropic thermodynamic potential analogous to the
      free energy.
    * Entropic_explosion â an explosion in which the reactants undergo a
      large change in volume without releasing a large amount of heat.
    * Entropy change â a change in entropy dS between two equilibrium_states
      is given by the heat transferred dQrev divided by the absolute
      temperature T of the system in this interval.
    * SackurâTetrode_entropy â the entropy of a monatomic classical ideal
      gas determined via quantum considerations.
**** The arrow of time[edit] ****
Main article: Entropy_(arrow_of_time)
Entropy is the only quantity in the physical sciences that seems to imply a
particular direction of progress, sometimes called an arrow_of_time. As time
progresses, the second law of thermodynamics states that the entropy of an
isolated_system never decreases in large systems over significant periods of
time. Hence, from this perspective, entropy measurement is thought of as a
clock in these conditions.
**** Entropy in DNA sequences[edit] ****
Entropy has been proven useful in the analysis of DNA sequences. Many entropy-
based measures have been shown to distinguish between different structural
regions of the genome, differentiate between coding and non-coding regions of
DNA and can also be applied for the recreation of evolutionary trees by
determining the evolutionary distance between different species.[80]
**** Cosmology[edit] ****
Main article: Heat_death_of_the_universe
Since a finite universe is an isolated system, the second_law_of_thermodynamics
states that its total entropy is continually increasing. It has been
speculated, since the 19th century, that the universe is fated to a heat_death
in which all the energy ends up as a homogeneous distribution of thermal energy
so that no more work can be extracted from any source.
If the universe can be considered to have generally increasing entropy, then
â as Roger_Penrose has pointed out â gravity plays an important role in the
increase because gravity causes dispersed matter to accumulate into stars,
which collapse eventually into black_holes. The_entropy_of_a_black_hole is
proportional to the surface area of the black hole's event_horizon.[81] Jacob
Bekenstein and Stephen_Hawking have shown that black holes have the maximum
possible entropy of any object of equal size. This makes them likely end points
of all entropy-increasing processes, if they are totally effective matter and
energy traps.[citation_needed] However, the escape of energy from black holes
might be possible due to quantum activity (see Hawking_radiation).
The role of entropy in cosmology remains a controversial subject since the time
of Ludwig_Boltzmann. Recent work has cast some doubt on the heat death
hypothesis and the applicability of any simple thermodynamic model to the
universe in general. Although entropy does increase in the model of an
expanding universe, the maximum possible entropy rises much more rapidly,
moving the universe further from the heat death with time, not closer.[82][83]
[84] This results in an "entropy gap" pushing the system further away from the
posited heat death equilibrium.[85] Other complicating factors, such as the
energy density of the vacuum and macroscopic quantum effects, are difficult to
reconcile with thermodynamical models, making any predictions of large-scale
thermodynamics extremely difficult.[86]
Current theories suggest the entropy gap to have been originally opened up by
the_early_rapid_exponential_expansion of the universe.[87]
**** Economics[edit] ****
See also: Nicholas_Georgescu-Roegen_Â§ The_relevance_of_thermodynamics_to
economics, and Ecological_economics_Â§ Methodology
Romanian_American economist Nicholas_Georgescu-Roegen, a progenitor in
economics and a paradigm_founder of ecological_economics, made extensive use of
the entropy concept in his magnum_opus_on_The_Entropy_Law_and_the_Economic
Process.[88] Due to Georgescu-Roegen's work, the laws of thermodynamics now
form an integral_part_of_the_ecological_economics_school.[89]:204f [90]:29â35
Although his work was blemished_somewhat_by_mistakes, a full chapter on the
economics of Georgescu-Roegen has approvingly been included in one elementary
physics textbook on the historical development of thermodynamics.[91]:95â112
In economics, Georgescu-Roegen's work has generated the term 'entropy
pessimism'.[92]:116 Since the 1990s, leading ecological economist and steady-
state_theorist Herman_Daly â a student of Georgescu-Roegen â has been the
economics profession's most influential proponent of the entropy pessimism
position.[93]:545f [94]
**** Hermeneutics[edit] ****
In Hermeneutics, Arianna BÃ©atrice Fabbricatore has used the term entropy
relying on the works of Umberto Eco,[95] to identify and assess the loss of
meaning between the verbal description of dance and the choreotext (the moving
silk engaged by the dancer when he puts into action the choreographic writing)
[96] generated by inter-semiotic translation operations[97].[98].
This use is linked to the notions of logotext and choreotext. In the transition
from logotext to choreotext it is possible to identify two typologies of
entropy: the first, called "natural", is related to the uniqueness of the
performative act and its ephemeral character. The second is caused by "voids"
more or less important in the logotext (i.e. the verbal text that reflects the
action danced [99]).
***** See also[edit] *****
    * Autocatalytic_reactions_and_order_creation
    * Brownian_ratchet
    * ClausiusâDuhem_inequality
    * Configuration_entropy
    * Departure_function
    * Enthalpy
    * Entropic_force
    * Entropic_value_at_risk
    * Entropy_(information_theory)
    * Entropy_(computing)
    * Entropy_and_life
    * Entropy_(order_and_disorder)
    * Entropy_rate
    * Entropy_production
    * Extropy
    * Geometrical_frustration
    * Harmonic_entropy
    * Heat_death_of_the_universe
    * Info-metrics
    * Laws_of_thermodynamics
    * Multiplicity_function
    * Negentropy (negative entropy)
    * Orders_of_magnitude_(entropy)
    * Phase_space
    * Principle_of_maximum_entropy
    * Stirling's_formula
    * Thermodynamic_databases_for_pure_substances
    * Thermodynamic_potential
    * Thermodynamic_equilibrium
    * Wavelet entropy
***** Notes[edit] *****
   1. ^ The overdots represent derivatives of the quantities with respect to
      time.
***** References[edit] *****
   1. ^"Carnot,_Sadi_(1796â1832)". Wolfram Research. 2007. Retrieved 24
      February 2010.
   2. .mw-parser-output cite.citation{font-style:inherit}.mw-parser-output
      .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-
      free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/
      65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:
      right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-
      parser-output .citation .cs1-lock-registration a{background:url("//
      upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/
      9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em
      center}.mw-parser-output .citation .cs1-lock-subscription a{background:
      url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-
      2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right
      .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-
      registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-
      parser-output .cs1-registration span{border-bottom:1px dotted;cursor:
      help}.mw-parser-output .cs1-ws-icon a{background:url("//
      upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/
      12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em
      center}.mw-parser-output code.cs1-code{color:inherit;background:
      inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-
      error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error
      {font-size:100%}.mw-parser-output .cs1-maint{display:none;color:
      #33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-
      output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-
      parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-
      left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-
      wl-right{padding-right:0.2em}
   3. ^McCulloch, Richard, S. (1876). Treatise on the Mechanical Theory of Heat
      and its Applications to the Steam-Engine, etc. D. Van Nostrand.
   4. ^ a bClausius, Rudolf (1850). "Ãber die bewegende Kraft der WÃ¤rme und
      die Gesetze, welche sich daraus fÃ¼r die WÃ¤rmelehre selbst ableiten
      lassen [On the Motive Power of Heat, and on the Laws which can be deduced
      from it for the Theory of Heat]". Poggendorff's Annalen der Physik und
      Chemie. doi:10.1002/andp.18501550306.
   5. ^The_scientific_papers_of_J._Willard_Gibbs_in_Two_Volumes. 1. Longmans,
      Green, and Co. 1906. p. 11. Retrieved 26 February 2011.
   6. ^ J. A. McGovern,"2.5_Entropy". Archived from the_original on 23
      September 2012. Retrieved 5 February 2013.
   7. ^"6.5_Irreversibility,_Entropy_Changes,_and_Lost_Work". web.mit.edu.
      Retrieved 21 May 2016.
   8. ^Lower, Stephen. "What_is_entropy?". www.chem1.com. Retrieved 21 May
      2016.
   9. ^Lavenda, Bernard H. (2010). "2.3.4". A new perspective on thermodynamics
      (Online-Ausg. ed.). New York: Springer. ISBN 978-1-4419-1430-9.
  10. ^Carnot, Sadi Carnot (1986). Fox, Robert (ed.). Reflexions on the motive
      power of fire. New York: Lilian Barber Press. p. 26. ISBN 978-0-936508-
      16-0.
  11. ^Truesdell, C. (1980). The tragicomical history of thermodynamics
      1822â1854. New York: Springer. pp. 78â85. ISBN 978-0-387-90403-0.
  12. ^Clerk Maxwel, James (2001). Pesic, Peter (ed.). Theory of heat. Mineola:
      Dover Publications. pp. 115â158. ISBN 978-0-486-41735-6.
  13. ^Rudolf Clausius (1867). The_Mechanical_Theory_of_Heat:_With_Its
      Applications_to_the_Steam-engine_and_to_the_Physical_Properties_of
      Bodies. J. Van Voorst. p. 28. ISBN 978-1-4981-6733-8.
  14. ^Clausius, Rudolf (1865). "Ueber verschiedene fÃ¼r die Anwendung bequeme
      Formen der Hauptgleichungen der mechanischen WÃ¤rmetheorie (Vorgetragen
      in der naturforsch. Gesellschaft zu ZÃ¼rich den 24. April 1865)". Annalen
      der Physik und Chemie. 125 (7): 353â400. Bibcode:1865AnP...201..353C.
      doi:10.1002/andp.18652010702.
  15.  "Sucht man fÃ¼r S einen bezeichnenden Namen, so kÃ¶nnte man, Ã¤hnlich
      wie von der GrÃ¶Å¿se U gesagt ist, sie sey der WÃ¤rme- und Werkinhalt des
      KÃ¶rpers, von der GrÃ¶Å¿se S sagen, sie sey der Verwandlungsinhalt des
      KÃ¶rpers. Da ich es aber fÃ¼r besser halte, die Namen derartiger fÃ¼r die
      Wissenschaft wichtiger GrÃ¶Å¿sen aus den alten Sprachen zu entnehmen,
      damit sie unverÃ¤ndert in allen neuen Sprachen angewandt werden kÃ¶nnen,
      so schlage ich vor, die GrÃ¶Å¿se S nach dem griechischen Worte á¼¡
      ÏÏÎ¿Ïá½´, die Verwandlung, die Entropie des KÃ¶rpers zu nennen. Das
      Wort Entropie habei ich absichtlich dem Worte Energie mÃ¶glichst Ã¤hnlich
      gebildet, denn die beiden GrÃ¶Å¿sen, welche durch diese Worte benannt
      werden sollen, sind ihren physikalischen Bedeutungen nach einander so
      nahe verwandt, daÅ¿s eine gewisse Gleichartigkeit in der Benennung mir
      zweckmÃ¤Å¿sig zu seyn scheint." (p. 390).
  16. ^Atkins, Peter; Julio De Paula (2006). Physical Chemistry, 8th ed. Oxford
      University Press. p. 79. ISBN 978-0-19-870072-2.
  17. ^Engel, Thomas; Philip Reid (2006). Physical Chemistry. Pearson Benjamin
      Cummings. p. 86. ISBN 978-0-8053-3842-3.
  18. ^ a b cLicker, Mark D. (2004). McGraw-Hill concise encyclopedia of
      chemistry. New York: McGraw-Hill Professional. ISBN 978-0-07-143953-4.
  19. ^ a bSethna, James P. (2006). Statistical mechanics : entropy, order
      parameters, and complexity ([Online-Ausg.]. ed.). Oxford: Oxford
      University Press. p. 78. ISBN 978-0-19-856677-9.
  20. ^Clark, John O.E. (2004). The essential dictionary of science. New York:
      Barnes & Noble. ISBN 978-0-7607-4616-5.
  21. ^ a b Frigg,_R._and_Werndl,_C._"Entropy_â_A_Guide_for_the_Perplexed".
      In Probabilities in Physics; Beisbart C. and Hartmann, S. Eds; Oxford
      University Press, Oxford, 2010
  22. ^Schroeder, Daniel V. (2000). An introduction to thermal physics. San
      Francisco, CA: Addison Wesley. p. 57. ISBN 978-0-201-38027-9.
  23. ^"EntropyOrderParametersComplexity.pdf_www.physics.cornell.edu" (PDF).
      Retrieved 17 August 2012.
  24. ^"Jaynes,_E._T.,_"The_Gibbs_Paradox,"_In_Maximum_Entropy_and_Bayesian
      Methods;_Smith,_C._R;_Erickson,_G._J;_Neudorfer,_P._O.,_Eds;_Kluwer
      Academic:_Dordrecht,_1992,_pp._1â22" (PDF). Retrieved 17 August 2012.
  25. ^ a bSandler, Stanley I. (2006). Chemical, biochemical, and engineering
      thermodynamics (4th ed.). New York: John Wiley & Sons. p. 91. ISBN 978-0-
      471-66174-0.
  26. ^Simon, Donald A. McQuarrie; John D. (1997). Physical chemistry : a
      molecular approach (Rev. ed.). Sausalito, Calif.: Univ. Science Books.
      p. 817. ISBN 978-0-935702-99-6.
  27. ^Haynie, Donald, T. (2001). Biological Thermodynamics. Cambridge
      University_Press. ISBN 978-0-521-79165-6.
  28. ^Daintith, John (2005). A dictionary of science (5th ed.). Oxford: Oxford
      University Press. ISBN 978-0-19-280641-3.
  29. ^de Rosnay, Joel (1979). The_Macroscope â_a_New_World_View_(written_by
      an_M.I.T.-trained_biochemist). Harper & Row, Publishers. ISBN 978-0-06-
      011029-1.
  30. ^McGovern, J. A. "Heat_Capacities". Archived from the_original on 19
      August 2012. Retrieved 27 January 2013.
  31. ^Ben-Naim, Arieh (21 September 2007). "On_the_So-Called_Gibbs_Paradox,
      and_on_the_Real_Paradox" (PDF). Entropy. 9 (3): 132â136. Bibcode:
      2007Entrp...9..132B. doi:10.3390/e9030133.
  32. ^Callen, Herbert (2001). Thermodynamics and an Introduction to
      Thermostatistics (2nd ed.). John Wiley and Sons. ISBN 978-0-471-86256-7.
  33. ^Gao, Xiang; Gallicchio, Emilio; Roitberg, Adrian (2019). "The
      generalized_Boltzmann_distribution_is_the_only_distribution_in_which_the
      Gibbs-Shannon_entropy_equals_the_thermodynamic_entropy". The Journal of
      Chemical Physics. 151 (3): 034113. doi:10.1063/1.5111333.
  34. ^Daintith, John (2005). Oxford Dictionary of Physics. Oxford University
      Press. ISBN 978-0-19-280628-4.
  35. ^Saha, Arnab; Lahiri, Sourabh; Jayannavar, A. M. (2009). "Entropy
      production theorems and some consequences". Physical Review E. 80 (1):
      1â10. arXiv:0903.4147. Bibcode:2009PhRvE..80a1117S. doi:10.1103/
      PhysRevE.80.011117.
  36. ^Martyushev, L. M.; Seleznev, V. D. (2014). "The restrictions of the
      maximum entropy production principle". Physica A: Statistical Mechanics
      and its Applications. 410: 17â21. arXiv:1311.2068. Bibcode:
      2014PhyA..410...17M. doi:10.1016/j.physa.2014.05.014.
  37. ^Ziegler, H. (1983). An Introduction to Thermomechanics. North Holland,
      Amsterdam.
  38. ^Onsager, Lars (1931). "Reciprocal Relations in Irreversible Processes".
      Phys. Rev. 37 (4): 405. Bibcode:1931PhRv...37..405O. doi:10.1103/
      PhysRev.37.405.
  39. ^Kleidon, A.; et., al. (2005). Non-equilibrium Thermodynamics and the
      Production of Entropy. Heidelberg: Springer.
  40. ^Belkin, Andrey; et., al. (2015). "Self-assembled_wiggling_nano-
      structures_and_the_principle_of_maximum_entropy_production". Scientific
      Reports. 5: 8323. Bibcode:2015NatSR...5E8323B. doi:10.1038/srep08323.
      PMC 4321171. PMID 25662746.
  41. ^ a bMoore, J. W.; C. L. Stanistski; P. C. Jurs (2005). Chemistry, The
      Molecular Science. Brooks Cole. ISBN 978-0-534-42201-1.
  42. ^Jungermann, A.H. (2006). "Entropy and the Shelf Model: A Quantum
      Physical Approach to a Physical Property". Journal of Chemical Education.
      83 (11): 1686â1694. Bibcode:2006JChEd..83.1686J. doi:10.1021/
      ed083p1686.
  43. ^Levine, I. N. (2002). Physical Chemistry, 5th ed. McGraw-Hill. ISBN 978-
      0-07-231808-1.
  44. ^Late Nobel Laureate Max Born (8 August 2015). Natural_Philosophy_of
      Cause_and_Chance. BiblioLife. pp. 44, 146â147. ISBN 978-1-298-49740-6.
  45. ^Haase, R. (1971). Thermodynamics. New York: Academic Press. pp. 1â97.
      ISBN 978-0-12-245601-5.
  46. ^Sandler, Stanley, I. (1989). Chemical and Engineering Thermodynamics.
      John Wiley & Sons. ISBN 978-0-471-83050-4.
  47. ^"GRC.nasa.gov". GRC.nasa.gov. 27 March 2000. Retrieved 17 August 2012.
  48. ^Franzen, Stefan. "Third_Law" (PDF). ncsu.ed.
  49. ^"GRC.nasa.gov". GRC.nasa.gov. 11 July 2008. Retrieved 17 August 2012.
  50. ^Gribbin, John (1999). Gribbin, Mary (ed.). Q_is_for_quantum :_an
      encyclopedia_of_particle_physics. New York: Free Press. ISBN 978-0-684-
      85578-3.
  51. ^"Entropy:_Definition_and_Equation". EncyclopÃ¦dia Britannica. Retrieved
      22 May 2016.
  52. ^ a bBrooks, Daniel R.; Wiley, E. O. (1988). Evolution as entropy :
      toward a unified theory of biology (2nd ed.). Chicago [etc.]: University
      of Chicago Press. ISBN 978-0-226-07574-7.
  53. ^ a bLandsberg, P.T. (1984). "Is Equilibrium always an Entropy Maximum?".
      J. Stat. Physics. 35 (1â2): 159â169. Bibcode:1984JSP....35..159L.
      doi:10.1007/bf01017372.
  54. ^ a bLandsberg, P.T. (1984). "Can Entropy and "Order" Increase
      Together?". Physics Letters. 102A (4): 171â173. Bibcode:
      1984PhLA..102..171L. doi:10.1016/0375-9601(84)90934-4.
  55. ^Lambert, Frank L. "A_Student's_Approach_to_the_Second_Law_and_Entropy".
      entropysite.oxy.edu. Archived from the_original on 17 July 2009.
      Retrieved 22 May 2016.
  56. ^Watson, J.R.; Carson, E.M. (May 2002). "Undergraduate_students'
      understandings_of_entropy_and_Gibbs_free_energy" (PDF). University
      Chemistry Education. 6 (1): 4. ISSN 1369-5614.
  57. ^Lambert, Frank L. (February 2002). "Disorder â A Cracked Crutch for
      Supporting Entropy Discussions". Journal of Chemical Education. 79 (2):
      187. Bibcode:2002JChEd..79..187L. doi:10.1021/ed079p187.
  58. ^Atkins, Peter (1984). The Second Law. Scientific American Library.
      ISBN 978-0-7167-5004-8.
  59. ^Sandra Saary (23 February 1993). "Book_Review_of_"A_Science
      Miscellany"". Khaleej Times. UAE: Galadari Press: xi.
  60. ^Lathia, R; Agrawal, T; Parmar, V; Dobariya, K; Patel, A (20 October
      2015). "Heat_Death_(The_Ultimate_Fate_of_the_Universe)". doi:10.13140/
      rg.2.1.4158.2485.
  61. ^Lieb, Elliott H.; Yngvason, Jakob (March 1999). "The physics and
      mathematics of the second law of thermodynamics". Physics Reports. 310
      (1): 1â96. arXiv:cond-mat/9708200. Bibcode:1999PhR...310....1L. doi:
      10.1016/S0370-1573(98)00082-9.
  62. ^CarathÃ©odory, C. (September 1909). "Untersuchungen Ã¼ber die Grundlagen
      der Thermodynamik". Mathematische Annalen (in German). 67 (3): 355â386.
      doi:10.1007/BF01450409.
  63. ^R. Giles (2016). Mathematical_Foundations_of_Thermodynamics:
      International_Series_of_Monographs_on_Pure_and_Applied_Mathematics.
      Elsevier Science. ISBN 978-1-4831-8491-3.
  64. ^ M. Tribus, E.C. McIrvine, âEnergy and informationâ, Scientific
      American, 224 (September 1971), pp. 178â184
  65. ^Balian, Roger (2004). "Entropy, a Protean concept". In Dalibard, Jean
      (ed.). PoincarÃ© Seminar 2003: Bose-Einstein condensation â entropy.
      Basel: BirkhÃ¤user. pp. 119â144. ISBN 978-3-7643-7116-6.
  66. ^Brillouin, Leon (1956). Science and Information Theory. ISBN 978-0-486-
      43918-1.
  67. ^Georgescu-Roegen, Nicholas (1971). The Entropy Law and the Economic
      Process. Harvard University Press. ISBN 978-0-674-25781-8.
  68. ^Chen, Jing (2005). The Physical Foundation of Economics â an
      Analytical Thermodynamic Theory. World Scientific. ISBN 978-981-256-323-
      1.
  69. ^Kalinin, M.I.; Kononogov, S.A. (2005). "Boltzmann's constant".
      Measurement Techniques. 48 (7): 632â636. doi:10.1007/s11018-005-0195-9.
  70. ^Ben-Naim, Arieh (2008). Entropy demystified the second law reduced to
      plain common sense (Expanded ed.). Singapore: World Scientific.
      ISBN 9789812832269.
  71. ^"Edwin_T._Jaynes_â_Bibliography". Bayes.wustl.edu. 2 March 1998.
      Retrieved 6 December 2009.
  72. ^ Schneider, Tom, DELILA system (Deoxyribonucleic acid Library Language),
      (Information Theory Analysis of binding sites), Laboratory of
      Mathematical Biology, National Cancer Institute, Frederick, MD
  73. ^Schroeder, Daniel V. (2000). An introduction to thermal physics (
      [Nachdr.] ed.). San Francisco, CA [u.a.]: Addison Wesley. p. 88.
      ISBN 978-0-201-38027-9.
  74. ^"Measuring_Entropy". www.chem.wisc.edu.
  75. ^Brooks, Daniel, R.; Wiley, E.O. (1988). Evolution as Entropyâ Towards
      a Unified Theory of Biology. University_of_Chicago_Press. ISBN 978-0-226-
      07574-7.
  76. ^Avery, John (2003). Information Theory and Evolution. World Scientific.
      ISBN 978-981-238-399-0.
  77. ^Yockey, Hubert, P. (2005). Information Theory, Evolution, and the Origin
      of Life. Cambridge University Press. ISBN 978-0-521-80293-2.
  78. ^Chiavazzo, Eliodoro; Fasano, Matteo; Asinari, Pietro (2013). "Inference
      of analytical thermodynamic models for biological networks". Physica A:
      Statistical Mechanics and its Applications. 392 (5): 1122â1132.
      Bibcode:2013PhyA..392.1122C. doi:10.1016/j.physa.2012.11.030.
  79. ^Chen, Jing (2015). The Unity of Science and Economics: A New Foundation
      of Economic Theory. https://www.springer.com/us/book/9781493934645:
      Springer.
  80. ^Chiavazzo, Eliodoro; Isaia, Marco; Mammola, Stefano; Lepore, Emiliano;
      Ventola, Luigi; Asinari, Pietro; Pugno, Nicola Maria (2015). "Cave
      spiders_choose_optimal_environmental_factors_with_respect_to_the
      generated_entropy_when_laying_their_cocoon". Scientific Reports. 5: 7611.
      Bibcode:2015NatSR...5E7611C. doi:10.1038/srep07611. PMC 5154591.
      PMID 25556697.
  81. ^ IUPAC, Compendium_of_Chemical_Terminology, 2nd ed. (the "Gold Book")
      (1997). Online corrected version:  (2006–) "Entropy_unit". doi:10.1351/
      goldbook.E02151
  82. ^Thanos, Dimitrios; Li, Wentian; Provata, Astero (1 March 2018).
      "Entropic fluctuations in DNA sequences". Physica A: Statistical
      Mechanics and its Applications. 493: 444â457. doi:10.1016/
      j.physa.2017.11.119. ISSN 0378-4371.
  83. ^von Baeyer, Christian, H. (2003). Informationâthe_New_Language_of
      Science. Harvard University Press. ISBN 978-0-674-01387-2.
  84. Srednicki M (August 1993). "Entropy and area". Phys. Rev. Lett. 71 (5):
      666â669. arXiv:hep-th/9303048. Bibcode:1993PhRvL..71..666S. doi:
      10.1103/PhysRevLett.71.666. PMID 10055336.
  85. Callaway_DJE (April 1996). "Surface tension, hydrophobicity, and black
      holes: The entropic connection". Phys. Rev. E. 53 (4): 3738â3744.
      arXiv:cond-mat/9601111. Bibcode:1996PhRvE..53.3738C. doi:10.1103/
      PhysRevE.53.3738. PMID 9964684.
  86. ^Layzer, David (1988). Growth of Order in the Universe. MIT Press.
  87. ^Chaisson, Eric J. (2001). Cosmic_Evolution:_The_Rise_of_Complexity_in
      Nature. Harvard University Press. ISBN 978-0-674-00342-2.
  88. ^Lineweaver, Charles H.; Davies, Paul C. W.; Ruse, Michael, eds. (2013).
      Complexity and the Arrow of Time. Cambridge University Press. ISBN 978-1-
      107-02725-1.
  89. ^Stenger, Victor J. (2007). God: The Failed Hypothesis. Prometheus Books.
      ISBN 978-1-59102-481-1.
  90. ^Benjamin Gal-Or (1987). Cosmology, Physics and Philosophy. Springer
      Verlag. ISBN 978-0-387-96526-0.
  91. ^Albrecht,_Andreas (2004). "Cosmic_inflation_and_the_arrow_of_time"
      (PDF). In Barrow,_John_D.; Davies,_Paul_C.W.; Harper, Charles L. Jr.
      (eds.). Science and Ultimate Reality: From Quantum to Cosmos. Cambridge,
      UK: Cambridge University Press. arXiv:astro-ph/0210527. Bibcode:
      2002astro.ph.10527A. Retrieved 28 June 2017
  92.  (in honor of John Wheelerâs 90th birthday)
  93. ^Georgescu-Roegen,_Nicholas (1971). The_Entropy_Law_and_the_Economic
      Process (Full book accessible at Scribd). Cambridge, MA: Harvard
      University Press. ISBN 978-0-674-25780-1.
  94. ^Cleveland,_Cutler_J.; Ruth, Matthias (1997). "When, where, and by how
      much do biophysical limits constrain the economic process? A survey of
      Nicholas Georgescu-Roegen's contribution to ecological economics".
      Ecological_Economics. Amsterdam: Elsevier. 22 (3): 203â223. doi:
      10.1016/s0921-8009(97)00079-7.
  95. ^Daly,_Herman_E.; Farley, Joshua (2011). Ecological_Economics._Principles
      and_Applications (PDF contains full book) (2nd ed.). Washington: Island
      Press. ISBN 978-1-59726-681-9.
  96. ^Schmitz, John E.J. (2007). The_Second_Law_of_Life:_Energy,_Technology,
      and_the_Future_of_Earth_As_We_Know_It (Link to the author's science blog,
      based on his textbook). Norwich: William Andrew Publishing. ISBN 978-0-
      8155-1537-1.
  97. ^Ayres,_Robert_U. (2007). "On_the_practical_limits_to_substitution"
      (PDF). Ecological_Economics. Amsterdam: Elsevier. 61: 115â128. doi:
      10.1016/j.ecolecon.2006.02.011.
  98. ^Kerschner, Christian (2010). "Economic_de-growth_vs._steady-state
      economy" (PDF). Journal_of_Cleaner_Production. Amsterdam: Elsevier. 18
      (6): 544â551. doi:10.1016/j.jclepro.2009.10.019.
  99. ^ Daly,_Herman_E. (2015). "Economics_for_a_Full_World". Great Transition
      Initiative. Retrieved 23 November 2016.
 100. ^ Umberto Eco, Opera aperta. Forma e indeterminazione nelle poetiche
      contemporanee, Bompiani 2013
 101. ^ Arianna Beatrice Fabbricatore. (2017). La Querelle des Pantomimes.
      Danse, culture et sociÃ©tÃ© dans lâEurope des LumiÃ¨res. Rennes:
      Presses universitaires de Rennes.
 102. ^ Arianna Beatrice Fabbricatore. (2018). Lâaction dans le texte. Pour
      une approche hermÃ©neutique du Trattato teorico-prattico di Ballo (1779)
      de G. Magri. [Ressource ARDP 2015], Pantin, CN D.
 103. ^"HDDanse_272". Hypotheses.
 104. ^"Laction_dans_le_texte_CND_fabbricatore" (PDF). Hypotheses. March 2019.
      pp. 1â115.
***** Further reading[edit] *****
    * Adam, Gerhard; Otto_Hittmair (1992). WÃ¤rmetheorie. Vieweg, Braunschweig.
      ISBN 978-3-528-33311-9.
Atkins, Peter; Julio De Paula (2006). Physical Chemistry (8th ed.). Oxford
University Press. ISBN 978-0-19-870072-2.
Baierlein, Ralph (2003). Thermal Physics. Cambridge University Press. ISBN 978-
0-521-65838-6.
Ben-Naim, Arieh (2007). Entropy Demystified. World Scientific. ISBN 978-981-
270-055-1.
Callen, Herbert, B (2001). Thermodynamics and an Introduction to
Thermostatistics (2nd ed.). John Wiley and Sons. ISBN 978-0-471-86256-7.
Chang, Raymond (1998). Chemistry (6th ed.). New York: McGraw Hill. ISBN 978-0-
07-115221-1.
Cutnell, John, D.; Johnson, Kenneth, J. (1998). Physics (4th ed.). John Wiley
and Sons, Inc. ISBN 978-0-471-19113-1.
Dugdale, J. S. (1996). Entropy and its Physical Meaning (2nd ed.). Taylor and
Francis (UK); CRC (US). ISBN 978-0-7484-0569-5.
Fermi,_Enrico (1937). Thermodynamics. Prentice Hall. ISBN 978-0-486-60361-2.
Goldstein, Martin; Inge, F (1993). The Refrigerator and the Universe. Harvard
University Press. ISBN 978-0-674-75325-9.
Gyftopoulos, E.P.; G.P. Beretta (2010). Thermodynamics. Foundations and
Applications. Dover. ISBN 978-0-486-43932-7.
Haddad, Wassim M.; Chellaboina, VijaySekhar; Nersesov, Sergey G. (2005).
Thermodynamics â A Dynamical Systems Approach. Princeton_University_Press.
ISBN 978-0-691-12327-1.
Kroemer, Herbert; Charles Kittel (1980). Thermal Physics (2nd ed.). W. H.
Freeman Company. ISBN 978-0-7167-1088-2.
Lambert, Frank L.; entropysite.oxy.edu
MÃ¼ller-Kirsten,_Harald_J._W. (2013). Basics of Statistical Physics (2nd ed.).
Singapore: World Scientific. ISBN 978-981-4449-53-3.
Penrose,_Roger (2005). The_Road_to_Reality:_A_Complete_Guide_to_the_Laws_of_the
Universe. New York: A. A. Knopf. ISBN 978-0-679-45443-4.
Reif, F. (1965). Fundamentals_of_statistical_and_thermal_physics. McGraw-Hill.
ISBN 978-0-07-051800-1.
Schroeder, Daniel V. (2000). Introduction to Thermal Physics. New York: Addison
Wesley Longman. ISBN 978-0-201-38027-9.
Serway, Raymond, A. (1992). Physics_for_Scientists_and_Engineers. Saunders
Golden Subburst Series. ISBN 978-0-03-096026-0.
Spirax-Sarco Limited, Entropy_â_A_Basic_Understanding A primer on entropy
tables for steam engineering
vonBaeyer; Hans Christian (1998). Maxwell's_Demon:_Why_Warmth_Disperses_and
Time_Passes. Random House. ISBN 978-0-679-43342-2.
***** External links[edit] *****
 Look up entropy in Wiktionary, the free dictionary.
 Wikibooks has a book on the topic of: Entropy_for_Beginners
 Wikibooks has a book on the topic of: An_Intuitive_Guide_to_the_Concept_of
 Entropy_Arising_in_Various_Sectors_of_Science
    * Entropy_and_the_Second_Law_of_Thermodynamics â an A-level physics
      lecture with detailed derivation of entropy based on Carnot cycle
    * Khan Academy: entropy lectures, part of Chemistry_playlist
          o Proof:_S_(or_Entropy)_is_a_valid_state_variable
          o Thermodynamic_Entropy_Definition_Clarification
          o Reconciling_Thermodynamic_and_State_Definitions_of_Entropy
          o Entropy_Intuition
          o More_on_Entropy
    * The_Second_Law_of_Thermodynamics_and_Entropy â Yale OYC lecture, part
      of Fundamentals of Physics I (PHYS 200)
    * Entropy_and_the_Clausius_inequality MIT OCW lecture, part of 5.60
      Thermodynamics & Kinetics, Spring 2008
    * The_Discovery_of_Entropy by Adam Shulman. Hour-long video, January 2013.
    * Moriarty, Philip; Merrifield, Michael (2009). "S_Entropy". Sixty Symbols.
      Brady_Haran for the University_of_Nottingham.
"Entropy" at Scholarpedia
    * v
    * t
    * e
Statistical_mechanics
Theory                         * Principle_of_maximum_entropy
                               * ergodic_theory
                               * Ensembles
                               * partition_functions
                               * equations_of_state
                               * thermodynamic_potential:
Statistical_thermodynamics           o U
                                     o H
                                     o F
                                     o G
                               * Maxwell_relations
                               * Ferromagnetism_models
                                     o Ising
                                     o Potts
Models                               o Heisenberg
                                     o percolation
                               * Particles with force_field
                                     o depletion_force
                                     o Lennard-Jones_potential
                               * Boltzmann_equation
                               * H-theorem
Mathematical approaches        * Vlasov_equation
                               * BBGKY_hierarchy
                               * stochastic_process
                               * mean_field_theory and conformal_field_theory
                               * Phase_transition
Critical_phenomena             * Critical_exponents
                                     o correlation_length
                                     o size_scaling
                               * Boltzmann
                               * Shannon
Entropy                        * Tsallis
                               * RÃ©nyi
                               * von_Neumann
                               * Statistical_field_theory
                                     o elementary_particle
                                     o superfluidity
Applications                   * condensed_matter_physics
                               * complex_system
                                     o chaos
                                     o information_theory
                                     o Boltzmann_machine
Authority_control [Edit_this_at_Wikidata]     * GND: 4014894-4
                                              * NDL: 00562019

Retrieved from "https://en.wikipedia.org/w/
index.php?title=Entropy&oldid=909414864"
Categories:
    * Entropy
    * Concepts_in_physics
    * Philosophy_of_thermal_and_statistical_physics
    * State_functions
    * Asymmetry
Hidden categories:
    * CS1_German-language_sources_(de)
    * Articles_with_separate_introductions
    * Use_dmy_dates_from_March_2017
    * All_articles_with_unsourced_statements
    * Articles_with_unsourced_statements_from_February_2019
    * Articles_with_unsourced_statements_from_December_2018
    * Articles_containing_German-language_text
    * Articles_containing_Ancient_Greek-language_text
    * Articles_with_unsourced_statements_from_November_2018
    * Articles_with_unsourced_statements_from_October_2016
    * Wikipedia_articles_with_GND_identifiers
    * Wikipedia_articles_with_NDL_identifiers
***** Navigation menu *****
**** Personal tools ****
    * Not logged in
    * Talk
    * Contributions
    * Create_account
    * Log_in
**** Namespaces ****
    * Article
    * Talk
⁰
**** Variants ****
**** Views ****
    * Read
    * Edit
    * View_history
⁰
**** More ****
**** Search ****
[Unknown INPUT type][Search][Go]
**** Navigation ****
    * Main_page
    * Contents
    * Featured_content
    * Current_events
    * Random_article
    * Donate_to_Wikipedia
    * Wikipedia_store
**** Interaction ****
    * Help
    * About_Wikipedia
    * Community_portal
    * Recent_changes
    * Contact_page
**** Tools ****
    * What_links_here
    * Related_changes
    * Upload_file
    * Special_pages
    * Permanent_link
    * Page_information
    * Wikidata_item
    * Cite_this_page
**** In other projects ****
    * Wikimedia_Commons
**** Print/export ****
    * Create_a_book
    * Download_as_PDF
    * Printable_version
**** Languages ****
    * Afrikaans
    * á áá­á
    * Ø§ÙØ¹Ø±Ø¨ÙØ©
    * Asturianu
    * AzÉrbaycanca
    * à¦¬à¦¾à¦à¦²à¦¾
    * BÃ¢n-lÃ¢m-gÃº
    * ÐÐµÐ»Ð°ÑÑÑÐºÐ°Ñ
    * ÐÐµÐ»Ð°ÑÑÑÐºÐ°Ñ_(ÑÐ°ÑÐ°ÑÐºÐµÐ²ÑÑÐ°)â
    * ÐÑÐ»Ð³Ð°ÑÑÐºÐ¸
    * Boarisch
    * Bosanski
    * Brezhoneg
    * CatalÃ 
    * ÄeÅ¡tina
    * Dansk
    * Deutsch
    * Eesti
    * ÎÎ»Î»Î·Î½Î¹ÎºÎ¬
    * EspaÃ±ol
    * Esperanto
    * Euskara
    * ÙØ§Ø±Ø³Û
    * FranÃ§ais
    * Gaeilge
    * Gaelg
    * Galego
    * íêµ­ì´
    * ÕÕ¡ÕµÕ¥ÖÕ¥Õ¶
    * à¤¹à¤¿à¤¨à¥à¤¦à¥
    * Hrvatski
    * Bahasa_Hulontalo
    * Bahasa_Indonesia
    * Interlingua
    * Italiano
    * ×¢××¨××ª
    * à²à²¨à³à²¨à²¡
    * á¥áá áá£áá
    * ÒÐ°Ð·Ð°ÒÑÐ°
    * KreyÃ²l_ayisyen
    * ÐÑÑÐ³ÑÐ·ÑÐ°
    * Latina
    * LatvieÅ¡u
    * LietuviÅ³
    * Lumbaart
    * Magyar
    * ÐÐ°ÐºÐµÐ´Ð¾Ð½ÑÐºÐ¸
    * à´®à´²à´¯à´¾à´³à´
    * ÐÐ¾Ð½Ð³Ð¾Ð»
    * áá¼ááºáá¬áá¬áá¬
    * Nederlands
    * æ¥æ¬èª
    * Norsk
    * Norsk_nynorsk
    * Occitan
    * OÊ»zbekcha/ÑÐ·Ð±ÐµÐºÑÐ°
    * à¨ªà©°à¨à¨¾à¨¬à©
    * Ù¾ÚØªÙ
    * Polski
    * PortuguÃªs
    * RomÃ¢nÄ
    * Ð ÑÑÑÐºÐ¸Ð¹
    * Sardu
    * Scots
    * Sicilianu
    * Simple_English
    * SlovenÄina
    * SlovenÅ¡Äina
    * Ð¡ÑÐ¿ÑÐºÐ¸_/_srpski
    * Srpskohrvatski_/_ÑÑÐ¿ÑÐºÐ¾ÑÑÐ²Ð°ÑÑÐºÐ¸
    * Suomi
    * Svenska
    * à®¤à®®à®¿à®´à¯
    * à°¤à±à°²à±à°à±
    * à¹à¸à¸¢
    * TÃ¼rkÃ§e
    * Ð£ÐºÑÐ°ÑÐ½ÑÑÐºÐ°
    * Tiáº¿ng_Viá»t
    * æè¨
    * å´è¯­
    * ××Ö´×××©
    * ç²µèª
    * ä¸­æ
Edit_links
    * This page was last edited on 5 August 2019, at 08:36 (UTC).
    * Text is available under the Creative_Commons_Attribution-ShareAlike
      License; additional terms may apply. By using this site, you agree to the
      Terms_of_Use and Privacy_Policy. WikipediaÂ® is a registered trademark of
      the Wikimedia_Foundation,_Inc., a non-profit organization.
    * Privacy_policy
    * About_Wikipedia
    * Disclaimers
    * Contact_Wikipedia
    * Developers
    * Cookie_statement
    * Mobile_view
    * [Wikimedia_Foundation]
    * [Powered_by_MediaWiki]
