The following text has been accessed from https://en.wikipedia.org/wiki/Variance at Fri Aug 9 04:01:25 IST 2019
Creative_Commons_Attribution-ShareAlike_License





















****** Variance ******
From Wikipedia, the free encyclopedia
Jump_to_navigation Jump_to_search
This article is about the mathematical concept. For other uses, see Variance_
(disambiguation).
Statistical measure
Example of samples from two populations with the same mean but different
variances. The red population has mean 100 and variance 100 (SD=10) while the
blue population has mean 100 and variance 2500 (SD=50).
In probability_theory and statistics, variance is the expectation of the
squared deviation of a random_variable from its mean. Informally, it measures
how far a set of (random) numbers are spread out from their average value.
Variance has a central role in statistics, where some ideas that use it include
descriptive_statistics, statistical_inference, hypothesis_testing, goodness_of
fit, and Monte_Carlo_sampling. Variance is an important tool in the sciences,
where statistical analysis of data is common. The variance is the square of the
standard_deviation, the second central_moment of a distribution, and the
covariance of the random variable with itself, and it is often represented by
&#x03C3;  2     {\displaystyle \sigma ^{2}}  [\sigma ^{2}],      s  2
{\displaystyle s^{2}}  [s^{2}], or     Var &#x2061; ( X )   {\displaystyle
\operatorname {Var} (X)}  [\operatorname {Var} (X)].
⁰
***** Contents *****
    * 1_Definition
          o 1.1_Discrete_random_variable
          o 1.2_Absolutely_continuous_random_variable
    * 2_Examples
          o 2.1_Normal_distribution
          o 2.2_Exponential_distribution
          o 2.3_Poisson_distribution
          o 2.4_Binomial_distribution
          o 2.5_Fair_die
    * 3_Properties
          o 3.1_Basic_properties
          o 3.2_Issues_of_finiteness
          o 3.3_Sum_of_uncorrelated_variables_(BienaymÃ©_formula)
          o 3.4_Sum_of_variables
                # 3.4.1_With_Correlation_and_fixed_sample_size
                # 3.4.2_I.i.d._with_random_sample_size
          o 3.5_Matrix_notation_for_the_variance_of_a_linear_combination
          o 3.6_Weighted_sum_of_variables
          o 3.7_Product_of_independent_variables
          o 3.8_Product_of_statistically_dependent_variables
          o 3.9_Decomposition
          o 3.10_Formulae_for_the_variance
          o 3.11_Calculation_from_the_CDF
          o 3.12_Characteristic_property
          o 3.13_Units_of_measurement
    * 4_Approximating_the_variance_of_a_function
    * 5_Population_variance_and_sample_variance
          o 5.1_Population_variance
          o 5.2_Sample_variance
          o 5.3_Distribution_of_the_sample_variance
          o 5.4_Samuelson's_inequality
          o 5.5_Relations_with_the_harmonic_and_arithmetic_means
    * 6_Tests_of_equality_of_variances
    * 7_History
    * 8_Moment_of_inertia
    * 9_Semivariance
    * 10_Generalizations
          o 10.1_For_complex_variables
          o 10.2_For_vector-valued_random_variables
                # 10.2.1_As_a_matrix
                # 10.2.2_As_a_scalar
    * 11_See_also
    * 12_References
***** Definition[edit] *****
The variance of a random variable     X   {\displaystyle X}  [X] is the
expected_value of the squared deviation from the mean of     X   {\displaystyle
X}  [X],     &#x03BC; = E &#x2061; [ X ]   {\displaystyle \mu =\operatorname
{E} [X]}  [{\displaystyle \mu =\operatorname {E} [X]}]:
         Var &#x2061; ( X ) = E &#x2061;  [  ( X &#x2212; &#x03BC;  )  2    ]
      .   {\displaystyle \operatorname {Var} (X)=\operatorname {E} \left[(X-\mu
      )^{2}\right].}  [\operatorname {Var} (X)=\operatorname {E} \left[(X-\mu
      )^{2}\right].]
This definition encompasses random variables that are generated by processes
that are discrete, continuous, neither, or mixed. The variance can also be
thought of as the covariance of a random variable with itself:
         Var &#x2061; ( X ) = Cov &#x2061; ( X , X ) .   {\displaystyle
      \operatorname {Var} (X)=\operatorname {Cov} (X,X).}  [\operatorname {Var}
      (X)=\operatorname {Cov} (X,X).]
The variance is also equivalent to the second cumulant of a probability
distribution that generates     X   {\displaystyle X}  [X]. The variance is
typically designated as     Var &#x2061; ( X )   {\displaystyle \operatorname
{Var} (X)}  [\operatorname {Var} (X)],      &#x03C3;  X   2     {\displaystyle
\sigma _{X}^{2}}  [\sigma _{X}^{2}], or simply      &#x03C3;  2
{\displaystyle \sigma ^{2}}  [\sigma ^{2}] (pronounced "sigma squared"). The
expression for the variance can be expanded:
             Var &#x2061; ( X )    = E &#x2061;  [  ( X &#x2212; E &#x2061; [ X
      ]  )  2    ]        = E &#x2061;  [   X  2   &#x2212; 2 X E &#x2061; [ X
      ] + E &#x2061; [ X  ]  2    ]        = E &#x2061;  [  X  2   ]  &#x2212;
      2 E &#x2061; [ X ] E &#x2061; [ X ] + E &#x2061; [ X  ]  2         = E
      &#x2061;  [  X  2   ]  &#x2212; E &#x2061; [ X  ]  2
      {\displaystyle {\begin{aligned}\operatorname {Var} (X)&=\operatorname {E}
      \left[(X-\operatorname {E} [X])^{2}\right]\\[4pt]&=\operatorname {E}
      \left[X^{2}-2X\operatorname {E} [X]+\operatorname {E} [X]^{2}\right]\\
      [4pt]&=\operatorname {E} \left[X^{2}\right]-2\operatorname {E}
      [X]\operatorname {E} [X]+\operatorname {E} [X]^{2}\\[4pt]&=\operatorname
      {E} \left[X^{2}\right]-\operatorname {E} [X]^{2}\end{aligned}}}  [
      {\displaystyle {\begin{aligned}\operatorname {Var} (X)&=\operatorname {E}
      \left[(X-\operatorname {E} [X])^{2}\right]\\[4pt]&=\operatorname {E}
      \left[X^{2}-2X\operatorname {E} [X]+\operatorname {E} [X]^{2}\right]\\
      [4pt]&=\operatorname {E} \left[X^{2}\right]-2\operatorname {E}
      [X]\operatorname {E} [X]+\operatorname {E} [X]^{2}\\[4pt]&=\operatorname
      {E} \left[X^{2}\right]-\operatorname {E} [X]^{2}\end{aligned}}}]
In other words, the variance of X is equal to the mean of the square of X minus
the square of the mean of X. This equation should not be used for computations
using floating_point_arithmetic because it suffers from catastrophic
cancellation if the two components of the equation are similar in magnitude.
There exist numerically_stable_alternatives.
**** Discrete random variable[edit] ****
If the generator of random variable     X   {\displaystyle X}  [X] is discrete
with probability_mass_function      x  1   &#x21A6;  p  1   ,  x  2   &#x21A6;
p  2   , &#x2026; ,  x  n   &#x21A6;  p  n     {\displaystyle x_{1}\mapsto p_
{1},x_{2}\mapsto p_{2},\ldots ,x_{n}\mapsto p_{n}}  [{\displaystyle x_
{1}\mapsto p_{1},x_{2}\mapsto p_{2},\ldots ,x_{n}\mapsto p_{n}}] then
         Var &#x2061; ( X ) =  &#x2211;  i = 1   n    p  i   &#x22C5; (  x  i
      &#x2212; &#x03BC;  )  2   ,   {\displaystyle \operatorname {Var} (X)=\sum
      _{i=1}^{n}p_{i}\cdot (x_{i}-\mu )^{2},}  [\operatorname {Var} (X)=\sum _
      {i=1}^{n}p_{i}\cdot (x_{i}-\mu )^{2},]
or equivalently
         Var &#x2061; ( X ) =  (   &#x2211;  i = 1   n    p  i    x  i   2    )
      &#x2212;  &#x03BC;  2   ,   {\displaystyle \operatorname {Var} (X)=\left
      (\sum _{i=1}^{n}p_{i}x_{i}^{2}\right)-\mu ^{2},}  [{\displaystyle
      \operatorname {Var} (X)=\left(\sum _{i=1}^{n}p_{i}x_{i}^{2}\right)-\mu ^
      {2},}]
where     &#x03BC;   {\displaystyle \mu }  [\mu ] is the expected value, i.e.
         &#x03BC; =  &#x2211;  i = 1   n    p  i    x  i   .   {\displaystyle
      \mu =\sum _{i=1}^{n}p_{i}x_{i}.}  [{\displaystyle \mu =\sum _{i=1}^{n}p_
      {i}x_{i}.}]
(When such a discrete weighted_variance is specified by weights whose sum is
not 1, then one divides by the sum of the weights.)
The variance of a set of     n   {\displaystyle n}  [n] equally likely values
can be written as
         Var &#x2061; ( X ) =   1 n    &#x2211;  i = 1   n   (  x  i   &#x2212;
      &#x03BC;  )  2   ,   {\displaystyle \operatorname {Var} (X)={\frac {1}
      {n}}\sum _{i=1}^{n}(x_{i}-\mu )^{2},}  [{\displaystyle \operatorname
      {Var} (X)={\frac {1}{n}}\sum _{i=1}^{n}(x_{i}-\mu )^{2},}]
where     &#x03BC;   {\displaystyle \mu }  [\mu ] is the average value, i.e.,
         &#x03BC; =   1 n    &#x2211;  i = 1   n    x  i   .   {\displaystyle
      \mu ={\frac {1}{n}}\sum _{i=1}^{n}x_{i}.}  [{\displaystyle \mu ={\frac
      {1}{n}}\sum _{i=1}^{n}x_{i}.}]
The variance of a set of     n   {\displaystyle n}  [n] equally likely values
can be equivalently expressed, without directly referring to the mean, in terms
of squared deviations of all points from each other:[1]
         Var &#x2061; ( X ) =   1  n  2      &#x2211;  i = 1   n    &#x2211;  j
      = 1   n     1 2   (  x  i   &#x2212;  x  j    )  2   =   1  n  2
      &#x2211;  i    &#x2211;  j > i   (  x  i   &#x2212;  x  j    )  2   .
      {\displaystyle \operatorname {Var} (X)={\frac {1}{n^{2}}}\sum _{i=1}^
      {n}\sum _{j=1}^{n}{\frac {1}{2}}(x_{i}-x_{j})^{2}={\frac {1}{n^{2}}}\sum
      _{i}\sum _{j>i}(x_{i}-x_{j})^{2}.}  [\operatorname {Var} (X)={\frac {1}
      {n^{2}}}\sum _{i=1}^{n}\sum _{j=1}^{n}{\frac {1}{2}}(x_{i}-x_{j})^{2}=
      {\frac {1}{n^{2}}}\sum _{i}\sum _{j>i}(x_{i}-x_{j})^{2}.]
**** Absolutely continuous random variable[edit] ****
If the random variable     X   {\displaystyle X}  [X] has a probability_density
function     f ( x )   {\displaystyle f(x)}  [f(x)], and     F ( x )
{\displaystyle F(x)}  [F(x)] is the corresponding cumulative_distribution
function, then
             Var &#x2061; ( X ) =  &#x03C3;  2      =  &#x222B;   R    ( x
      &#x2212; &#x03BC;  )  2   f ( x )  d x       =  &#x222B;   R     x  2   f
      ( x )  d x &#x2212; 2 &#x03BC;  &#x222B;   R    x f ( x )  d x +
      &#x222B;   R     &#x03BC;  2   f ( x )  d x       =  &#x222B;   R     x
      2    d F ( x ) &#x2212; 2 &#x03BC;  &#x222B;   R    x  d F ( x ) +
      &#x03BC;  2    &#x222B;   R     d F ( x )       =  &#x222B;   R     x  2
      d F ( x ) &#x2212; 2 &#x03BC; &#x22C5; &#x03BC; +  &#x03BC;  2   &#x22C5;
      1       =  &#x222B;   R     x  2    d F ( x ) &#x2212;  &#x03BC;  2   ,
      {\displaystyle {\begin{aligned}\operatorname {Var} (X)=\sigma ^{2}&=\int
      _{\mathbb {R} }(x-\mu )^{2}f(x)\,dx\\[4pt]&=\int _{\mathbb {R} }x^{2}f
      (x)\,dx-2\mu \int _{\mathbb {R} }xf(x)\,dx+\int _{\mathbb {R} }\mu ^{2}f
      (x)\,dx\\[4pt]&=\int _{\mathbb {R} }x^{2}\,dF(x)-2\mu \int _{\mathbb {R}
      }x\,dF(x)+\mu ^{2}\int _{\mathbb {R} }\,dF(x)\\[4pt]&=\int _{\mathbb {R}
      }x^{2}\,dF(x)-2\mu \cdot \mu +\mu ^{2}\cdot 1\\[4pt]&=\int _{\mathbb {R}
      }x^{2}\,dF(x)-\mu ^{2},\end{aligned}}}  [{\displaystyle {\begin
      {aligned}\operatorname {Var} (X)=\sigma ^{2}&=\int _{\mathbb {R} }(x-\mu
      )^{2}f(x)\,dx\\[4pt]&=\int _{\mathbb {R} }x^{2}f(x)\,dx-2\mu \int _
      {\mathbb {R} }xf(x)\,dx+\int _{\mathbb {R} }\mu ^{2}f(x)\,dx\\[4pt]&=\int
      _{\mathbb {R} }x^{2}\,dF(x)-2\mu \int _{\mathbb {R} }x\,dF(x)+\mu ^
      {2}\int _{\mathbb {R} }\,dF(x)\\[4pt]&=\int _{\mathbb {R} }x^{2}\,dF(x)-
      2\mu \cdot \mu +\mu ^{2}\cdot 1\\[4pt]&=\int _{\mathbb {R} }x^{2}\,dF(x)-
      \mu ^{2},\end{aligned}}}]
or equivalently,
         Var &#x2061; ( X ) =  &#x222B;   R     x  2   f ( x )  d x &#x2212;
      &#x03BC;  2   ,   {\displaystyle \operatorname {Var} (X)=\int _{\mathbb
      {R} }x^{2}f(x)\,dx-\mu ^{2},}  [{\displaystyle \operatorname {Var}
      (X)=\int _{\mathbb {R} }x^{2}f(x)\,dx-\mu ^{2},}]
where     &#x03BC;   {\displaystyle \mu }  [\mu ] is the expected value of
X   {\displaystyle X}  [X] given by
         &#x03BC; =  &#x222B;   R    x f ( x )  d x =  &#x222B;   R    x  d F
      ( x ) .   {\displaystyle \mu =\int _{\mathbb {R} }xf(x)\,dx=\int _
      {\mathbb {R} }x\,dF(x).}  [{\displaystyle \mu =\int _{\mathbb {R} }xf
      (x)\,dx=\int _{\mathbb {R} }x\,dF(x).}]
In these formulas, the integrals with respect to     d x   {\displaystyle dx}
[dx] and     d F ( x )   {\displaystyle dF(x)}  [{\displaystyle dF(x)}] are
Lebesgue and LebesgueâStieltjes integrals, respectively.
If the function      x  2   f ( x )   {\displaystyle x^{2}f(x)}  [
{\displaystyle x^{2}f(x)}] is Riemann-integrable on every finite interval
[ a , b ] &#x2282;  R  ,   {\displaystyle [a,b]\subset \mathbb {R} ,}  [
{\displaystyle [a,b]\subset \mathbb {R} ,}] then
         Var &#x2061; ( X ) =  &#x222B;  &#x2212; &#x221E;   + &#x221E;    x  2
      f ( x )  d x &#x2212;  &#x03BC;  2   ,   {\displaystyle \operatorname
      {Var} (X)=\int _{-\infty }^{+\infty }x^{2}f(x)\,dx-\mu ^{2},}  [
      {\displaystyle \operatorname {Var} (X)=\int _{-\infty }^{+\infty }x^{2}f
      (x)\,dx-\mu ^{2},}]
where the integral is an improper_Riemann_integral.
***** Examples[edit] *****
**** Normal distribution[edit] ****
The normal_distribution with parameters     &#x03BC;   {\displaystyle \mu }
[\mu ] and     &#x03C3;   {\displaystyle \sigma }  [\sigma ] is a continuous
distribution (also known as gaussian distribution) whose probability_density
function is given by
         f ( x ) =   1  2 &#x03C0;  &#x03C3;  2       e  &#x2212;    ( x
      &#x2212; &#x03BC;  )  2     2  &#x03C3;  2        .   {\displaystyle f
      (x)={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}e^{-{\frac {(x-\mu )^{2}}
      {2\sigma ^{2}}}}.}  [
      f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{ -\frac{(x-\mu)^2}{2\sigma^2} }.
      ]
In this distribution,     E &#x2061; [ X ] = &#x03BC;   {\displaystyle
\operatorname {E} [X]=\mu }  [{\displaystyle \operatorname {E} [X]=\mu }] and
the variance     Var &#x2061; ( X )   {\displaystyle \operatorname {Var} (X)}
[\operatorname {Var} (X)] is related with     &#x03C3;   {\displaystyle \sigma
}  [\sigma ] via
         Var &#x2061; ( X ) =  &#x222B;  &#x2212; &#x221E;   &#x221E;      x  2
      2 &#x03C0;  &#x03C3;  2       e  &#x2212;    ( x &#x2212; &#x03BC;  )  2
      2  &#x03C3;  2         d x &#x2212;  &#x03BC;  2   =  &#x03C3;  2   .
      {\displaystyle \operatorname {Var} (X)=\int _{-\infty }^{\infty }{\frac
      {x^{2}}{\sqrt {2\pi \sigma ^{2}}}}e^{-{\frac {(x-\mu )^{2}}{2\sigma ^
      {2}}}}\,dx-\mu ^{2}=\sigma ^{2}.}  [{\displaystyle \operatorname {Var}
      (X)=\int _{-\infty }^{\infty }{\frac {x^{2}}{\sqrt {2\pi \sigma ^{2}}}}e^
      {-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}}\,dx-\mu ^{2}=\sigma ^{2}.}]
The role of the normal distribution in the central_limit_theorem is in part
responsible for the prevalence of the variance in probability and statistics.
**** Exponential distribution[edit] ****
The exponential_distribution with parameter     &#x03BB;   {\displaystyle
\lambda }  [\lambda ] is a continuous distribution whose support is the semi-
infinite interval     [ 0 , &#x221E; )   {\displaystyle [0,\infty )}  [
[0,\infty )]. Its probability_density_function is given by
         f ( x ) = &#x03BB;  e  &#x2212; &#x03BB; x     {\displaystyle f
      (x)=\lambda e^{-\lambda x}}  [{\displaystyle f(x)=\lambda e^{-\lambda
      x}}]
and it has expected value     &#x03BC; =  &#x03BB;  &#x2212; 1
{\displaystyle \mu =\lambda ^{-1}}  [{\displaystyle \mu =\lambda ^{-1}}]. The
variance is equal to
         Var &#x2061; ( X ) =  &#x222B;  0   &#x221E;    x  2   &#x03BB;  e
      &#x2212; &#x03BB; x    d x &#x2212;  &#x03BC;  2   =  &#x03BB;  &#x2212;
      2   .   {\displaystyle \operatorname {Var} (X)=\int _{0}^{\infty }x^
      {2}\lambda e^{-\lambda x}\,dx-\mu ^{2}=\lambda ^{-2}.}  [{\displaystyle
      \operatorname {Var} (X)=\int _{0}^{\infty }x^{2}\lambda e^{-\lambda
      x}\,dx-\mu ^{2}=\lambda ^{-2}.}]
So for an exponentially distributed random variable,      &#x03C3;  2   =
&#x03BC;  2   .   {\displaystyle \sigma ^{2}=\mu ^{2}.}  [{\displaystyle \sigma
^{2}=\mu ^{2}.}]
**** Poisson distribution[edit] ****
The Poisson_distribution with parameter     &#x03BB;   {\displaystyle \lambda }
[\lambda ] is a discrete distribution for     k = 0 , 1 , 2 , &#x2026;
{\displaystyle k=0,1,2,\ldots }  [{\displaystyle k=0,1,2,\ldots }]. Its
probability_mass_function is given by
         p ( k ) =    &#x03BB;  k    k !     e  &#x2212; &#x03BB;   ,
      {\displaystyle p(k)={\frac {\lambda ^{k}}{k!}}e^{-\lambda },}  [p(k)=
      {\frac {\lambda ^{k}}{k!}}e^{-\lambda },]
and it has expected value     &#x03BC; = &#x03BB;   {\displaystyle \mu =\lambda
}  [{\displaystyle \mu =\lambda }]. The variance is equal to
         Var &#x2061; ( X ) =  (   &#x2211;  k = 0   &#x221E;    k  2
      &#x03BB;  k    k !     e  &#x2212; &#x03BB;    )  &#x2212;  &#x03BC;  2
      = &#x03BB; ,   {\displaystyle \operatorname {Var} (X)=\left(\sum _{k=0}^
      {\infty }k^{2}{\frac {\lambda ^{k}}{k!}}e^{-\lambda }\right)-\mu ^
      {2}=\lambda ,}  [{\displaystyle \operatorname {Var} (X)=\left(\sum _
      {k=0}^{\infty }k^{2}{\frac {\lambda ^{k}}{k!}}e^{-\lambda }\right)-\mu ^
      {2}=\lambda ,}]
So for a Poisson-distributed random variable,      &#x03C3;  2   = &#x03BC;
{\displaystyle \sigma ^{2}=\mu }  [{\displaystyle \sigma ^{2}=\mu }].
**** Binomial distribution[edit] ****
The binomial_distribution with parameters     n   {\displaystyle n}  [n] and
p   {\displaystyle p}  [p] is a discrete distribution for     k = 0 , 1 , 2 ,
&#x2026; , n   {\displaystyle k=0,1,2,\ldots ,n}  [{\displaystyle
k=0,1,2,\ldots ,n}]. Its probability_mass_function is given by
         p ( k ) =    (   n k   )     p  k   ( 1 &#x2212; p  )  n &#x2212; k
      ,   {\displaystyle p(k)={n \choose k}p^{k}(1-p)^{n-k},}  [p(k)={n \choose
      k}p^{k}(1-p)^{n-k},]
and it has expected value     &#x03BC; = n p   {\displaystyle \mu =np}  [\mu
=np]. The variance is equal to
         Var &#x2061; ( X ) =  (   &#x2211;  k = 0   n    k  2      (   n k   )
      p  k   ( 1 &#x2212; p  )  n &#x2212; k    )  &#x2212;  &#x03BC;  2   = n
      p ( 1 &#x2212; p ) .   {\displaystyle \operatorname {Var} (X)=\left(\sum
      _{k=0}^{n}k^{2}{n \choose k}p^{k}(1-p)^{n-k}\right)-\mu ^{2}=np(1-p).}  [
      {\displaystyle \operatorname {Var} (X)=\left(\sum _{k=0}^{n}k^{2}{n
      \choose k}p^{k}(1-p)^{n-k}\right)-\mu ^{2}=np(1-p).}]
As a simple example, the binomial distribution with     p = 1  /  2
{\displaystyle p=1/2}  [p=1/2] describes the probability of getting     k
{\displaystyle k}  [k] heads in     n   {\displaystyle n}  [n] tosses of a fair
coin. Thus the expected value of the number of heads is     n  /  2 ,
{\displaystyle n/2,}  [{\displaystyle n/2,}] and the variance is     n  /  4.
{\displaystyle n/4.}  [{\displaystyle n/4.}]
**** Fair die[edit] ****
A fair six-sided_die can be modeled as a discrete random variable, X, with
outcomes 1 through 6, each with equal probability 1/6. The expected value of X
is     ( 1 + 2 + 3 + 4 + 5 + 6 )  /  6 = 7  /  2.   {\displaystyle
(1+2+3+4+5+6)/6=7/2.}  [{\displaystyle (1+2+3+4+5+6)/6=7/2.}] Therefore, the
variance of X is
             Var &#x2061; ( X )    =  &#x2211;  i = 1   6     1 6     (  i
      &#x2212;   7 2    )   2         =   1 6    (  ( &#x2212; 5  /  2  )  2
      + ( &#x2212; 3  /  2  )  2   + ( &#x2212; 1  /  2  )  2   + ( 1  /  2  )
      2   + ( 3  /  2  )  2   + ( 5  /  2  )  2    )        =   35 12
      &#x2248; 2.92.       {\displaystyle {\begin{aligned}\operatorname {Var}
      (X)&=\sum _{i=1}^{6}{\frac {1}{6}}\left(i-{\frac {7}{2}}\right)^{2}\\
      [5pt]&={\frac {1}{6}}\left((-5/2)^{2}+(-3/2)^{2}+(-1/2)^{2}+(1/2)^{2}+(3/
      2)^{2}+(5/2)^{2}\right)\\[5pt]&={\frac {35}{12}}\approx 2.92.\end
      {aligned}}}  [{\displaystyle {\begin{aligned}\operatorname {Var}
      (X)&=\sum _{i=1}^{6}{\frac {1}{6}}\left(i-{\frac {7}{2}}\right)^{2}\\
      [5pt]&={\frac {1}{6}}\left((-5/2)^{2}+(-3/2)^{2}+(-1/2)^{2}+(1/2)^{2}+(3/
      2)^{2}+(5/2)^{2}\right)\\[5pt]&={\frac {35}{12}}\approx 2.92.\end
      {aligned}}}]
The general formula for the variance of the outcome, X, of an n-sided die is
             Var &#x2061; ( X )    = E &#x2061; (  X  2   ) &#x2212; ( E
      &#x2061; ( X )  )  2         =   1 n    &#x2211;  i = 1   n    i  2
      &#x2212;   (    1 n    &#x2211;  i = 1   n   i  )   2         =    ( n +
      1 ) ( 2 n + 1 )  6   &#x2212;   (    n + 1  2   )   2         =     n  2
      &#x2212; 1  12   .       {\displaystyle {\begin{aligned}\operatorname
      {Var} (X)&=\operatorname {E} (X^{2})-(\operatorname {E} (X))^{2}\\[5pt]&=
      {\frac {1}{n}}\sum _{i=1}^{n}i^{2}-\left({\frac {1}{n}}\sum _{i=1}^
      {n}i\right)^{2}\\[5pt]&={\frac {(n+1)(2n+1)}{6}}-\left({\frac {n+1}
      {2}}\right)^{2}\\[4pt]&={\frac {n^{2}-1}{12}}.\end{aligned}}}  [
      {\displaystyle {\begin{aligned}\operatorname {Var} (X)&=\operatorname {E}
      (X^{2})-(\operatorname {E} (X))^{2}\\[5pt]&={\frac {1}{n}}\sum _{i=1}^
      {n}i^{2}-\left({\frac {1}{n}}\sum _{i=1}^{n}i\right)^{2}\\[5pt]&={\frac {
      (n+1)(2n+1)}{6}}-\left({\frac {n+1}{2}}\right)^{2}\\[4pt]&={\frac {n^{2}-
      1}{12}}.\end{aligned}}}]
***** Properties[edit] *****
**** Basic properties[edit] ****
Variance is non-negative because the squares are positive or zero:
         Var &#x2061; ( X ) &#x2265; 0.   {\displaystyle \operatorname {Var}
      (X)\geq 0.}  [\operatorname {Var} (X)\geq 0.]
The variance of a constant random variable is zero, and if the variance of a
variable in a data_set is 0, then all the entries have the same value:
         P ( X = a ) = 1  &#x27FA;  Var &#x2061; ( X ) = 0.   {\displaystyle P
      (X=a)=1\iff \operatorname {Var} (X)=0.}  [{\displaystyle P(X=a)=1\iff
      \operatorname {Var} (X)=0.}]
Variance is invariant with respect to changes in a location_parameter. That is,
if a constant is added to all values of the variable, the variance is
unchanged:
         Var &#x2061; ( X + a ) = Var &#x2061; ( X ) .   {\displaystyle
      \operatorname {Var} (X+a)=\operatorname {Var} (X).}  [\operatorname {Var}
      (X+a)=\operatorname {Var} (X).]
If all values are scaled by a constant, the variance is scaled by the square of
that constant:
         Var &#x2061; ( a X ) =  a  2   Var &#x2061; ( X ) .   {\displaystyle
      \operatorname {Var} (aX)=a^{2}\operatorname {Var} (X).}  [\operatorname
      {Var} (aX)=a^{2}\operatorname {Var} (X).]
The variance of a sum of two random variables is given by
         Var &#x2061; ( a X + b Y ) =  a  2   Var &#x2061; ( X ) +  b  2   Var
      &#x2061; ( Y ) + 2 a b  Cov &#x2061; ( X , Y ) ,   {\displaystyle
      \operatorname {Var} (aX+bY)=a^{2}\operatorname {Var} (X)+b^
      {2}\operatorname {Var} (Y)+2ab\,\operatorname {Cov} (X,Y),}
      [\operatorname {Var} (aX+bY)=a^{2}\operatorname {Var} (X)+b^
      {2}\operatorname {Var} (Y)+2ab\,\operatorname {Cov} (X,Y),]
         Var &#x2061; ( a X &#x2212; b Y ) =  a  2   Var &#x2061; ( X ) +  b  2
      Var &#x2061; ( Y ) &#x2212; 2 a b  Cov &#x2061; ( X , Y ) ,
      {\displaystyle \operatorname {Var} (aX-bY)=a^{2}\operatorname {Var}
      (X)+b^{2}\operatorname {Var} (Y)-2ab\,\operatorname {Cov} (X,Y),}
      [\operatorname {Var} (aX-bY)=a^{2}\operatorname {Var} (X)+b^
      {2}\operatorname {Var} (Y)-2ab\,\operatorname {Cov} (X,Y),]
where Cov(â, â) is the covariance. In general we have for the sum of     N
{\displaystyle N}  [N] random variables     {  X  1   , &#x2026; ,  X  N   }
{\displaystyle \{X_{1},\dots ,X_{N}\}}  [\{X_{1},\dots ,X_{N}\}]:
         Var &#x2061;  (   &#x2211;  i = 1   N    X  i    )  =  &#x2211;  i , j
      = 1   N   Cov &#x2061; (  X  i   ,  X  j   ) =  &#x2211;  i = 1   N   Var
      &#x2061; (  X  i   ) +  &#x2211;  i &#x2260; j   Cov &#x2061; (  X  i   ,
      X  j   ) .   {\displaystyle \operatorname {Var} \left(\sum _{i=1}^{N}X_
      {i}\right)=\sum _{i,j=1}^{N}\operatorname {Cov} (X_{i},X_{j})=\sum _
      {i=1}^{N}\operatorname {Var} (X_{i})+\sum _{i\neq j}\operatorname {Cov}
      (X_{i},X_{j}).}  [\operatorname {Var} \left(\sum _{i=1}^{N}X_
      {i}\right)=\sum _{i,j=1}^{N}\operatorname {Cov} (X_{i},X_{j})=\sum _
      {i=1}^{N}\operatorname {Var} (X_{i})+\sum _{i\neq j}\operatorname {Cov}
      (X_{i},X_{j}).]
These results lead to the variance of a linear_combination as:
             Var &#x2061;  (   &#x2211;  i = 1   N    a  i    X  i    )     =
      &#x2211;  i , j = 1   N    a  i    a  j   Cov &#x2061; (  X  i   ,  X  j
      )       =  &#x2211;  i = 1   N    a  i   2   Var &#x2061; (  X  i   ) +
      &#x2211;  i &#x2260; j    a  i    a  j   Cov &#x2061; (  X  i   ,  X  j
      )       =  &#x2211;  i = 1   N    a  i   2   Var &#x2061; (  X  i   ) + 2
      &#x2211;  1 &#x2264; i < j &#x2264; N    a  i    a  j   Cov &#x2061; (  X
      i   ,  X  j   ) .       {\displaystyle {\begin{aligned}\operatorname
      {Var} \left(\sum _{i=1}^{N}a_{i}X_{i}\right)&=\sum _{i,j=1}^{N}a_{i}a_
      {j}\operatorname {Cov} (X_{i},X_{j})\\&=\sum _{i=1}^{N}a_{i}^
      {2}\operatorname {Var} (X_{i})+\sum _{i\not =j}a_{i}a_{j}\operatorname
      {Cov} (X_{i},X_{j})\\&=\sum _{i=1}^{N}a_{i}^{2}\operatorname {Var} (X_
      {i})+2\sum _{1\leq i<j\leq N}a_{i}a_{j}\operatorname {Cov} (X_{i},X_
      {j}).\end{aligned}}}  [{\begin{aligned}\operatorname {Var} \left(\sum _
      {i=1}^{N}a_{i}X_{i}\right)&=\sum _{i,j=1}^{N}a_{i}a_{j}\operatorname
      {Cov} (X_{i},X_{j})\\&=\sum _{i=1}^{N}a_{i}^{2}\operatorname {Var} (X_
      {i})+\sum _{i\not =j}a_{i}a_{j}\operatorname {Cov} (X_{i},X_{j})\\&=\sum
      _{i=1}^{N}a_{i}^{2}\operatorname {Var} (X_{i})+2\sum _{1\leq i<j\leq N}a_
      {i}a_{j}\operatorname {Cov} (X_{i},X_{j}).\end{aligned}}]
If the random variables      X  1   , &#x2026; ,  X  N     {\displaystyle X_
{1},\dots ,X_{N}}  [X_{1},\dots ,X_{N}] are such that
         Cov &#x2061; (  X  i   ,  X  j   ) = 0 &#xA0; , &#xA0; &#x2200; &#xA0;
      ( i &#x2260; j ) ,   {\displaystyle \operatorname {Cov} (X_{i},X_{j})=0\
      ,\ \forall \ (i\neq j),}  [\operatorname {Cov} (X_{i},X_{j})=0\ ,\
      \forall \ (i\neq j),]
they are said to be uncorrelated. It follows immediately from the expression
given earlier that if the random variables      X  1   , &#x2026; ,  X  N
{\displaystyle X_{1},\dots ,X_{N}}  [X_{1},\dots ,X_{N}] are uncorrelated, then
the variance of their sum is equal to the sum of their variances, or, expressed
symbolically:
         Var &#x2061;  (   &#x2211;  i = 1   N    X  i    )  =  &#x2211;  i = 1
      N   Var &#x2061; (  X  i   ) .   {\displaystyle \operatorname {Var} \left
      (\sum _{i=1}^{N}X_{i}\right)=\sum _{i=1}^{N}\operatorname {Var} (X_{i}).}
      [\operatorname {Var} \left(\sum _{i=1}^{N}X_{i}\right)=\sum _{i=1}^
      {N}\operatorname {Var} (X_{i}).]
Since independent_random_variables_are_always_uncorrelated, the equation above
holds in particular when the random variables      X  1   , &#x2026; ,  X  n
{\displaystyle X_{1},\dots ,X_{n}}  [X_{1},\dots ,X_{n}] are independent. Thus
independence is sufficient but not necessary for the variance of the sum to
equal the sum of the variances.
**** Issues of finiteness[edit] ****
If a distribution does not have a finite expected value, as is the case for the
Cauchy_distribution, then the variance cannot be finite either. However, some
distributions may not have a finite variance despite their expected value being
finite. An example is a Pareto_distribution whose index     k   {\displaystyle
k}  [k] satisfies     1 < k &#x2264; 2.   {\displaystyle 1<k\leq 2.}  [
{\displaystyle 1<k\leq 2.}]
**** Sum of uncorrelated variables (BienaymÃ© formula)[edit] ****
See also: Sum_of_normally_distributed_random_variables
One reason for the use of the variance in preference to other measures of
dispersion is that the variance of the sum (or the difference) of uncorrelated
random variables is the sum of their variances:
         Var &#x2061;  (   &#x2211;  i = 1   n    X  i    )  =  &#x2211;  i = 1
      n   Var &#x2061; (  X  i   ) .   {\displaystyle \operatorname {Var} \left
      (\sum _{i=1}^{n}X_{i}\right)=\sum _{i=1}^{n}\operatorname {Var} (X_{i}).}
      [{\displaystyle \operatorname {Var} \left(\sum _{i=1}^{n}X_
      {i}\right)=\sum _{i=1}^{n}\operatorname {Var} (X_{i}).}]
This statement is called the BienaymÃ© formula[2] and was discovered in 1853.
[3][4] It is often made with the stronger condition that the variables are
independent, but being uncorrelated suffices. So if all the variables have the
same variance Ï2, then, since division by n is a linear transformation, this
formula immediately implies that the variance of their mean is
         Var &#x2061;  (   X &#x00AF;   )  = Var &#x2061;  (    1 n    &#x2211;
      i = 1   n    X  i    )  =   1  n  2      &#x2211;  i = 1   n   Var
      &#x2061;  (  X  i   )  =   1  n  2     n  &#x03C3;  2   =    &#x03C3;  2
      n   .   {\displaystyle \operatorname {Var} \left({\overline
      {X}}\right)=\operatorname {Var} \left({\frac {1}{n}}\sum _{i=1}^{n}X_
      {i}\right)={\frac {1}{n^{2}}}\sum _{i=1}^{n}\operatorname {Var} \left(X_
      {i}\right)={\frac {1}{n^{2}}}n\sigma ^{2}={\frac {\sigma ^{2}}{n}}.}  [
      {\displaystyle \operatorname {Var} \left({\overline
      {X}}\right)=\operatorname {Var} \left({\frac {1}{n}}\sum _{i=1}^{n}X_
      {i}\right)={\frac {1}{n^{2}}}\sum _{i=1}^{n}\operatorname {Var} \left(X_
      {i}\right)={\frac {1}{n^{2}}}n\sigma ^{2}={\frac {\sigma ^{2}}{n}}.}]
That is, the variance of the mean decreases when n increases. This formula for
the variance of the mean is used in the definition of the standard_error of the
sample mean, which is used in the central_limit_theorem.
To prove the initial statement, it suffices to show that
         Var &#x2061; ( X + Y ) = Var &#x2061; ( X ) + Var &#x2061; ( Y ) .
      {\displaystyle \operatorname {Var} (X+Y)=\operatorname {Var}
      (X)+\operatorname {Var} (Y).}  [{\displaystyle \operatorname {Var}
      (X+Y)=\operatorname {Var} (X)+\operatorname {Var} (Y).}]
The general result then follows by induction. Starting with the definition,
             Var &#x2061; ( X + Y )    = E &#x2061; [ ( X + Y  )  2   ]
      &#x2212; ( E &#x2061; [ X + Y ]  )  2         = E &#x2061; [  X  2   + 2
      X Y +  Y  2   ] &#x2212; ( E &#x2061; [ X ] + E &#x2061; [ Y ]  )  2   .
      {\displaystyle {\begin{aligned}\operatorname {Var} (X+Y)&=\operatorname
      {E} [(X+Y)^{2}]-(\operatorname {E} [X+Y])^{2}\\[5pt]&=\operatorname {E}
      [X^{2}+2XY+Y^{2}]-(\operatorname {E} [X]+\operatorname {E} [Y])^{2}.\end
      {aligned}}}  [{\displaystyle {\begin{aligned}\operatorname {Var}
      (X+Y)&=\operatorname {E} [(X+Y)^{2}]-(\operatorname {E} [X+Y])^{2}\\
      [5pt]&=\operatorname {E} [X^{2}+2XY+Y^{2}]-(\operatorname {E}
      [X]+\operatorname {E} [Y])^{2}.\end{aligned}}}]
Using the linearity of the expectation operator and the assumption of
independence (or uncorrelatedness) of X and Y, this further simplifies as
follows:
             Var &#x2061; ( X + Y )    = E &#x2061; [  X  2   ] + 2 E &#x2061;
      [ X Y ] + E &#x2061; [  Y  2   ] &#x2212; ( E &#x2061; [ X  ]  2   + 2 E
      &#x2061; [ X ] E &#x2061; [ Y ] + E &#x2061; [ Y  ]  2   )       = E
      &#x2061; [  X  2   ] + E &#x2061; [  Y  2   ] &#x2212; E &#x2061; [ X  ]
      2   &#x2212; E &#x2061; [ Y  ]  2         = Var &#x2061; ( X ) + Var
      &#x2061; ( Y ) .       {\displaystyle {\begin{aligned}\operatorname {Var}
      (X+Y)&=\operatorname {E} [X^{2}]+2\operatorname {E} [XY]+\operatorname
      {E} [Y^{2}]-(\operatorname {E} [X]^{2}+2\operatorname {E}
      [X]\operatorname {E} [Y]+\operatorname {E} [Y]^{2})\\[5pt]&=\operatorname
      {E} [X^{2}]+\operatorname {E} [Y^{2}]-\operatorname {E} [X]^{2}-
      \operatorname {E} [Y]^{2}\\[5pt]&=\operatorname {Var} (X)+\operatorname
      {Var} (Y).\end{aligned}}}  [{\displaystyle {\begin{aligned}\operatorname
      {Var} (X+Y)&=\operatorname {E} [X^{2}]+2\operatorname {E}
      [XY]+\operatorname {E} [Y^{2}]-(\operatorname {E} [X]^{2}+2\operatorname
      {E} [X]\operatorname {E} [Y]+\operatorname {E} [Y]^{2})\\
      [5pt]&=\operatorname {E} [X^{2}]+\operatorname {E} [Y^{2}]-\operatorname
      {E} [X]^{2}-\operatorname {E} [Y]^{2}\\[5pt]&=\operatorname {Var}
      (X)+\operatorname {Var} (Y).\end{aligned}}}]
**** Sum of variables[edit] ****
*** With Correlation and fixed sample size[edit] ***
In general the variance of the sum of n variables is the sum of their
covariances:
         Var &#x2061;  (   &#x2211;  i = 1   n    X  i    )  =  &#x2211;  i = 1
      n    &#x2211;  j = 1   n   Cov &#x2061; (  X  i   ,  X  j   ) =  &#x2211;
      i = 1   n   Var &#x2061; (  X  i   ) + 2  &#x2211;  1 &#x2264; i < j
      &#x2264; n   Cov &#x2061; (  X  i   ,  X  j   ) .   {\displaystyle
      \operatorname {Var} \left(\sum _{i=1}^{n}X_{i}\right)=\sum _{i=1}^{n}\sum
      _{j=1}^{n}\operatorname {Cov} (X_{i},X_{j})=\sum _{i=1}^{n}\operatorname
      {Var} (X_{i})+2\sum _{1\leq i<j\leq n}\operatorname {Cov} (X_{i},X_{j}).}
      [{\displaystyle \operatorname {Var} \left(\sum _{i=1}^{n}X_
      {i}\right)=\sum _{i=1}^{n}\sum _{j=1}^{n}\operatorname {Cov} (X_{i},X_
      {j})=\sum _{i=1}^{n}\operatorname {Var} (X_{i})+2\sum _{1\leq i<j\leq
      n}\operatorname {Cov} (X_{i},X_{j}).}]
(Note: The second equality comes from the fact that Cov(Xi,Xi) = Var(Xi).)
Here Cov(â, â) is the covariance, which is zero for independent random
variables (if it exists). The formula states that the variance of a sum is
equal to the sum of all elements in the covariance matrix of the components.
The next expression states equivalently that the variance of the sum is the sum
of the diagonal of covariance matrix plus two times the sum of its upper
triangular elements (or its lower triangular elements); this emphasizes that
the covariance matrix is symmetric. This formula is used in the theory of
Cronbach's_alpha in classical_test_theory.
So if the variables have equal variance Ï2 and the average correlation of
distinct variables is Ï, then the variance of their mean is
         Var &#x2061; (   X &#x00AF;   ) =    &#x03C3;  2   n   +    n &#x2212;
      1  n   &#x03C1;  &#x03C3;  2   .   {\displaystyle \operatorname {Var} (
      {\overline {X}})={\frac {\sigma ^{2}}{n}}+{\frac {n-1}{n}}\rho \sigma ^
      {2}.}  [\operatorname {Var} ({\overline {X}})={\frac {\sigma ^{2}}{n}}+
      {\frac {n-1}{n}}\rho \sigma ^{2}.]
This implies that the variance of the mean increases with the average of the
correlations. In other words, additional correlated observations are not as
effective as additional independent observations at reducing the uncertainty_of
the_mean. Moreover, if the variables have unit variance, for example if they
are standardized, then this simplifies to
         Var &#x2061; (   X &#x00AF;   ) =   1 n   +    n &#x2212; 1  n
      &#x03C1; .   {\displaystyle \operatorname {Var} ({\overline {X}})={\frac
      {1}{n}}+{\frac {n-1}{n}}\rho .}  [\operatorname {Var} ({\overline {X}})=
      {\frac {1}{n}}+{\frac {n-1}{n}}\rho .]
This formula is used in the SpearmanâBrown_prediction_formula of classical
test theory. This converges to Ï if n goes to infinity, provided that the
average correlation remains constant or converges too. So for the variance of
the mean of standardized variables with equal correlations or converging
average correlation we have
          lim  n &#x2192; &#x221E;   Var &#x2061; (   X &#x00AF;   ) = &#x03C1;
      .   {\displaystyle \lim _{n\to \infty }\operatorname {Var} ({\overline
      {X}})=\rho .}  [\lim _{n\to \infty }\operatorname {Var} ({\overline
      {X}})=\rho .]
Therefore, the variance of the mean of a large number of standardized variables
is approximately equal to their average correlation. This makes clear that the
sample mean of correlated variables does not generally converge to the
population mean, even though the law_of_large_numbers states that the sample
mean will converge for independent variables.
*** I.i.d. with random sample size[edit] ***
There are cases when a sample is taken without knowing, in advance, how many
observations will be acceptable according to some criterion. In such cases, the
sample size N is a random variable whose variation adds to the variation of X,
such that,
      Var(âX) = E(N)Var(X) + Var(N)E2(X).[5]
If N has a Poisson_distribution, then E(N) = Var(N) with estimator N=n. So, the
estimator of Var(âX) becomes nS2X + nXbar2 giving
      standard_error(Xbar) = â[(S2X + Xbar2)/n].
**** Matrix notation for the variance of a linear combination[edit] ****
Define     X   {\displaystyle X}  [X] as a column vector of     n
{\displaystyle n}  [n] random variables      X  1   , &#x2026; ,  X  n
{\displaystyle X_{1},\ldots ,X_{n}}  [X_{1},\ldots ,X_{n}], and     c
{\displaystyle c}  [c] as a column vector of     n   {\displaystyle n}  [n]
scalars      c  1   , &#x2026; ,  c  n     {\displaystyle c_{1},\ldots ,c_{n}}
[c_{1},\ldots ,c_{n}]. Therefore,      c  T   X   {\displaystyle c^{T}X}  [c^
{T}X] is a linear_combination of these random variables, where      c  T
{\displaystyle c^{T}}  [c^{T}] denotes the transpose of     c   {\displaystyle
c}  [c]. Also let     &#x03A3;   {\displaystyle \Sigma }  [\Sigma ] be the
covariance_matrix of     X   {\displaystyle X}  [X]. The variance of      c  T
X   {\displaystyle c^{T}X}  [c^{T}X] is then given by:[6]
         Var &#x2061; (  c  T   X ) =  c  T   &#x03A3; c .   {\displaystyle
      \operatorname {Var} (c^{T}X)=c^{T}\Sigma c.}  [\operatorname {Var} (c^
      {T}X)=c^{T}\Sigma c.]
**** Weighted sum of variables[edit] ****
Not to be confused with Weighted_variance.
The scaling property and the BienaymÃ© formula, along with the property of the
covariance Cov(aX, bY) = ab Cov(X, Y) jointly imply that
         Var &#x2061; ( a X &#x00B1; b Y ) =  a  2   Var &#x2061; ( X ) +  b  2
      Var &#x2061; ( Y ) &#x00B1; 2 a b  Cov &#x2061; ( X , Y ) .
      {\displaystyle \operatorname {Var} (aX\pm bY)=a^{2}\operatorname {Var}
      (X)+b^{2}\operatorname {Var} (Y)\pm 2ab\,\operatorname {Cov} (X,Y).}  [
      {\displaystyle \operatorname {Var} (aX\pm bY)=a^{2}\operatorname {Var}
      (X)+b^{2}\operatorname {Var} (Y)\pm 2ab\,\operatorname {Cov} (X,Y).}]
This implies that in a weighted sum of variables, the variable with the largest
weight will have a disproportionally large weight in the variance of the total.
For example, if X and Y are uncorrelated and the weight of X is two times the
weight of Y, then the weight of the variance of X will be four times the weight
of the variance of Y.
The expression above can be extended to a weighted sum of multiple variables:
         Var &#x2061;  (   &#x2211;  i   n    a  i    X  i    )  =  &#x2211;  i
      = 1   n    a  i   2   Var &#x2061; (  X  i   ) + 2  &#x2211;  1 &#x2264;
      i    &#x2211;  < j &#x2264; n    a  i    a  j   Cov &#x2061; (  X  i   ,
      X  j   )   {\displaystyle \operatorname {Var} \left(\sum _{i}^{n}a_{i}X_
      {i}\right)=\sum _{i=1}^{n}a_{i}^{2}\operatorname {Var} (X_{i})+2\sum _
      {1\leq i}\sum _{<j\leq n}a_{i}a_{j}\operatorname {Cov} (X_{i},X_{j})}
      [\operatorname {Var} \left(\sum _{i}^{n}a_{i}X_{i}\right)=\sum _{i=1}^
      {n}a_{i}^{2}\operatorname {Var} (X_{i})+2\sum _{1\leq i}\sum _{<j\leq
      n}a_{i}a_{j}\operatorname {Cov} (X_{i},X_{j})]
**** Product of independent variables[edit] ****
If two variables X and Y are independent, the variance of their product is
given by[7]
             Var &#x2061; ( X Y )    = [ E &#x2061; ( X )  ]  2   Var &#x2061;
      ( Y ) + [ E &#x2061; ( Y )  ]  2   Var &#x2061; ( X ) + Var &#x2061; ( X
      ) Var &#x2061; ( Y ) .       {\displaystyle {\begin{aligned}\operatorname
      {Var} (XY)&=[\operatorname {E} (X)]^{2}\operatorname {Var} (Y)+
      [\operatorname {E} (Y)]^{2}\operatorname {Var} (X)+\operatorname {Var}
      (X)\operatorname {Var} (Y).\end{aligned}}}  [{\displaystyle {\begin
      {aligned}\operatorname {Var} (XY)&=[\operatorname {E} (X)]^
      {2}\operatorname {Var} (Y)+[\operatorname {E} (Y)]^{2}\operatorname {Var}
      (X)+\operatorname {Var} (X)\operatorname {Var} (Y).\end{aligned}}}]
Equivalently, using the basic properties of expectation, it is given by
         Var &#x2061; ( X Y ) = E &#x2061; (  X  2   ) E &#x2061; (  Y  2   )
      &#x2212; [ E &#x2061; ( X )  ]  2   [ E &#x2061; ( Y )  ]  2   .
      {\displaystyle \operatorname {Var} (XY)=\operatorname {E} (X^
      {2})\operatorname {E} (Y^{2})-[\operatorname {E} (X)]^{2}[\operatorname
      {E} (Y)]^{2}.}  [{\displaystyle \operatorname {Var} (XY)=\operatorname
      {E} (X^{2})\operatorname {E} (Y^{2})-[\operatorname {E} (X)]^{2}
      [\operatorname {E} (Y)]^{2}.}]
**** Product of statistically dependent variables[edit] ****
In general, if two variables are statistically dependent, the variance of their
product is given by:
             Var &#x2061; ( X Y ) =     E &#x2061; [  X  2    Y  2   ] &#x2212;
      [ E &#x2061; ( X Y )  ]  2       =     Cov &#x2061; (  X  2   ,  Y  2   )
      + E &#x2061; (  X  2   ) E &#x2061; (  Y  2   ) &#x2212; [ E &#x2061; ( X
      Y )  ]  2       =     Cov &#x2061; (  X  2   ,  Y  2   ) + ( Var &#x2061;
      ( X ) + [ E &#x2061; ( X )  ]  2   ) ( Var &#x2061; ( Y ) + [ E &#x2061;
      ( Y )  ]  2   )         &#x2212; [ Cov &#x2061; ( X , Y ) + E &#x2061;
      ( X ) E &#x2061; ( Y )  ]  2         {\displaystyle {\begin
      {aligned}\operatorname {Var} (XY)={}&\operatorname {E} [X^{2}Y^{2}]-
      [\operatorname {E} (XY)]^{2}\\[5pt]={}&\operatorname {Cov} (X^{2},Y^
      {2})+\operatorname {E} (X^{2})\operatorname {E} (Y^{2})-[\operatorname
      {E} (XY)]^{2}\\[5pt]={}&\operatorname {Cov} (X^{2},Y^{2})+(\operatorname
      {Var} (X)+[\operatorname {E} (X)]^{2})(\operatorname {Var} (Y)+
      [\operatorname {E} (Y)]^{2})\\[5pt]&{}-[\operatorname {Cov}
      (X,Y)+\operatorname {E} (X)\operatorname {E} (Y)]^{2}\end{aligned}}}  [
      {\displaystyle {\begin{aligned}\operatorname {Var} (XY)={}&\operatorname
      {E} [X^{2}Y^{2}]-[\operatorname {E} (XY)]^{2}\\[5pt]={}&\operatorname
      {Cov} (X^{2},Y^{2})+\operatorname {E} (X^{2})\operatorname {E} (Y^{2})-
      [\operatorname {E} (XY)]^{2}\\[5pt]={}&\operatorname {Cov} (X^{2},Y^{2})+
      (\operatorname {Var} (X)+[\operatorname {E} (X)]^{2})(\operatorname {Var}
      (Y)+[\operatorname {E} (Y)]^{2})\\[5pt]&{}-[\operatorname {Cov}
      (X,Y)+\operatorname {E} (X)\operatorname {E} (Y)]^{2}\end{aligned}}}]
**** Decomposition[edit] ****
The general formula for variance decomposition or the law_of_total_variance is:
If     X   {\displaystyle X}  [X] and     Y   {\displaystyle Y}  [Y] are two
random variables, and the variance of     X   {\displaystyle X}  [X] exists,
then
         Var &#x2061; [ X ] = E &#x2061; ( Var &#x2061; [ X &#x2223; Y ] ) +
      Var &#x2061; ( E &#x2061; [ X &#x2223; Y ] ) .   {\displaystyle
      \operatorname {Var} [X]=\operatorname {E} (\operatorname {Var} [X\mid
      Y])+\operatorname {Var} (\operatorname {E} [X\mid Y]).}  [{\displaystyle
      \operatorname {Var} [X]=\operatorname {E} (\operatorname {Var} [X\mid
      Y])+\operatorname {Var} (\operatorname {E} [X\mid Y]).}]
The conditional_expectation     E &#x2061; ( X &#x2223; Y )   {\displaystyle
\operatorname {E} (X\mid Y)}  [{\displaystyle \operatorname {E} (X\mid Y)}] of
X   {\displaystyle X}  [X] given     Y   {\displaystyle Y}  [Y], and the
conditional_variance     Var &#x2061; ( X &#x2223; Y )   {\displaystyle
\operatorname {Var} (X\mid Y)}  [{\displaystyle \operatorname {Var} (X\mid Y)}]
may be understood as follows. Given any particular value y of the random
variable Y, there is a conditional expectation     E &#x2061; ( X &#x2223; Y =
y )   {\displaystyle \operatorname {E} (X\mid Y=y)}  [{\displaystyle
\operatorname {E} (X\mid Y=y)}] given the event Y = y. This quantity depends on
the particular value y; it is a function     g ( y ) = E &#x2061; ( X &#x2223;
Y = y )   {\displaystyle g(y)=\operatorname {E} (X\mid Y=y)}  [{\displaystyle g
(y)=\operatorname {E} (X\mid Y=y)}]. That same function evaluated at the random
variable Y is the conditional expectation     E &#x2061; ( X &#x2223; Y ) = g
( Y ) .   {\displaystyle \operatorname {E} (X\mid Y)=g(Y).}  [{\displaystyle
\operatorname {E} (X\mid Y)=g(Y).}]
In particular, if     Y   {\displaystyle Y}  [Y] is a discrete random variable
assuming possible values      y  1   ,  y  2   ,  y  3   &#x2026;
{\displaystyle y_{1},y_{2},y_{3}\ldots }  [{\displaystyle y_{1},y_{2},y_
{3}\ldots }] with corresponding probabilities      p  1   ,  p  2   ,  p  3
&#x2026; ,   {\displaystyle p_{1},p_{2},p_{3}\ldots ,}  [{\displaystyle p_
{1},p_{2},p_{3}\ldots ,}], then in the formula for total variance, the first
term on the right-hand side becomes
         E &#x2061; ( Var &#x2061; [ X &#x2223; Y ] ) =  &#x2211;  i    p  i
      &#x03C3;  i   2   ,   {\displaystyle \operatorname {E} (\operatorname
      {Var} [X\mid Y])=\sum _{i}p_{i}\sigma _{i}^{2},}  [{\displaystyle
      \operatorname {E} (\operatorname {Var} [X\mid Y])=\sum _{i}p_{i}\sigma _
      {i}^{2},}]
where      &#x03C3;  i   2   = Var &#x2061; [ X &#x2223; Y =  y  i   ]
{\displaystyle \sigma _{i}^{2}=\operatorname {Var} [X\mid Y=y_{i}]}  [
{\displaystyle \sigma _{i}^{2}=\operatorname {Var} [X\mid Y=y_{i}]}].
Similarly, the second term on the right-hand side becomes
         Var &#x2061; ( E &#x2061; [ X &#x2223; Y ] ) =  &#x2211;  i    p  i
      &#x03BC;  i   2   &#x2212;   (   &#x2211;  i    p  i    &#x03BC;  i    )
      2   =  &#x2211;  i    p  i    &#x03BC;  i   2   &#x2212;  &#x03BC;  2   ,
      {\displaystyle \operatorname {Var} (\operatorname {E} [X\mid Y])=\sum _
      {i}p_{i}\mu _{i}^{2}-\left(\sum _{i}p_{i}\mu _{i}\right)^{2}=\sum _{i}p_
      {i}\mu _{i}^{2}-\mu ^{2},}  [{\displaystyle \operatorname {Var}
      (\operatorname {E} [X\mid Y])=\sum _{i}p_{i}\mu _{i}^{2}-\left(\sum _
      {i}p_{i}\mu _{i}\right)^{2}=\sum _{i}p_{i}\mu _{i}^{2}-\mu ^{2},}]
where      &#x03BC;  i   = E &#x2061; [ X &#x2223; Y =  y  i   ]
{\displaystyle \mu _{i}=\operatorname {E} [X\mid Y=y_{i}]}  [{\displaystyle \mu
_{i}=\operatorname {E} [X\mid Y=y_{i}]}] and     &#x03BC; =  &#x2211;  i    p
i    &#x03BC;  i     {\displaystyle \mu =\sum _{i}p_{i}\mu _{i}}  [
{\displaystyle \mu =\sum _{i}p_{i}\mu _{i}}]. Thus the total variance is given
by
         Var &#x2061; [ X ] =  &#x2211;  i    p  i    &#x03C3;  i   2   +
      (   &#x2211;  i    p  i    &#x03BC;  i   2   &#x2212;  &#x03BC;  2    )
      .   {\displaystyle \operatorname {Var} [X]=\sum _{i}p_{i}\sigma _{i}^
      {2}+\left(\sum _{i}p_{i}\mu _{i}^{2}-\mu ^{2}\right).}  [{\displaystyle
      \operatorname {Var} [X]=\sum _{i}p_{i}\sigma _{i}^{2}+\left(\sum _{i}p_
      {i}\mu _{i}^{2}-\mu ^{2}\right).}]
A similar formula is applied in analysis_of_variance, where the corresponding
formula is
            M S    total   =    M S    between   +    M S    within   ;
      {\displaystyle {\mathit {MS}}_{\text{total}}={\mathit {MS}}_{\text
      {between}}+{\mathit {MS}}_{\text{within}};}  [{\mathit {MS}}_{\text
      {total}}={\mathit {MS}}_{\text{between}}+{\mathit {MS}}_{\text{within}};]
here       M S     {\displaystyle {\mathit {MS}}}  [{\mathit {MS}}] refers to
the Mean of the Squares. In linear_regression analysis the corresponding
formula is
            M S    total   =    M S    regression   +    M S    residual   .
      {\displaystyle {\mathit {MS}}_{\text{total}}={\mathit {MS}}_{\text
      {regression}}+{\mathit {MS}}_{\text{residual}}.}  [{\mathit {MS}}_{\text
      {total}}={\mathit {MS}}_{\text{regression}}+{\mathit {MS}}_{\text
      {residual}}.]
This can also be derived from the additivity of variances, since the total
(observed) score is the sum of the predicted score and the error score, where
the latter two are uncorrelated.
Similar decompositions are possible for the sum of squared deviations (sum of
squares,       S S     {\displaystyle {\mathit {SS}}}  [{\mathit {SS}}]):
            S S    total   =    S S    between   +    S S    within   ,
      {\displaystyle {\mathit {SS}}_{\text{total}}={\mathit {SS}}_{\text
      {between}}+{\mathit {SS}}_{\text{within}},}  [{\mathit {SS}}_{\text
      {total}}={\mathit {SS}}_{\text{between}}+{\mathit {SS}}_{\text{within}},]
            S S    total   =    S S    regression   +    S S    residual   .
      {\displaystyle {\mathit {SS}}_{\text{total}}={\mathit {SS}}_{\text
      {regression}}+{\mathit {SS}}_{\text{residual}}.}  [{\mathit {SS}}_{\text
      {total}}={\mathit {SS}}_{\text{regression}}+{\mathit {SS}}_{\text
      {residual}}.]
**** Formulae for the variance[edit] ****
Main articles: Algebraic_formula_for_the_variance and Algorithms_for
calculating_variance
A formula often used for deriving the variance of a theoretical distribution is
as follows:
         Var &#x2061; ( X ) = E &#x2061; (  X  2   ) &#x2212; ( E &#x2061; ( X
      )  )  2   .   {\displaystyle \operatorname {Var} (X)=\operatorname {E}
      (X^{2})-(\operatorname {E} (X))^{2}.}  [\operatorname {Var}
      (X)=\operatorname {E} (X^{2})-(\operatorname {E} (X))^{2}.]
This will be useful when it is possible to derive formulae for the expected
value and for the expected value of the square.
This formula is also sometimes used in connection with the sample variance.
While useful for hand calculations, it is not advised for computer calculations
as it suffers from catastrophic_cancellation if the two components of the
equation are similar in magnitude and floating point arithmetic is used. This
is discussed in the article Algorithms_for_calculating_variance.
**** Calculation from the CDF[edit] ****
The population variance for a non-negative random variable can be expressed in
terms of the cumulative_distribution_function F using
         2  &#x222B;  0   &#x221E;   u ( 1 &#x2212; F ( u ) )  d u &#x2212;
      (    &#x222B;  0   &#x221E;   ( 1 &#x2212; F ( u ) )  d u    )    2   .
      {\displaystyle 2\int _{0}^{\infty }u(1-F(u))\,du-{\Big (}\int _{0}^
      {\infty }(1-F(u))\,du{\Big )}^{2}.}  [{\displaystyle 2\int _{0}^{\infty
      }u(1-F(u))\,du-{\Big (}\int _{0}^{\infty }(1-F(u))\,du{\Big )}^{2}.}]
This expression can be used to calculate the variance in situations where the
CDF, but not the density, can be conveniently expressed.
**** Characteristic property[edit] ****
The second moment of a random variable attains the minimum value when taken
around the first moment (i.e., mean) of the random variable, i.e.       a r g m
i n   m     E   (   (  X &#x2212; m  )   2   )  =  E  ( X )   {\displaystyle
\mathrm {argmin} _{m}\,\mathrm {E} \left(\left(X-m\right)^{2}\right)=\mathrm
{E} (X)}  [{\displaystyle \mathrm {argmin} _{m}\,\mathrm {E} \left(\left(X-
m\right)^{2}\right)=\mathrm {E} (X)}]. Conversely, if a continuous function
&#x03C6;   {\displaystyle \varphi }  [\varphi ] satisfies       a r g m i n   m
E  ( &#x03C6; ( X &#x2212; m ) ) =  E  ( X )   {\displaystyle \mathrm {argmin}
_{m}\,\mathrm {E} (\varphi (X-m))=\mathrm {E} (X)}  [{\displaystyle \mathrm
{argmin} _{m}\,\mathrm {E} (\varphi (X-m))=\mathrm {E} (X)}] for all random
variables X, then it is necessarily of the form     &#x03C6; ( x ) = a  x  2
+ b   {\displaystyle \varphi (x)=ax^{2}+b}  [\varphi (x)=ax^{2}+b], where a >
0. This also holds in the multidimensional case.[8]
**** Units of measurement[edit] ****
Unlike expected absolute deviation, the variance of a variable has units that
are the square of the units of the variable itself. For example, a variable
measured in meters will have a variance measured in meters squared. For this
reason, describing data sets via their standard_deviation or root_mean_square
deviation is often preferred over using the variance. In the dice example the
standard deviation is â2.9 â 1.7, slightly larger than the expected
absolute deviation of 1.5.
The standard deviation and the expected absolute deviation can both be used as
an indicator of the "spread" of a distribution. The standard deviation is more
amenable to algebraic manipulation than the expected absolute deviation, and,
together with variance and its generalization covariance, is used frequently in
theoretical statistics; however the expected absolute deviation tends to be
more robust as it is less sensitive to outliers arising from measurement
anomalies or an unduly heavy-tailed_distribution.
***** Approximating the variance of a function[edit] *****
The delta_method uses second-order Taylor_expansions to approximate the
variance of a function of one or more random variables: see Taylor_expansions
for_the_moments_of_functions_of_random_variables. For example, the approximate
variance of a function of one variable is given by
               Var &#x2061;  [  f ( X )  ]  &#x2248;   (   f &#x2032;  ( E
            &#x2061;  [ X ]  )  )   2   Var &#x2061;  [ X ]    {\displaystyle
            \operatorname {Var} \left[f(X)\right]\approx \left(f'(\operatorname
            {E} \left[X\right])\right)^{2}\operatorname {Var} \left[X\right]}
            [\operatorname {Var} \left[f(X)\right]\approx \left(f'
            (\operatorname {E} \left[X\right])\right)^{2}\operatorname {Var}
            \left[X\right]]
provided that f is twice differentiable and that the mean and variance of X are
finite.
***** Population variance and sample variance[edit] *****
See also: Unbiased_estimation_of_standard_deviation
Real-world observations such as the measurements of yesterday's rain throughout
the day typically cannot be complete sets of all possible observations that
could be made. As such, the variance calculated from the finite set will in
general not match the variance that would have been calculated from the full
population of possible observations. This means that one estimates the mean and
variance that would have been calculated from an omniscient set of observations
by using an estimator equation. The estimator is a function of the sample of n
observations drawn without observational bias from the whole population of
potential observations. In this example that sample would be the set of actual
measurements of yesterday's rainfall from available rain gauges within the
geography of interest.
The simplest estimators for population mean and population variance are simply
the mean and variance of the sample, the sample mean and (uncorrected) sample
variance â these are consistent_estimators (they converge to the correct
value as the number of samples increases), but can be improved. Estimating the
population variance by taking the sample's variance is close to optimal in
general, but can be improved in two ways. Most simply, the sample variance is
computed as an average of squared_deviations about the (sample) mean, by
dividing by n. However, using values other than n improves the estimator in
various ways. Four common values for the denominator are n, n â 1, n + 1, and
n â 1.5: n is the simplest (population variance of the sample), n â 1
eliminates bias, n + 1 minimizes mean_squared_error for the normal
distribution, and n â 1.5 mostly eliminates bias in unbiased_estimation_of
standard_deviation for the normal distribution.
Firstly, if the omniscient mean is unknown (and is computed as the sample
mean), then the sample variance is a biased_estimator: it underestimates the
variance by a factor of (n â 1) / n; correcting by this factor (dividing by
n â 1 instead of n) is called Bessel's_correction. The resulting estimator is
unbiased, and is called the (corrected) sample variance or unbiased sample
variance. For example, when n = 1 the variance of a single observation about
the sample mean (itself) is obviously zero regardless of the population
variance. If the mean is determined in some other way than from the same
samples used to estimate the variance then this bias does not arise and the
variance can safely be estimated as that of the samples about the
(independently known) mean.
Secondly, the sample variance does not generally minimize mean_squared_error
between sample variance and population variance. Correcting for bias often
makes this worse: one can always choose a scale factor that performs better
than the corrected sample variance, though the optimal scale factor depends on
the excess_kurtosis of the population (see mean_squared_error:_variance), and
introduces bias. This always consists of scaling down the unbiased estimator
(dividing by a number larger than n â 1), and is a simple example of a
shrinkage_estimator: one "shrinks" the unbiased estimator towards zero. For the
normal distribution, dividing by n + 1 (instead of n â 1 or n) minimizes mean
squared error. The resulting estimator is biased, however, and is known as the
biased sample variation.
**** Population variance[edit] ****
In general, the population variance of a finite population of size N with
values xi is given by
              &#x03C3;  2      =   1 N    &#x2211;  i = 1   N     (   x  i
      &#x2212; &#x03BC;  )   2   =   1 N    &#x2211;  i = 1   N    (   x  i   2
      &#x2212; 2 &#x03BC;  x  i   +  &#x03BC;  2    )        =  (    1 N
      &#x2211;  i = 1   N    x  i   2    )  &#x2212; 2 &#x03BC;  (    1 N
      &#x2211;  i = 1   N    x  i    )  +  &#x03BC;  2         =  (    1 N
      &#x2211;  i = 1   N    x  i   2    )  &#x2212;  &#x03BC;  2
      {\displaystyle {\begin{aligned}\sigma ^{2}&={\frac {1}{N}}\sum _{i=1}^
      {N}\left(x_{i}-\mu \right)^{2}={\frac {1}{N}}\sum _{i=1}^{N}\left(x_{i}^
      {2}-2\mu x_{i}+\mu ^{2}\right)\\[5pt]&=\left({\frac {1}{N}}\sum _{i=1}^
      {N}x_{i}^{2}\right)-2\mu \left({\frac {1}{N}}\sum _{i=1}^{N}x_
      {i}\right)+\mu ^{2}\\[5pt]&=\left({\frac {1}{N}}\sum _{i=1}^{N}x_{i}^
      {2}\right)-\mu ^{2}\end{aligned}}}
      [{\displaystyle {\begin{aligned}\sigma ^{2}&={\frac {1}{N}}\sum _{i=1}^
      {N}\left(x_{i}-\mu \right)^{2}={\frac {1}{N}}\sum _{i=1}^{N}\left(x_{i}^
      {2}-2\mu x_{i}+\mu ^{2}\right)\\[5pt]&=\left({\frac {1}{N}}\sum _{i=1}^
      {N}x_{i}^{2}\right)-2\mu \left({\frac {1}{N}}\sum _{i=1}^{N}x_
      {i}\right)+\mu ^{2}\\[5pt]&=\left({\frac {1}{N}}\sum _{i=1}^{N}x_{i}^
      {2}\right)-\mu ^{2}\end{aligned}}}]
where the population mean is
         &#x03BC; =   1 N    &#x2211;  i = 1   N    x  i   .   {\displaystyle
      \mu ={\frac {1}{N}}\sum _{i=1}^{N}x_{i}.}  [{\displaystyle \mu ={\frac
      {1}{N}}\sum _{i=1}^{N}x_{i}.}]
The population variance can also be computed using
          &#x03C3;  2   =   1  N  2      &#x2211;  i < j     (   x  i
      &#x2212;  x  j    )   2   =   1  2  N  2       &#x2211;  i , j = 1   N
      (   x  i   &#x2212;  x  j    )   2   .   {\displaystyle \sigma ^{2}=
      {\frac {1}{N^{2}}}\sum _{i<j}\left(x_{i}-x_{j}\right)^{2}={\frac {1}{2N^
      {2}}}\sum _{i,j=1}^{N}\left(x_{i}-x_{j}\right)^{2}.}  [{\displaystyle
      \sigma ^{2}={\frac {1}{N^{2}}}\sum _{i<j}\left(x_{i}-x_{j}\right)^{2}=
      {\frac {1}{2N^{2}}}\sum _{i,j=1}^{N}\left(x_{i}-x_{j}\right)^{2}.}]
This is true because
               1  2  N  2       &#x2211;  i , j = 1   N     (   x  i   &#x2212;
      x  j    )   2      =   1  2  N  2       &#x2211;  i , j = 1   N    (   x
      i   2   &#x2212; 2  x  i    x  j   +  x  j   2    )        =   1  2 N
      &#x2211;  j = 1   N    (    1 N    &#x2211;  i = 1   N    x  i   2    )
      &#x2212;  (    1 N    &#x2211;  i = 1   N    x  i    )   (    1 N
      &#x2211;  j = 1   N    x  j    )         +   1  2 N     &#x2211;  i = 1
      N    (    1 N    &#x2211;  j = 1   N    x  j   2    )        =   1 2
      (   &#x03C3;  2   +  &#x03BC;  2    )  &#x2212;  &#x03BC;  2   +   1 2
      (   &#x03C3;  2   +  &#x03BC;  2    )        =  &#x03C3;  2
      {\displaystyle {\begin{aligned}{\frac {1}{2N^{2}}}\sum _{i,j=1}^{N}\left
      (x_{i}-x_{j}\right)^{2}&={\frac {1}{2N^{2}}}\sum _{i,j=1}^{N}\left(x_{i}^
      {2}-2x_{i}x_{j}+x_{j}^{2}\right)\\[5pt]&={\frac {1}{2N}}\sum _{j=1}^
      {N}\left({\frac {1}{N}}\sum _{i=1}^{N}x_{i}^{2}\right)-\left({\frac {1}
      {N}}\sum _{i=1}^{N}x_{i}\right)\left({\frac {1}{N}}\sum _{j=1}^{N}x_
      {j}\right)\\[5pt]&\quad +{\frac {1}{2N}}\sum _{i=1}^{N}\left({\frac {1}
      {N}}\sum _{j=1}^{N}x_{j}^{2}\right)\\[5pt]&={\frac {1}{2}}\left(\sigma ^
      {2}+\mu ^{2}\right)-\mu ^{2}+{\frac {1}{2}}\left(\sigma ^{2}+\mu ^
      {2}\right)\\[5pt]&=\sigma ^{2}\end{aligned}}}
      [{\displaystyle {\begin{aligned}{\frac {1}{2N^{2}}}\sum _{i,j=1}^{N}\left
      (x_{i}-x_{j}\right)^{2}&={\frac {1}{2N^{2}}}\sum _{i,j=1}^{N}\left(x_{i}^
      {2}-2x_{i}x_{j}+x_{j}^{2}\right)\\[5pt]&={\frac {1}{2N}}\sum _{j=1}^
      {N}\left({\frac {1}{N}}\sum _{i=1}^{N}x_{i}^{2}\right)-\left({\frac {1}
      {N}}\sum _{i=1}^{N}x_{i}\right)\left({\frac {1}{N}}\sum _{j=1}^{N}x_
      {j}\right)\\[5pt]&\quad +{\frac {1}{2N}}\sum _{i=1}^{N}\left({\frac {1}
      {N}}\sum _{j=1}^{N}x_{j}^{2}\right)\\[5pt]&={\frac {1}{2}}\left(\sigma ^
      {2}+\mu ^{2}\right)-\mu ^{2}+{\frac {1}{2}}\left(\sigma ^{2}+\mu ^
      {2}\right)\\[5pt]&=\sigma ^{2}\end{aligned}}}]
The population variance matches the variance of the generating probability
distribution. In this sense, the concept of population can be extended to
continuous random variables with infinite populations.
**** Sample variance[edit] ****
In many practical situations, the true variance of a population is not known a
priori and must be computed somehow. When dealing with extremely large
populations, it is not possible to count every object in the population, so the
computation must be performed on a sample of the population.[9] Sample variance
can also be applied to the estimation of the variance of a continuous
distribution from a sample of that distribution.
We take a sample_with_replacement of n values Y1, ..., Yn from the population,
where n < N, and estimate the variance on the basis of this sample.[10]
Directly taking the variance of the sample data gives the average of the
squared_deviations:
          &#x03C3;  Y   2   =   1 n    &#x2211;  i = 1   n     (   Y  i
      &#x2212;   Y &#x00AF;    )   2   =  (    1 n    &#x2211;  i = 1   n    Y
      i   2    )  &#x2212;    Y &#x00AF;    2   =   1  n  2      &#x2211;  i ,
      j  :  i < j     (   Y  i   &#x2212;  Y  j    )   2   .   {\displaystyle
      \sigma _{Y}^{2}={\frac {1}{n}}\sum _{i=1}^{n}\left(Y_{i}-{\overline
      {Y}}\right)^{2}=\left({\frac {1}{n}}\sum _{i=1}^{n}Y_{i}^{2}\right)-
      {\overline {Y}}^{2}={\frac {1}{n^{2}}}\sum _{i,j\,:\,i<j}\left(Y_{i}-Y_
      {j}\right)^{2}.}  [{\displaystyle \sigma _{Y}^{2}={\frac {1}{n}}\sum _
      {i=1}^{n}\left(Y_{i}-{\overline {Y}}\right)^{2}=\left({\frac {1}{n}}\sum
      _{i=1}^{n}Y_{i}^{2}\right)-{\overline {Y}}^{2}={\frac {1}{n^{2}}}\sum _
      {i,j\,:\,i<j}\left(Y_{i}-Y_{j}\right)^{2}.}]
Here,       Y &#x00AF;     {\displaystyle {\overline {Y}}}  [{\overline {Y}}]
denotes the sample_mean:
           Y &#x00AF;   =   1 n    &#x2211;  i = 1   n    Y  i   .
      {\displaystyle {\overline {Y}}={\frac {1}{n}}\sum _{i=1}^{n}Y_{i}.}  [
      {\displaystyle {\overline {Y}}={\frac {1}{n}}\sum _{i=1}^{n}Y_{i}.}]
Since the Yi are selected randomly, both       Y &#x00AF;     {\displaystyle
{\overline {Y}}}  [{\overline {Y}}] and      &#x03C3;  Y   2     {\displaystyle
\sigma _{Y}^{2}}  [{\displaystyle \sigma _{Y}^{2}}] are random variables. Their
expected values can be evaluated by averaging over the ensemble of all possible
samples {Yi} of size n from the population. For      &#x03C3;  Y   2
{\displaystyle \sigma _{Y}^{2}}  [{\displaystyle \sigma _{Y}^{2}}] this gives:
             E &#x2061; [  &#x03C3;  Y   2   ]    = E &#x2061;  [    1 n
      &#x2211;  i = 1   n     (   Y  i   &#x2212;   1 n    &#x2211;  j = 1   n
      Y  j    )   2    ]        =   1 n    &#x2211;  i = 1   n   E &#x2061;
      [   Y  i   2   &#x2212;   2 n    Y  i    &#x2211;  j = 1   n    Y  j   +
      1  n  2      &#x2211;  j = 1   n    Y  j    &#x2211;  k = 1   n    Y  k
      ]        =   1 n    &#x2211;  i = 1   n    [     n &#x2212; 2  n   E
      &#x2061; [  Y  i   2   ] &#x2212;   2 n    &#x2211;  j &#x2260; i   E
      &#x2061; [  Y  i    Y  j   ] +   1  n  2      &#x2211;  j = 1   n
      &#x2211;  k &#x2260; j   n   E &#x2061; [  Y  j    Y  k   ] +   1  n  2
      &#x2211;  j = 1   n   E &#x2061; [  Y  j   2   ]  ]        =   1 n
      &#x2211;  i = 1   n    [     n &#x2212; 2  n   (  &#x03C3;  2   +
      &#x03BC;  2   ) &#x2212;   2 n   ( n &#x2212; 1 )  &#x03BC;  2   +   1  n
      2     n ( n &#x2212; 1 )  &#x03BC;  2   +   1 n   (  &#x03C3;  2   +
      &#x03BC;  2   )  ]        =    n &#x2212; 1  n    &#x03C3;  2   .
      {\displaystyle {\begin{aligned}\operatorname {E} [\sigma _{Y}^
      {2}]&=\operatorname {E} \left[{\frac {1}{n}}\sum _{i=1}^{n}\left(Y_{i}-
      {\frac {1}{n}}\sum _{j=1}^{n}Y_{j}\right)^{2}\right]\\[5pt]&={\frac {1}
      {n}}\sum _{i=1}^{n}\operatorname {E} \left[Y_{i}^{2}-{\frac {2}{n}}Y_
      {i}\sum _{j=1}^{n}Y_{j}+{\frac {1}{n^{2}}}\sum _{j=1}^{n}Y_{j}\sum _
      {k=1}^{n}Y_{k}\right]\\[5pt]&={\frac {1}{n}}\sum _{i=1}^{n}\left[{\frac
      {n-2}{n}}\operatorname {E} [Y_{i}^{2}]-{\frac {2}{n}}\sum _{j\neq
      i}\operatorname {E} [Y_{i}Y_{j}]+{\frac {1}{n^{2}}}\sum _{j=1}^{n}\sum _
      {k\neq j}^{n}\operatorname {E} [Y_{j}Y_{k}]+{\frac {1}{n^{2}}}\sum _
      {j=1}^{n}\operatorname {E} [Y_{j}^{2}]\right]\\[5pt]&={\frac {1}{n}}\sum
      _{i=1}^{n}\left[{\frac {n-2}{n}}(\sigma ^{2}+\mu ^{2})-{\frac {2}{n}}(n-
      1)\mu ^{2}+{\frac {1}{n^{2}}}n(n-1)\mu ^{2}+{\frac {1}{n}}(\sigma ^
      {2}+\mu ^{2})\right]\\[5pt]&={\frac {n-1}{n}}\sigma ^{2}.\end{aligned}}}
      [{\displaystyle {\begin{aligned}\operatorname {E} [\sigma _{Y}^
      {2}]&=\operatorname {E} \left[{\frac {1}{n}}\sum _{i=1}^{n}\left(Y_{i}-
      {\frac {1}{n}}\sum _{j=1}^{n}Y_{j}\right)^{2}\right]\\[5pt]&={\frac {1}
      {n}}\sum _{i=1}^{n}\operatorname {E} \left[Y_{i}^{2}-{\frac {2}{n}}Y_
      {i}\sum _{j=1}^{n}Y_{j}+{\frac {1}{n^{2}}}\sum _{j=1}^{n}Y_{j}\sum _
      {k=1}^{n}Y_{k}\right]\\[5pt]&={\frac {1}{n}}\sum _{i=1}^{n}\left[{\frac
      {n-2}{n}}\operatorname {E} [Y_{i}^{2}]-{\frac {2}{n}}\sum _{j\neq
      i}\operatorname {E} [Y_{i}Y_{j}]+{\frac {1}{n^{2}}}\sum _{j=1}^{n}\sum _
      {k\neq j}^{n}\operatorname {E} [Y_{j}Y_{k}]+{\frac {1}{n^{2}}}\sum _
      {j=1}^{n}\operatorname {E} [Y_{j}^{2}]\right]\\[5pt]&={\frac {1}{n}}\sum
      _{i=1}^{n}\left[{\frac {n-2}{n}}(\sigma ^{2}+\mu ^{2})-{\frac {2}{n}}(n-
      1)\mu ^{2}+{\frac {1}{n^{2}}}n(n-1)\mu ^{2}+{\frac {1}{n}}(\sigma ^
      {2}+\mu ^{2})\right]\\[5pt]&={\frac {n-1}{n}}\sigma ^{2}.\end{aligned}}}]
Hence      &#x03C3;  Y   2     {\displaystyle \sigma _{Y}^{2}}  [{\displaystyle
\sigma _{Y}^{2}}] gives an estimate of the population variance that is biased
by a factor of        n &#x2212; 1  n     {\displaystyle {\frac {n-1}{n}}}  [
{\frac {n-1}{n}}]. For this reason,      &#x03C3;  Y   2     {\displaystyle
\sigma _{Y}^{2}}  [{\displaystyle \sigma _{Y}^{2}}] is referred to as the
biased sample variance. Correcting for this bias yields the unbiased sample
variance:
          s  2   =   n  n &#x2212; 1     &#x03C3;  Y   2   =   n  n &#x2212; 1
      (    1 n    &#x2211;  i = 1   n     (   Y  i   &#x2212;   Y &#x00AF;    )
      2    )  =   1  n &#x2212; 1     &#x2211;  i = 1   n     (   Y  i
      &#x2212;   Y &#x00AF;    )   2     {\displaystyle s^{2}={\frac {n}{n-
      1}}\sigma _{Y}^{2}={\frac {n}{n-1}}\left({\frac {1}{n}}\sum _{i=1}^
      {n}\left(Y_{i}-{\overline {Y}}\right)^{2}\right)={\frac {1}{n-1}}\sum _
      {i=1}^{n}\left(Y_{i}-{\overline {Y}}\right)^{2}}  [{\displaystyle s^{2}=
      {\frac {n}{n-1}}\sigma _{Y}^{2}={\frac {n}{n-1}}\left({\frac {1}{n}}\sum
      _{i=1}^{n}\left(Y_{i}-{\overline {Y}}\right)^{2}\right)={\frac {1}{n-
      1}}\sum _{i=1}^{n}\left(Y_{i}-{\overline {Y}}\right)^{2}}]
Either estimator may be simply referred to as the sample variance when the
version can be determined by context. The same proof is also applicable for
samples taken from a continuous probability distribution.
The use of the term n â 1 is called Bessel's_correction, and it is also used
in sample_covariance and the sample_standard_deviation (the square root of
variance). The square root is a concave_function and thus introduces negative
bias (by Jensen's_inequality), which depends on the distribution, and thus the
corrected sample standard deviation (using Bessel's correction) is biased. The
unbiased_estimation_of_standard_deviation is a technically involved problem,
though for the normal distribution using the term n â 1.5 yields an almost
unbiased estimator.
The unbiased sample variance is a U-statistic for the function Æ(y1, y2) = 
(y1 â y2)2/2, meaning that it is obtained by averaging a 2-sample statistic
over 2-element subsets of the population.
**** Distribution of the sample variance[edit] ****
Distribution and cumulative distribution of S2/σ2, for various values of ν = n
â 1, when the yi are independent normally distributed.
Being a function of random_variables, the sample variance is itself a random
variable, and it is natural to study its distribution. In the case that Yi are
independent observations from a normal_distribution, Cochran's_theorem shows
that S2 follows a scaled chi-squared_distribution:[11]
         ( n &#x2212; 1 )    S  2    &#x03C3;  2     &#x223C;  &#x03C7;  n
      &#x2212; 1   2   .   {\displaystyle (n-1){\frac {S^{2}}{\sigma ^{2}}}\sim
      \chi _{n-1}^{2}.}  [{\displaystyle (n-1){\frac {S^{2}}{\sigma ^{2}}}\sim
      \chi _{n-1}^{2}.}]
As a direct consequence, it follows that
         E &#x2061; (  S  2   ) = E &#x2061;  (     &#x03C3;  2    n &#x2212; 1
      &#x03C7;  n &#x2212; 1   2    )  =  &#x03C3;  2   ,   {\displaystyle
      \operatorname {E} (S^{2})=\operatorname {E} \left({\frac {\sigma ^{2}}{n-
      1}}\chi _{n-1}^{2}\right)=\sigma ^{2},}  [{\displaystyle \operatorname
      {E} (S^{2})=\operatorname {E} \left({\frac {\sigma ^{2}}{n-1}}\chi _{n-
      1}^{2}\right)=\sigma ^{2},}]
and[12]
         Var &#x2061; [  s  2   ] = Var &#x2061;  (     &#x03C3;  2    n
      &#x2212; 1     &#x03C7;  n &#x2212; 1   2    )  =    &#x03C3;  4    ( n
      &#x2212; 1  )  2      Var &#x2061;  (  &#x03C7;  n &#x2212; 1   2   )  =
      2  &#x03C3;  4     n &#x2212; 1    .   {\displaystyle \operatorname {Var}
      [s^{2}]=\operatorname {Var} \left({\frac {\sigma ^{2}}{n-1}}\chi _{n-1}^
      {2}\right)={\frac {\sigma ^{4}}{(n-1)^{2}}}\operatorname {Var} \left(\chi
      _{n-1}^{2}\right)={\frac {2\sigma ^{4}}{n-1}}.}
      [{\displaystyle \operatorname {Var} [s^{2}]=\operatorname {Var} \left(
      {\frac {\sigma ^{2}}{n-1}}\chi _{n-1}^{2}\right)={\frac {\sigma ^{4}}{(n-
      1)^{2}}}\operatorname {Var} \left(\chi _{n-1}^{2}\right)={\frac {2\sigma
      ^{4}}{n-1}}.}]
If the Yi are independent and identically distributed, but not necessarily
normally distributed, then[13][14]
         E &#x2061; [  S  2   ] =  &#x03C3;  2   ,  Var &#x2061; [  S  2   ] =
      &#x03C3;  4   n    (  ( &#x03BA; &#x2212; 1 ) +   2  n &#x2212; 1     )
      =   1 n    (   &#x03BC;  4   &#x2212;    n &#x2212; 3   n &#x2212; 1
      &#x03C3;  4    )  ,   {\displaystyle \operatorname {E} [S^{2}]=\sigma ^
      {2},\quad \operatorname {Var} [S^{2}]={\frac {\sigma ^{4}}{n}}\left(
      (\kappa -1)+{\frac {2}{n-1}}\right)={\frac {1}{n}}\left(\mu _{4}-{\frac
      {n-3}{n-1}}\sigma ^{4}\right),}
      [{\displaystyle \operatorname {E} [S^{2}]=\sigma ^{2},\quad \operatorname
      {Var} [S^{2}]={\frac {\sigma ^{4}}{n}}\left((\kappa -1)+{\frac {2}{n-
      1}}\right)={\frac {1}{n}}\left(\mu _{4}-{\frac {n-3}{n-1}}\sigma ^
      {4}\right),}]
where Îº is the kurtosis of the distribution and Î¼4 is the fourth central
moment.
If the conditions of the law_of_large_numbers hold for the squared
observations, s2 is a consistent_estimator of Ï2. One can see indeed that the
variance of the estimator tends asymptotically to zero. An asymptotically
equivalent formula was given in Kenney and Keeping (1951:164), Rose and Smith
(2002:264), and Weisstein (n.d.).[15][16][17]
**** Samuelson's inequality[edit] ****
Samuelson's_inequality is a result that states bounds on the values that
individual observations in a sample can take, given that the sample mean and
(biased) variance have been calculated.[18] Values must lie within the limits
y &#x00AF;    &#x00B1;  &#x03C3;  Y   ( n &#x2212; 1  )  1  /  2   .
{\displaystyle {\bar {y}}\pm \sigma _{Y}(n-1)^{1/2}.}  [{\displaystyle {\bar
{y}}\pm \sigma _{Y}(n-1)^{1/2}.}]
**** Relations with the harmonic and arithmetic means[edit] ****
It has been shown[19] that for a sample {yi} of real numbers,
          &#x03C3;  y   2   &#x2264; 2  y  max   ( A &#x2212; H ) ,
      {\displaystyle \sigma _{y}^{2}\leq 2y_{\max }(A-H),}  [\sigma _{y}^
      {2}\leq 2y_{\max }(A-H),]
where ymax is the maximum of the sample, A is the arithmetic mean, H is the
harmonic_mean of the sample and      &#x03C3;  y   2     {\displaystyle \sigma
_{y}^{2}}  [\sigma _{y}^{2}] is the (biased) variance of the sample.
This bound has been improved, and it is known that variance is bounded by
          &#x03C3;  y   2   &#x2264;     y  max   ( A &#x2212; H ) (  y  max
      &#x2212; A )    y  max   &#x2212; H    ,   {\displaystyle \sigma _{y}^
      {2}\leq {\frac {y_{\max }(A-H)(y_{\max }-A)}{y_{\max }-H}},}  [\sigma _
      {y}^{2}\leq {\frac {y_{\max }(A-H)(y_{\max }-A)}{y_{\max }-H}},]
          &#x03C3;  y   2   &#x2265;     y  min   ( A &#x2212; H ) ( A &#x2212;
      y  min   )   H &#x2212;  y  min      ,   {\displaystyle \sigma _{y}^
      {2}\geq {\frac {y_{\min }(A-H)(A-y_{\min })}{H-y_{\min }}},}  [\sigma _
      {y}^{2}\geq {\frac {y_{\min }(A-H)(A-y_{\min })}{H-y_{\min }}},]
where ymin is the minimum of the sample.[20]
***** Tests of equality of variances[edit] *****
Testing for the equality of two or more variances is difficult. The F_test and
chi_square_tests are both adversely affected by non-normality and are not
recommended for this purpose.
Several non parametric tests have been proposed: these include the
BartonâDavidâAnsariâFreundâSiegelâTukey test, the Capon_test, Mood
test, the Klotz_test and the Sukhatme_test. The Sukhatme test applies to two
variances and requires that both medians be known and equal to zero. The Mood,
Klotz, Capon and BartonâDavidâAnsariâFreundâSiegelâTukey tests also
apply to two variances. They allow the median to be unknown but do require that
the two medians are equal.
The Lehmann_test is a parametric test of two variances. Of this test there are
several variants known. Other tests of the equality of variances include the
Box_test, the BoxâAnderson_test and the Moses_test.
Resampling methods, which include the bootstrap and the jackknife, may be used
to test the equality of variances.
***** History[edit] *****
The term variance was first introduced by Ronald_Fisher in his 1918 paper The
Correlation_Between_Relatives_on_the_Supposition_of_Mendelian_Inheritance:[21]
     The great body of available statistics show us that the deviations of
     a human_measurement from its mean follow very closely the Normal_Law
     of_Errors, and, therefore, that the variability may be uniformly
     measured by the standard_deviation corresponding to the square_root
     of the mean_square_error. When there are two independent causes of
     variability capable of producing in an otherwise uniform population
     distributions with standard deviations      &#x03C3;  1
     {\displaystyle \sigma _{1}}  [\sigma _{1}] and      &#x03C3;  2
     {\displaystyle \sigma _{2}}  [\sigma _{2}], it is found that the
     distribution, when both causes act together, has a standard deviation
     &#x03C3;  1   2   +  &#x03C3;  2   2       {\displaystyle {\sqrt
     {\sigma _{1}^{2}+\sigma _{2}^{2}}}}  [{\sqrt {\sigma _{1}^{2}+\sigma
     _{2}^{2}}}]. It is therefore desirable in analysing the causes of
     variability to deal with the square of the standard deviation as the
     measure of variability. We shall term this quantity the Variance...
Geometric visualisation of the variance of an arbitrary distribution (2, 4, 4,
4, 5, 5, 7, 9):
   1. A frequency distribution is constructed.
   2. The centroid of the distribution gives its mean.
   3. A square with sides equal to the difference of each value from the mean
      is formed for each value.
   4. Arranging the squares into a rectangle with one side equal to the number
      of values, n, results in the other side being the distribution's
      variance, σ².
***** Moment of inertia[edit] *****
See also: Moment_(physics)_Â§ Examples
The variance of a probability distribution is analogous to the moment_of
inertia in classical_mechanics of a corresponding mass distribution along a
line, with respect to rotation about its center of mass.[citation_needed] It is
because of this analogy that such things as the variance are called moments of
probability_distributions.[citation_needed] The covariance matrix is related to
the moment_of_inertia_tensor for multivariate distributions. The moment of
inertia of a cloud of n points with a covariance matrix of     &#x03A3;
{\displaystyle \Sigma }  [\Sigma ] is given by[citation_needed]
         I = n (   1   3 &#x00D7; 3   tr &#x2061; ( &#x03A3; ) &#x2212;
      &#x03A3; ) .   {\displaystyle I=n(\mathbf {1} _{3\times 3}\operatorname
      {tr} (\Sigma )-\Sigma ).}  [I=n(\mathbf {1} _{3\times 3}\operatorname
      {tr} (\Sigma )-\Sigma ).]
This difference between moment of inertia in physics and in statistics is clear
for points that are gathered along a line. Suppose many points are close to the
x axis and distributed along it. The covariance matrix might look like
         &#x03A3; =   [    10   0   0     0   0.1   0     0   0   0.1    ]   .
      {\displaystyle \Sigma ={\begin{bmatrix}10&0&0\\0&0.1&0\\0&0&0.1\end
      {bmatrix}}.}  [\Sigma ={\begin{bmatrix}10&0&0\\0&0.1&0\\0&0&0.1\end
      {bmatrix}}.]
That is, there is the most variance in the x direction. Physicists would
consider this to have a low moment about the x axis so the moment-of-inertia
tensor is
         I = n   [    0.2   0   0     0   10.1   0     0   0   10.1    ]   .
      {\displaystyle I=n{\begin{bmatrix}0.2&0&0\\0&10.1&0\\0&0&10.1\end
      {bmatrix}}.}  [I=n{\begin{bmatrix}0.2&0&0\\0&10.1&0\\0&0&10.1\end
      {bmatrix}}.]
***** Semivariance[edit] *****
The semivariance is calculated in the same manner as the variance but only
those observations that fall below the mean are included in the calculation. It
is sometimes described as a measure of downside_risk in an investments context.
For skewed distributions, the semivariance can provide additional information
that a variance does not.[citation_needed]
For inequalities associated with the semivariance, see Chebyshev's_inequality
Â§ Semivariances.
***** Generalizations[edit] *****
**** For complex variables[edit] ****
If     x   {\displaystyle x}  [x] is a scalar complex-valued random variable,
with values in      C  ,   {\displaystyle \mathbb {C} ,}  [{\displaystyle
\mathbb {C} ,}] then its variance is     E &#x2061;  [  ( x &#x2212; &#x03BC; )
( x &#x2212; &#x03BC;  )  &#x2217;    ]  ,   {\displaystyle \operatorname {E}
\left[(x-\mu )(x-\mu )^{*}\right],}  [{\displaystyle \operatorname {E} \left[
(x-\mu )(x-\mu )^{*}\right],}] where      x  &#x2217;     {\displaystyle x^{*}}
[x^{*}] is the complex_conjugate of     x .   {\displaystyle x.}  [x.] This
variance is a real scalar.
**** For vector-valued random variables[edit] ****
*** As a matrix[edit] ***
If     X   {\displaystyle X}  [X] is a vector-valued random variable, with
values in       R   n   ,   {\displaystyle \mathbb {R} ^{n},}  [{\displaystyle
\mathbb {R} ^{n},}] and thought of as a column vector, then a natural
generalization of variance is     E &#x2061;  [  ( X &#x2212; &#x03BC; ) ( X
&#x2212; &#x03BC;  )  T    ]  ,   {\displaystyle \operatorname {E} \left[(X-\mu
)(X-\mu )^{\operatorname {T} }\right],}  [{\displaystyle \operatorname {E}
\left[(X-\mu )(X-\mu )^{\operatorname {T} }\right],}] where     &#x03BC; = E
&#x2061; ( X )   {\displaystyle \mu =\operatorname {E} (X)}  [\mu
=\operatorname {E} (X)] and      X  T     {\displaystyle X^{\operatorname {T}
}}  [X^{\operatorname {T} }] is the transpose of     X ,   {\displaystyle X,}
[X,] and so is a row vector. The result is a positive_semi-definite_square
matrix, commonly referred to as the variance-covariance_matrix (or simply as
the covariance matrix).
If     X   {\displaystyle X}  [X] is a vector- and complex-valued random
variable, with values in       C   n   ,   {\displaystyle \mathbb {C} ^{n},}  [
{\displaystyle \mathbb {C} ^{n},}] then the covariance_matrix_is     E &#x2061;
[  ( X &#x2212; &#x03BC; ) ( X &#x2212; &#x03BC;  )  &#x2020;    ]  ,
{\displaystyle \operatorname {E} \left[(X-\mu )(X-\mu )^{\dagger }\right],}  [
{\displaystyle \operatorname {E} \left[(X-\mu )(X-\mu )^{\dagger }\right],}]
where      X  &#x2020;     {\displaystyle X^{\dagger }}  [X^{\dagger }] is the
conjugate_transpose of     X .   {\displaystyle X.}  [X.][citation_needed] This
matrix is also positive semi-definite and square.
*** As a scalar[edit] ***
Another natural generalization of variance for such vector-valued random
variables     X ,   {\displaystyle X,}  [X,] which results in a scalar value
rather than in a matrix, is obtained by interpreting the deviation between the
random variable and its mean as the Euclidean_distance. This results in     E
&#x2061;  [  ( X &#x2212; &#x03BC;  )  T   ( X &#x2212; &#x03BC; )  ]  = tr
&#x2061; ( C ) ,   {\displaystyle \operatorname {E} \left[(X-\mu )^
{\operatorname {T} }(X-\mu )\right]=\operatorname {tr} (C),}  [{\displaystyle
\operatorname {E} \left[(X-\mu )^{\operatorname {T} }(X-\mu
)\right]=\operatorname {tr} (C),}] which is the trace of the covariance matrix.
***** See also[edit] *****
 This "see_also" section may contain an excessive number of suggestions. Please
 ensure that only the most relevant links are given, that they are not red
 links, and that any links are not already in this article. (May 2017)(Learn
 how_and_when_to_remove_this_template_message)
    * [icon]Statistics_portal
 Look up variance in Wiktionary, the free dictionary.
    * Average_absolute_deviation
    * BhatiaâDavis_inequality
    * Common-method_variance
    * Correlation
    * Chebyshev's_inequality
    * Distance_variance
    * Estimation_of_covariance_matrices
    * Explained_variance
    * Homoscedasticity
    * Mean_absolute_error
    * Mean_absolute_difference
    * Mean_preserving_spread
    * Pooled_variance (also known as combined, composite, or overall variance)
    * Popoviciu's_inequality_on_variances
    * Qualitative_variation
    * Quasi-variance, used in linear regression when the explanatory variable
      is categorical
    * Reduced_chi-squared
    * Sample_mean_and_covariance
    * Semivariance
    * Skewness
    * Taylor's_law
    * Weighted_sample_variance
***** References[edit] *****
   1. ^Yuli Zhang, Huaiyu Wu, Lei Cheng (June 2012). Some new deformation
      formulas about variance and covariance. Proceedings of 4th International
      Conference on Modelling, Identification and Control(ICMIC2012).
      pp. 987â992.CS1 maint: Uses authors parameter (link)
   2. .mw-parser-output cite.citation{font-style:inherit}.mw-parser-output
      .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-
      free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/
      65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:
      right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-
      parser-output .citation .cs1-lock-registration a{background:url("//
      upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/
      9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em
      center}.mw-parser-output .citation .cs1-lock-subscription a{background:
      url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-
      2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right
      .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-
      registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-
      parser-output .cs1-registration span{border-bottom:1px dotted;cursor:
      help}.mw-parser-output .cs1-ws-icon a{background:url("//
      upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/
      12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em
      center}.mw-parser-output code.cs1-code{color:inherit;background:
      inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-
      error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error
      {font-size:100%}.mw-parser-output .cs1-maint{display:none;color:
      #33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-
      output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-
      parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-
      left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-
      wl-right{padding-right:0.2em}
   3. ^ LoÃ¨ve,_M. (1977) "Probability Theory", Graduate Texts in Mathematics,
      Volume 45, 4th edition, Springer-Verlag, p. 12.
   4. ^ BienaymÃ©,_I.-J. (1853) "ConsidÃ©rations Ã  l'appui de la dÃ©couverte
      de Laplace sur la loi de probabilitÃ© dans la mÃ©thode des moindres
      carrÃ©s", Comptes rendus de l'AcadÃ©mie des sciences Paris, 37,
      p. 309â317; digital copy available [1]
   5. ^ BienaymÃ©,_I.-J. (1867) "ConsidÃ©rations Ã  l'appui de la dÃ©couverte
      de Laplace sur la loi de probabilitÃ© dans la mÃ©thode des moindres
      carrÃ©s", Journal de MathÃ©matiques Pures et AppliquÃ©es, SÃ©rie 2, Tome
      12, p. 158â167; digital copy available [2][3]
   6. ^ Cornell, J R, and Benjamin, C A, Probability, Statistics, and Decisions
      for Civil Engineers, McGraw-Hill, NY, 1970, pp.178-9.
   7. ^Johnson, Richard; Wichern, Dean (2001). Applied Multivariate Statistical
      Analysis. Prentice Hall. p. 76. ISBN 0-13-187715-1
   8. ^Goodman,_Leo_A. (December 1960). "On the Exact Variance of Products".
      Journal of the American Statistical Association. 55 (292): 708. doi:
      10.2307/2281592. JSTOR 2281592.
   9. ^Kagan, A.; Shepp, L. A. (1998). "Why the variance?". Statistics &
      Probability Letters. 38 (4): 329â333. doi:10.1016/S0167-7152(98)00041-
      8.
  10. ^ Navidi, William (2006) Statistics for Engineers and Scientists, McGraw-
      Hill, pg 14.
  11. ^ Montgomery, D. C. and Runger, G. C. (1994) Applied statistics and
      probability for engineers, page 201. John Wiley & Sons New York
  12. ^ Knight K. (2000), Mathematical Statistics, Chapman and Hall, New York.
      (proposition 2.11)
  13. ^ Casella and Berger (2002) Statistical Inference, Example 7.3.3, p. 331
      [full_citation_needed]
  14. ^ Cho, Eungchun; Cho, Moon Jung; Eltinge, John (2005) The Variance of
      Sample Variance From a Finite Population. International Journal of Pure
      and Applied Mathematics 21 (3): 387-394. http://www.ijpam.eu/contents/
      2005-21-3/10/10.pdf
  15. ^ Cho, Eungchun; Cho, Moon Jung (2009) Variance of Sample Variance With
      Replacement. International Journal of Pure and Applied Mathematics 52
      (1): 43â47. http://www.ijpam.eu/contents/2009-52-1/5/5.pdf
  16. ^ Kenney, John F.; Keeping, E.S. (1951) Mathematics of Statistics. Part
      Two. 2nd ed. D. Van Nostrand Company, Inc. Princeton: New Jersey. http://
      krishikosh.egranth.ac.in/bitstream/1/2025521/1/G2257.pdf
  17. ^ Rose, Colin; Smith, Murray D. (2002) Mathematical Statistics with
      Mathematica. Springer-Verlag, New York. http://www.mathstatica.com/book/
      Mathematical_Statistics_with_Mathematica.pdf
  18. ^ Weisstein, Eric W. (n.d.) Sample Variance Distribution. MathWorldâA
      Wolfram Web Resource. http://mathworld.wolfram.com/
      SampleVarianceDistribution.html
  19. ^Samuelson, Paul (1968). "How Deviant Can You Be?". Journal_of_the
      American_Statistical_Association. 63 (324): 1522â1525. doi:10.1080/
      01621459.1968.10480944. JSTOR 2285901.
  20. ^Mercer, A. McD. (2000). "Bounds for AâG, AâH, GâH, and a family of
      inequalities of Ky Fan's type, using a general method". J. Math. Anal.
      Appl. 243 (1): 163â173. doi:10.1006/jmaa.1999.6688.
  21. ^Sharma, R. (2008). "Some more inequalities for arithmetic mean, harmonic
      mean and variance". J. Math. Inequalities. 2 (1): 109â114.
      CiteSeerX 10.1.1.551.9397. doi:10.7153/jmi-02-11.
  22. ^ Ronald_Fisher (1918) The_correlation_between_relatives_on_the
      supposition_of_Mendelian_Inheritance
    * v
    * t
    * e
Theory of probability_distributions
    * probability_mass_function (pmf)
    * probability_density_function (pdf)
    * cumulative_distribution_function (cdf)
    * quantile_function
    * raw_moment
    * central_moment
    * mean
    * variance
    * standard_deviation                     [Loglogisticpdf_no-labels.svg]
    * skewness
    * kurtosis
    * L-moment
    * moment-generating_function (mgf)
    * characteristic_function
    * probability-generating_function (pgf)
    * cumulant
    * combinant
    * v
    * t
    * e
Statistics
    * Outline
    * Index
Descriptive_statistics
                               * Mean
                                     o arithmetic
                Center               o geometric
                                     o harmonic
                               * Median
                               * Mode
                               * Variance
                               * Standard_deviation
Continuous_data Dispersion     * Coefficient_of_variation
                               * Percentile
                               * Range
                               * Interquartile_range
                               * Central_limit_theorem
                               * Moments
                Shape                o Skewness
                                     o Kurtosis
                                     o L-moments
Count_data          * Index_of_dispersion
                    * Grouped_data
Summary tables      * Frequency_distribution
                    * Contingency_table
                    * Pearson_product-moment_correlation
                    * Rank_correlation
Dependence                o Spearman's_rho
                          o Kendall's_tau
                    * Partial_correlation
                    * Scatter_plot
                    * Bar_chart
                    * Biplot
                    * Box_plot
                    * Control_chart
                    * Correlogram
                    * Fan_chart
Graphics            * Forest_plot
                    * Histogram
                    * Pie_chart
                    * QâQ_plot
                    * Run_chart
                    * Scatter_plot
                    * Stem-and-leaf_display
                    * Radar_chart
Data_collection
                           * Population
                           * Statistic
                           * Effect_size
Study_design               * Statistical_power
                           * Optimal_design
                           * Sample_size_determination
                           * Replication
                           * Missing_data
                           * Sampling
                                 o stratified
Survey_methodology               o cluster
                           * Standard_error
                           * Opinion_poll
                           * Questionnaire
                           * Scientific_control
                           * Randomized_experiment
                           * Randomized_controlled_trial
Controlled_experiments     * Random_assignment
                           * Blocking
                           * Interaction
                           * Factorial_experiment
                           * Adaptive_clinical_trial
Adaptive Designs           * Up-and-Down_Designs
                           * Stochastic_approximation
                           * Cross-sectional_study
Observational_Studies      * Cohort_study
                           * Natural_experiment
                           * Quasi-experiment
Statistical_inference
                * Population
                * Statistic
                * Probability_distribution
                * Sampling_distribution
                      o Order_statistic
                * Empirical_distribution
                      o Density_estimation
                * Statistical_model
                      o Model_specification
                      o Lp_space
                * Parameter
                      o location
                      o scale
                      o shape
Statistical     * Parametric_family
theory                o Likelihood (monotone)
                      o Locationâscale_family
                      o Exponential_family
                * Completeness
                * Sufficiency
                * Statistical_functional
                      o Bootstrap
                      o U
                      o V
                * Optimal_decision
                      o loss_function
                * Efficiency
                * Statistical_distance
                      o divergence
                * Asymptotics
                * Robustness
                                    * Estimating_equations
                                          o Maximum_likelihood
                                          o Method_of_moments
                                          o M-estimator
                                          o Minimum_distance
            Point_estimation        * Unbiased_estimators
                                          o Mean-unbiased_minimum-variance
                                                # RaoâBlackwellization
                                                # LehmannâScheffÃ©_theorem
                                          o Median_unbiased
                                    * Plug-in
                                    * Confidence_interval
                                    * Pivot
Frequentist                         * Likelihood_interval
inference   Interval_estimation     * Prediction_interval
                                    * Tolerance_interval
                                    * Resampling
                                          o Bootstrap
                                          o Jackknife
                                    * 1-_&_2-tails
                                    * Power
            Testing_hypotheses            o Uniformly_most_powerful_test
                                    * Permutation_test
                                          o Randomization_test
                                    * Multiple_comparisons
                                    * Likelihood-ratio
            Parametric_tests        * Score/Lagrange_multiplier
                                    * Wald
                * Z-test_(normal)
                * Student's_t-test
                * F-test
                           * Chi-squared
                           * G-test
                           * KolmogorovâSmirnov
                           * AndersonâDarling
                           * Lilliefors
            Goodness       * JarqueâBera
            of_fit         * Normality_(ShapiroâWilk)
                           * Likelihood-ratio_test
Specific                   * Model_selection
tests                            o Cross_validation
                                 o AIC
                                 o BIC
                           * Sign
                                 o Sample_median
                           * Signed_rank_(Wilcoxon)
            Rank                 o HodgesâLehmann_estimator
            statistics     * Rank_sum_(MannâWhitney)
                           * Nonparametric anova
                                 o 1-way_(KruskalâWallis)
                                 o 2-way_(Friedman)
                                 o Ordered_alternative_(JonckheereâTerpstra)
                * Bayesian_probability
                      o prior
Bayesian              o posterior
inference       * Credible_interval
                * Bayes_factor
                * Bayesian_estimator
                      o Maximum_posterior_estimator
    * Correlation
    * Regression_analysis
                       * Pearson_product-moment
Correlation            * Partial_correlation
                       * Confounding_variable
                       * Coefficient_of_determination
                       * Errors_and_residuals
Regression             * Regression_validation
analysis               * Mixed_effects_models
                       * Simultaneous_equations_models
                       * Multivariate_adaptive_regression_splines_(MARS)
                       * Simple_linear_regression
Linear_regression      * Ordinary_least_squares
                       * General_linear_model
                       * Bayesian_regression
                       * Nonlinear_regression
                       * Nonparametric
Non-standard           * Semiparametric
predictors             * Isotonic
                       * Robust
                       * Heteroscedasticity
                       * Homoscedasticity
Generalized_linear     * Exponential_families
model                  * Logistic_(Bernoulli) / Binomial / Poisson_regressions
                       * Analysis_of_variance_(ANOVA,_anova)
Partition_of           * Analysis_of_covariance
variance               * Multivariate_ANOVA
                       * Degrees_of_freedom
Categorical / Multivariate / Time-series / Survival_analysis
                 * Cohen's_kappa
                 * Contingency_table
Categorical      * Graphical_model
                 * Log-linear_model
                 * McNemar's_test
                 * Regression
                 * Manova
                 * Principal_components
                 * Canonical_correlation
                 * Discriminant_analysis
Multivariate     * Cluster_analysis
                 * Classification
                 * Structural_equation_model
                       o Factor_analysis
                 * Multivariate_distributions
                       o Elliptical_distributions
                             # Normal
                                  * Decomposition
                                  * Trend
                                  * Stationarity
             General              * Seasonal_adjustment
                                  * Exponential_smoothing
                                  * Cointegration
                                  * Structural_break
                                  * Granger_causality
                                  * DickeyâFuller
                                  * Johansen
             Specific tests       * Q-statistic_(LjungâBox)
                                  * DurbinâWatson
Time-series                       * BreuschâGodfrey
                                  * Autocorrelation_(ACF)
                                        o partial_(PACF)
                                  * Cross-correlation_(XCF)
             Time_domain          * ARMA_model
                                  * ARIMA_model_(BoxâJenkins)
                                  * Autoregressive_conditional
                                    heteroskedasticity_(ARCH)
                                  * Vector_autoregression_(VAR)
                                  * Spectral_density_estimation
             Frequency_domain     * Fourier_analysis
                                  * Wavelet
                                  * Whittle_likelihood
                                   * KaplanâMeier_estimator_(product_limit)
             Survival_function     * Proportional_hazards_models
Survival                           * Accelerated_failure_time_(AFT)_model
                                   * First_hitting_time
             Hazard_function       * NelsonâAalen_estimator
             Test                  * Log-rank_test
Applications
                           * Bioinformatics
Biostatistics              * Clinical_trials / studies
                           * Epidemiology
                           * Medical_statistics
                           * Chemometrics
                           * Methods_engineering
Engineering_statistics     * Probabilistic_design
                           * Process / quality_control
                           * Reliability
                           * System_identification
                           * Actuarial_science
                           * Census
                           * Crime_statistics
                           * Demography
Social_statistics          * Econometrics
                           * National_accounts
                           * Official_statistics
                           * Population_statistics
                           * Psychometrics
                           * Cartography
                           * Environmental_statistics
Spatial_statistics         * Geographic_information_system
                           * Geostatistics
                           * Kriging
    * [Category]Category
    * [Portal]Portal
    * [Commons page]Commons
    * [WikiProject] WikiProject
Authority_control [Edit_this_at_Wikidata]     * GND: 4078739-4
                                              * NDL: 00561029

Retrieved from "https://en.wikipedia.org/w/
index.php?title=Variance&oldid=907622507"
Categories:
    * Moment_(mathematics)
    * Statistical_deviation_and_dispersion
Hidden categories:
    * CS1_maint:_Uses_authors_parameter
    * Articles_with_inconsistent_citation_formats
    * Articles_with_incomplete_citations_from_March_2013
    * Articles_with_short_description
    * All_articles_with_unsourced_statements
    * Articles_with_unsourced_statements_from_February_2012
    * Articles_with_unsourced_statements_from_June_2015
    * Articles_with_unsourced_statements_from_September_2016
    * Articles_with_excessive_see_also_sections_from_May_2017
    * Wikipedia_articles_with_GND_identifiers
    * Wikipedia_articles_with_NDL_identifiers
    * Articles_containing_proofs
***** Navigation menu *****
**** Personal tools ****
    * Not logged in
    * Talk
    * Contributions
    * Create_account
    * Log_in
**** Namespaces ****
    * Article
    * Talk
⁰
**** Variants ****
**** Views ****
    * Read
    * Edit
    * View_history
⁰
**** More ****
**** Search ****
[Unknown INPUT type][Search][Go]
**** Navigation ****
    * Main_page
    * Contents
    * Featured_content
    * Current_events
    * Random_article
    * Donate_to_Wikipedia
    * Wikipedia_store
**** Interaction ****
    * Help
    * About_Wikipedia
    * Community_portal
    * Recent_changes
    * Contact_page
**** Tools ****
    * What_links_here
    * Related_changes
    * Upload_file
    * Special_pages
    * Permanent_link
    * Page_information
    * Wikidata_item
    * Cite_this_page
**** In other projects ****
    * Wikimedia_Commons
**** Print/export ****
    * Create_a_book
    * Download_as_PDF
    * Printable_version
**** Languages ****
    * Ø§ÙØ¹Ø±Ø¨ÙØ©
    * Asturianu
    * AzÉrbaycanca
    * à¦¬à¦¾à¦à¦²à¦¾
    * ÐÐµÐ»Ð°ÑÑÑÐºÐ°Ñ
    * ÐÑÐ»Ð³Ð°ÑÑÐºÐ¸
    * CatalÃ 
    * ÄeÅ¡tina
    * Dansk
    * Deutsch
    * Eesti
    * ÎÎ»Î»Î·Î½Î¹ÎºÎ¬
    * EspaÃ±ol
    * Esperanto
    * Euskara
    * ÙØ§Ø±Ø³Û
    * FranÃ§ais
    * Gaeilge
    * Galego
    * íêµ­ì´
    * ÕÕ¡ÕµÕ¥ÖÕ¥Õ¶
    * à¤¹à¤¿à¤¨à¥à¤¦à¥
    * Bahasa_Indonesia
    * Ãslenska
    * Italiano
    * ×¢××¨××ª
    * á¥áá áá£áá
    * LietuviÅ³
    * Magyar
    * ÐÐ°ÐºÐµÐ´Ð¾Ð½ÑÐºÐ¸
    * Bahasa_Melayu
    * Nederlands
    * æ¥æ¬èª
    * Norsk
    * Norsk_nynorsk
    * Polski
    * PortuguÃªs
    * RomÃ¢nÄ
    * Ð ÑÑÑÐºÐ¸Ð¹
    * Scots
    * Sicilianu
    * à·à·à¶à·à¶½
    * Simple_English
    * SlovenÄina
    * SlovenÅ¡Äina
    * Ú©ÙØ±Ø¯Û
    * Ð¡ÑÐ¿ÑÐºÐ¸_/_srpski
    * Srpskohrvatski_/_ÑÑÐ¿ÑÐºÐ¾ÑÑÐ²Ð°ÑÑÐºÐ¸
    * Basa_Sunda
    * Suomi
    * Svenska
    * à®¤à®®à®¿à®´à¯
    * TÃ¼rkÃ§e
    * Ð£ÐºÑÐ°ÑÐ½ÑÑÐºÐ°
    * Ø§Ø±Ø¯Ù
    * Tiáº¿ng_Viá»t
    * ç²µèª
    * ä¸­æ
Edit_links
    * This page was last edited on 24 July 2019, at 05:12 (UTC).
    * Text is available under the Creative_Commons_Attribution-ShareAlike
      License; additional terms may apply. By using this site, you agree to the
      Terms_of_Use and Privacy_Policy. WikipediaÂ® is a registered trademark of
      the Wikimedia_Foundation,_Inc., a non-profit organization.
    * Privacy_policy
    * About_Wikipedia
    * Disclaimers
    * Contact_Wikipedia
    * Developers
    * Cookie_statement
    * Mobile_view
    * [Wikimedia_Foundation]
    * [Powered_by_MediaWiki]
