The following text has been accessed from https://en.wikipedia.org/wiki/Lexical_analysis at Fri Aug 9 01:12:30 IST 2019
Creative_Commons_Attribution-ShareAlike_License




















****** Lexical analysis ******
From Wikipedia, the free encyclopedia
Jump_to_navigation Jump_to_search
"Lexer" redirects here. For people with this name, see Lexer_(surname).
In computer_science, lexical analysis, lexing or tokenization is the process of
converting a sequence of characters (such as in a computer program or web page)
into a sequence of tokens (strings with an assigned and thus identified
meaning). A program that performs lexical analysis may be termed a lexer,
tokenizer,[1] or scanner, though scanner is also a term for the first stage of
a lexer. A lexer is generally combined with a parser, which together analyze
the syntax_of_programming_languages, web pages, and so forth.
⁰
***** Contents *****
    * 1_Applications
    * 2_Lexeme
    * 3_Token
    * 4_Lexical_grammar
    * 5_Tokenization
          o 5.1_Scanner
          o 5.2_Evaluator
          o 5.3_Obstacles
          o 5.4_Software
    * 6_Lexer_generator
          o 6.1_List_of_lexer_generators
    * 7_Phrase_structure
          o 7.1_Line_continuation
          o 7.2_Semicolon_insertion
          o 7.3_Off-side_rule
    * 8_Context-sensitive_lexing
    * 9_Notes
    * 10_References
          o 10.1_Sources
    * 11_External_links
***** Applications[edit] *****
A lexer forms the first phase of a compiler_frontend in modern processing.
Analysis generally occurs in one pass.
In older languages such as ALGOL, the initial stage was instead line
reconstruction, which performed unstropping and removed whitespace and comments
(and had scannerless parsers, with no separate lexer). These steps are now done
as part of the lexer.
Lexers and parsers are most often used for compilers, but can be used for other
computer language tools, such as prettyprinters or linters. Lexing can be
divided into two stages: the scanning, which segments the input string into
syntactic units called lexemes and categorizes these into token classes; and
the evaluating, which converts lexemes into processed values.
Lexers are generally quite simple, with most of the complexity deferred to the
parser or semantic_analysis phases, and can often be generated by a lexer
generator, notably lex or derivatives. However, lexers can sometimes include
some complexity, such as phrase_structure processing to make input easier and
simplify the parser, and may be written partly or fully by hand, either to
support more features or for performance.
***** Lexeme[edit] *****
A lexeme is a sequence of characters in the source program that matches the
pattern for a token and is identified by the lexical analyzer as an instance of
that token.[2]
Some authors term this a "token", using "token" interchangeably to represent
the string being tokenized, and the token data structure resulting from putting
this string through the tokenization process.[3][4]
The word lexeme in computer science is defined differently than lexeme in
linguistics. A lexeme in computer science roughly corresponds to a word in
linguistics (not to be confused with a word_in_computer_architecture), although
in some cases it may be more similar to a morpheme.
***** Token[edit] *****
A lexical token or simply token is a string with an assigned and thus
identified meaning. It is structured as a pair consisting of a token name and
an optional token value. The token name is a category of lexical unit.[2]
Common token names are
    * identifier: names the programmer chooses;
    * keyword: names already in the programming language;
    * separator (also known as punctuators): punctuation characters and paired-
      delimiters;
    * operator: symbols that operate on arguments and produce results;
    * literal: numeric, logical, textual, reference literals;
    * comment: line, block.
                Examples of token values
Token name Sample token values
identifier x, color, UP
keyword    if, while, return
separator  }, (, ;
operator   +, <, =
literal    true, 6.02e23, "music"
comment    /* Retrieves user data */, // must be negative
Consider this expression in the C programming language:
      x = a + b * 2;
The lexical analysis of this expression yields the following sequence of
tokens:
      [(identifier, x), (operator, =), (identifier, a), (operator, +),
      (identifier, b), (operator, *), (literal, 2), (separator, ;)]
A token name is what might be termed a part_of_speech in linguistics.
***** Lexical grammar[edit] *****
Further information: Lexical_grammar
The specification of a programming_language often includes a set of rules, the
lexical_grammar, which defines the lexical syntax. The lexical syntax is
usually a regular_language, with the grammar rules consisting of regular
expressions; they define the set of possible character sequences (lexemes) of a
token. A lexer recognizes strings, and for each kind of string found the
lexical program takes an action, most simply producing a token.
Two important common lexical categories are white_space and comments. These are
also defined in the grammar and processed by the lexer, but may be discarded
(not producing any tokens) and considered non-significant, at most separating
two tokens (as in if x instead of ifx). There are two important exceptions to
this. First, in off-side_rule languages that delimit blocks with indenting,
initial whitespace is significant, as it determines block structure, and is
generally handled at the lexer level; see phrase_structure, below. Secondly, in
some uses of lexers, comments and whitespace must be preserved â for
examples, a prettyprinter also needs to output the comments and some debugging
tools may provide messages to the programmer showing the original source code.
In the 1960s, notably for ALGOL, whitespace and comments were eliminated as
part of the line_reconstruction phase (the initial phase of the compiler
frontend), but this separate phase has been eliminated and these are now
handled by the lexer.
***** Tokenization[edit] *****
Tokenization is the process of demarcating and possibly classifying sections of
a string of input characters. The resulting tokens are then passed on to some
other form of processing. The process can be considered a sub-task of parsing
input.
(Note: Tokenization in the field of computer security has a different meaning.)
For example, in the text string:
      The quick brown fox jumps over the lazy dog
the string isn't implicitly segmented on spaces, as a natural_language speaker
would do. The raw input, the 43 characters, must be explicitly split into the 9
tokens with a given space delimiter (i.e., matching the string " " or regular
expression /\s{1}/).
The tokens could be represented in XML,
<sentence>
  <word>The</word>
  <word>quick</word>
  <word>brown</word>
  <word>fox</word>
  <word>jumps</word>
  <word>over</word>
  <word>the</word>
  <word>lazy</word>
  <word>dog</word>
</sentence>
Or as an s-expression,
 (sentence
   (word The)
   (word quick)
   (word brown)
   (word fox)
   (word jumps)
   (word over)
   (word the)
   (word lazy)
   (word dog))
When a token class represents more than one possible lexeme, the lexer often
saves enough information to reproduce the original lexeme, so that it can be
used in semantic_analysis. The parser typically retrieves this information from
the lexer and stores it in the abstract_syntax_tree. This is necessary in order
to avoid information loss in the case of numbers and identifiers.
Tokens are identified based on the specific rules of the lexer. Some methods
used to identify tokens include: regular_expressions, specific sequences of
characters termed a flag, specific separating characters called delimiters, and
explicit definition by a dictionary. Special characters, including punctuation
characters, are commonly used by lexers to identify tokens because of their
natural use in written and programming languages.
Tokens are often categorized by character content or by context within the data
stream. Categories are defined by the rules of the lexer. Categories often
involve grammar elements of the language used in the data stream. Programming
languages often categorize tokens as identifiers, operators, grouping symbols,
or by data_type. Written languages commonly categorize tokens as nouns, verbs,
adjectives, or punctuation. Categories are used for post-processing of the
tokens either by the parser or by other functions in the program.
A lexical analyzer generally does nothing with combinations of tokens, a task
left for a parser. For example, a typical lexical analyzer recognizes
parentheses as tokens, but does nothing to ensure that each "(" is matched with
a ")".
When a lexer feeds tokens to the parser, the representation used is typically
an enumerated list of number representations. For example, "Identifier" is
represented with 0, "Assignment operator" with 1, "Addition operator" with 2,
etc.
Tokens are defined often by regular_expressions, which are understood by a
lexical analyzer generator such as lex. The lexical analyzer (generated
automatically by a tool like lex, or hand-crafted) reads in a stream of
characters, identifies the lexemes in the stream, and categorizes them into
tokens. This is termed tokenizing. If the lexer finds an invalid token, it will
report an error.
Following tokenizing is parsing. From there, the interpreted data may be loaded
into data structures for general use, interpretation, or compiling.
**** Scanner[edit] ****
The first stage, the scanner, is usually based on a finite-state_machine (FSM).
It has encoded within it information on the possible sequences of characters
that can be contained within any of the tokens it handles (individual instances
of these character sequences are termed lexemes). For example, an integer
lexeme may contain any sequence of numerical_digit characters. In many cases,
the first non-whitespace character can be used to deduce the kind of token that
follows and subsequent input characters are then processed one at a time until
reaching a character that is not in the set of characters acceptable for that
token (this is termed the maximal_munch, or longest match, rule). In some
languages, the lexeme creation rules are more complex and may involve
backtracking over previously read characters. For example, in C, one 'L'
character is not enough to distinguish between an identifier that begins with
'L' and a wide-character string literal.
**** Evaluator[edit] ****
A lexeme, however, is only a string of characters known to be of a certain kind
(e.g., a string literal, a sequence of letters). In order to construct a token,
the lexical analyzer needs a second stage, the evaluator, which goes over the
characters of the lexeme to produce a value. The lexeme's type combined with
its value is what properly constitutes a token, which can be given to a parser.
Some tokens such as parentheses do not really have values, and so the evaluator
function for these can return nothing: only the type is needed. Similarly,
sometimes evaluators can suppress a lexeme entirely, concealing it from the
parser, which is useful for whitespace and comments. The evaluators for
identifiers are usually simple (literally representing the identifier), but may
include some unstropping. The evaluators for integer_literals may pass the
string on (deferring evaluation to the semantic analysis phase), or may perform
evaluation themselves, which can be involved for different bases or floating
point numbers. For a simple quoted string literal, the evaluator needs to
remove only the quotes, but the evaluator for an escaped_string_literal
incorporates a lexer, which unescapes the escape sequences.
For example, in the source code of a computer program, the string
      net_worth_future = (assets - liabilities);
might be converted into the following lexical token stream; whitespace is
suppressed and special characters have no value:
IDENTIFIER net_worth_future
EQUALS
OPEN_PARENTHESIS
IDENTIFIER assets
MINUS
IDENTIFIER liabilities
CLOSE_PARENTHESIS
SEMICOLON
Due to licensing restrictions of existing parsers, it may be necessary to write
a lexer by hand. This is practical if the list of tokens is small, but in
general, lexers are generated by automated tools. These tools generally accept
regular expressions that describe the tokens allowed in the input stream. Each
regular expression is associated with a production_rule in the lexical grammar
of the programming language that evaluates the lexemes matching the regular
expression. These tools may generate source code that can be compiled and
executed or construct a state_transition_table for a finite-state_machine
(which is plugged into template code for compiling and executing).
Regular expressions compactly represent patterns that the characters in lexemes
might follow. For example, for an English-based language, an IDENTIFIER token
might be any English alphabetic character or an underscore, followed by any
number of instances of ASCII alphanumeric characters and/or underscores. This
could be represented compactly by the string [a-zA-Z_][a-zA-Z_0-9]*. This means
"any character a-z, A-Z or _, followed by 0 or more of a-z, A-Z, _ or 0-9".
Regular expressions and the finite-state machines they generate are not
powerful enough to handle recursive patterns, such as "n opening parentheses,
followed by a statement, followed by n closing parentheses." They are unable to
keep count, and verify that n is the same on both sides, unless a finite set of
permissible values exists for n. It takes a full parser to recognize such
patterns in their full generality. A parser can push parentheses on a stack and
then try to pop them off and see if the stack is empty at the end (see example
[5] in the Structure_and_Interpretation_of_Computer_Programs book).
**** Obstacles[edit] ****
Typically, tokenization occurs at the word level. However, it is sometimes
difficult to define what is meant by a "word". Often a tokenizer relies on
simple heuristics, for example:
    * Punctuation and whitespace may or may not be included in the resulting
      list of tokens.
    * All contiguous strings of alphabetic characters are part of one token;
      likewise with numbers.
    * Tokens are separated by whitespace characters, such as a space or line
      break, or by punctuation characters.
In languages that use inter-word spaces (such as most that use the Latin
alphabet, and most programming languages), this approach is fairly
straightforward. However, even here there are many edge cases such as
contractions, hyphenated_words, emoticons, and larger constructs such as URIs
(which for some purposes may count as single tokens). A classic example is "New
York-based", which a naive tokenizer may break at the space even though the
better break is (arguably) at the hyphen.
Tokenization is particularly difficult for languages written in scriptio
continua which exhibit no word boundaries such as Ancient_Greek, Chinese,[6] or
Thai. Agglutinative_languages, such as Korean, also make tokenization tasks
complicated.
Some ways to address the more difficult problems include developing more
complex heuristics, querying a table of common special-cases, or fitting the
tokens to a language_model that identifies collocations in a later processing
step.
**** Software[edit] ****
    * Apache_OpenNLP includes rule based and statistical tokenizers which
      support many languages
    * U-Tokenizer is an API over HTTP that can cut Mandarin and Japanese
      sentences at word boundary. English is supported as well.
    * HPE_Haven_OnDemand_Text_Tokenization_API (Commercial product, with
      freemium access) uses Advanced Probabilistic Concept Modelling to
      determine the weight that the term holds in the specified text indexes
    * The Lex tool and its compiler is designed to generate code for fast
      lexical analysers based on a formal description of the lexical syntax. It
      is generally considered insufficient for applications with a complex set
      of lexical rules and severe performance requirements. For example, the
      GNU_Compiler_Collection (GCC) uses hand-written lexers.
***** Lexer generator[edit] *****
See also: Parser_generator
Lexers are often generated by a lexer generator, analogous to parser
generators, and such tools often come together. The most established is lex,
paired with the yacc parser generator, and the free equivalents flex/bison.
These generators are a form of domain-specific_language, taking in a lexical
specification â generally regular expressions with some markup â and
emitting a lexer.
These tools yield very fast development, which is very important in early
development, both to get a working lexer and because a language specification
may change often. Further, they often provide advanced features, such as pre-
and post-conditions which are hard to program by hand. However, an
automatically generated lexer may lack flexibility, and thus may require some
manual modification, or an all-manually written lexer.
Lexer performance is a concern, and optimizing is worthwhile, more so in stable
languages where the lexer is run very often (such as C or HTML). lex/flex-
generated lexers are reasonably fast, but improvements of two to three times
are possible using more tuned generators. Hand-written lexers are sometimes
used, but modern lexer generators produce faster lexers than most hand-coded
ones. The lex/flex family of generators uses a table-driven approach which is
much less efficient than the directly coded approach.[dubious  – discuss] With
the latter approach the generator produces an engine that directly jumps to
follow-up states via goto statements. Tools like re2c[7] have proven to produce
engines that are between two and three times faster than flex produced engines.
[citation_needed] It is in general difficult to hand-write analyzers that
perform better than engines generated by these latter tools.
**** List of lexer generators[edit] ****
 This section may contain indiscriminate, excessive, or irrelevant examples.
 Please improve_the_article by adding more descriptive text and removing less
 pertinent_examples. See Wikipedia's guide_to_writing_better_articles for
 further suggestions. (September 2018)
See also: List_of_parser_generators
    * ANTLR â can generate lexical analyzers and parsers
    * Flex â variant of the classic lex for C/C++
    * Ragel â state machine and lexer generator with output in C, C++, and
      Assembly
    * re2c â lexer generator for C and C++
The following lexical analysers can handle Unicode:
    * JavaCC â generates lexical analyzers written in Java
    * RE/flex â generates lexical analyzers for C++ in direct code, in DFA
      tables, or using a NFA regex library
    * re2c â lexer generator for C and C++[8]
    * PLY - the Python module ply.lex enables the lexical analysis part
***** Phrase structure[edit] *****
Lexical analysis mainly segments the input stream of characters into tokens,
simply grouping the characters into pieces and categorizing them. However, the
lexing may be significantly more complex; most simply, lexers may omit tokens
or insert added tokens. Omitting tokens, notably whitespace and comments, is
very common, when these are not needed by the compiler. Less commonly, added
tokens may be inserted. This is done mainly to group tokens into statements, or
statements into blocks, to simplify the parser.
**** Line continuation[edit] ****
Line_continuation is a feature of some languages where a newline is normally a
statement terminator. Most often, ending a line with a backslash (immediately
followed by a newline) results in the line being continued â the following
line is joined to the prior line. This is generally done in the lexer: the
backslash and newline are discarded, rather than the newline being tokenized.
Examples include bash,[9] other shell scripts and Python.[10]
**** Semicolon insertion[edit] ****
Many languages use the semicolon as a statement terminator. Most often this is
mandatory, but in some languages the semicolon is optional in many contexts.
This is mainly done at the lexer level, where the lexer outputs a semicolon
into the token stream, despite one not being present in the input character
stream, and is termed semicolon insertion or automatic semicolon insertion. In
these cases, semicolons are part of the formal phrase grammar of the language,
but may not be found in input text, as they can be inserted by the lexer.
Optional semicolons or other terminators or separators are also sometimes
handled at the parser level, notably in the case of trailing_commas or
semicolons.
Semicolon insertion is a feature of BCPL and its distant descendant Go,[11]
though it is absent in B or C.[12] Semicolon insertion is present in
JavaScript, though the rules are somewhat complex and much-criticized; to avoid
bugs, some recommend always using semicolons, while others use initial
semicolons, termed defensive_semicolons, at the start of potentially ambiguous
statements.
Semicolon insertion (in languages with semicolon-terminated statements) and
line continuation (in languages with newline-terminated statements) can be seen
as complementary: semicolon insertion adds a token, even though newlines
generally do not generate tokens, while line continuation prevents a token from
being generated, even though newlines generally do generate tokens.
**** Off-side rule[edit] ****
Further information: Off-side_rule
The off-side_rule (blocks determined by indenting) can be implemented in the
lexer, as in Python, where increasing the indenting results in the lexer
emitting an INDENT token, and decreasing the indenting results in the lexer
emitting a DEDENT token.[10] These tokens correspond to the opening brace { and
closing brace } in languages that use braces for blocks, and means that the
phrase grammar does not depend on whether braces or indenting are used. This
requires that the lexer hold state, namely the current indent level, and thus
can detect changes in indenting when this changes, and thus the lexical grammar
is not context-free: INDENTâDEDENT depend on the contextual information of
prior indent level.
***** Context-sensitive lexing[edit] *****
Generally lexical grammars are context-free, or almost so, and thus require no
looking back or ahead, or backtracking, which allows a simple, clean, and
efficient implementation. This also allows simple one-way communication from
lexer to parser, without needing any information flowing back to the lexer.
There are exceptions, however. Simple examples include: semicolon insertion in
Go, which requires looking back one token; concatenation of consecutive string
literals in Python,[10] which requires holding one token in a buffer before
emitting it (to see if the next token is another string literal); and the off-
side rule in Python, which requires maintaining a count of indent level
(indeed, a stack of each indent level). These examples all only require lexical
context, and while they complicate a lexer somewhat, they are invisible to the
parser and later phases.
A more complex example is the_lexer_hack in C, where the token class of a
sequence of characters cannot be determined until the semantic analysis phase,
since typedef names and variable names are lexically identical but constitute
different token classes. Thus in the hack, the lexer calls the semantic
analyzer (say, symbol table) and checks if the sequence requires a typedef
name. In this case, information must flow back not from the parser only, but
from the semantic analyzer back to the lexer, which complicates design.
***** Notes[edit] *****
***** References[edit] *****
   1. ^"Anatomy_of_a_Compiler_and_The_Tokenizer". www.cs.man.ac.uk.
   2. .mw-parser-output cite.citation{font-style:inherit}.mw-parser-output
      .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-
      free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/
      65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:
      right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-
      parser-output .citation .cs1-lock-registration a{background:url("//
      upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/
      9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em
      center}.mw-parser-output .citation .cs1-lock-subscription a{background:
      url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-
      2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right
      .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-
      registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-
      parser-output .cs1-registration span{border-bottom:1px dotted;cursor:
      help}.mw-parser-output .cs1-ws-icon a{background:url("//
      upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/
      12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em
      center}.mw-parser-output code.cs1-code{color:inherit;background:
      inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-
      error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error
      {font-size:100%}.mw-parser-output .cs1-maint{display:none;color:
      #33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-
      output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-
      parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-
      left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-
      wl-right{padding-right:0.2em}
   3. ^ a b page 111, "Compilers Principles, Techniques, & Tools, 2nd Ed."
      (WorldCat) by Aho, Lam, Sethi and Ullman, as quoted in https://
      stackoverflow.com/questions/14954721/what-is-the-difference-between-
      token-and-lexeme
   4. ^Perl 5 Porters. "perlinterp:_Perl_5_version_24.0_documentation".
      perldoc.perl.org - Official documentation for the Perl programming
      language. perldoc.perl.org. Retrieved 26 January 2017.
   5. ^Guy Coder (19 February 2013). "What_is_the_difference_between_token_and
      lexeme?". Stack Overflow. Stack Exchange Inc. Retrieved 26 January 2017.
   6. ^"Structure_and_Interpretation_of_Computer_Programs". mitpress.mit.edu.
   7. ^ Huang, C., Simon, P., Hsieh, S., & Prevot, L. (2007) Rethinking_Chinese
      Word_Segmentation:_Tokenization,_Character_Classification,_or_Word_break
      Identification
   8. ^Bumbulis, P.; Cowan, D. D. (MarâDec 1993). "RE2C: A more versatile
      scanner generator". ACM Letters on Programming Languages and Systems. 2
      (1â4): 70â84. doi:10.1145/176454.176487.
   9. ^ [1], re2c manual
  10. ^ Bash_Reference_Manual, 3.1.2.1_Escape_Character
  11. ^ a b c"3.6.4_Documentation". docs.python.org.
  12. ^ Effective_Go, "Semicolons"
  13. ^ "Semicolons_in_Go", golang-nuts, Rob 'Commander' Pike, 12/10/09
**** Sources[edit] ****
    * Compiling with C# and Java, Pat Terry, 2005,
ISBN 032126360X
Algorithms + Data Structures = Programs, Niklaus Wirth, 1975,
ISBN 0-13-022418-9
Compiler Construction, Niklaus Wirth, 1996,
ISBN 0-201-40353-6
Sebesta, R. W. (2006). Concepts of programming languages (Seventh edition)
pp. 177. Boston: Pearson/Addison-Wesley.
***** External links[edit] *****
    * Yang, W.; Tsay, Chey-Woei; Chan, Jien-Tsai (2002). "On_the_applicability
      of_the_longest-match_rule_in_lexical_analysis". Computer Languages,
      Systems & Structures. 28 (3): 273â288. doi:10.1016/S0096-0551(02)00014-
      0. NSC 86-2213-E-009-021 and NSC 86-2213-E-009-079.
Trim, Craig (Jan 23, 2013). "The_Art_of_Tokenization". Developer Works. IBM.
Word_Mention_Segmentation_Task, an analysis

Retrieved from "https://en.wikipedia.org/w/
index.php?title=Lexical_analysis&oldid=909590433"
Categories:
    * Compiler_construction
    * Programming_language_implementation
    * Parsing
Hidden categories:
    * All_accuracy_disputes
    * Articles_with_disputed_statements_from_May_2010
    * All_articles_with_unsourced_statements
    * Articles_with_unsourced_statements_from_April_2008
    * Articles_with_too_many_examples_from_September_2018
    * All_articles_with_too_many_examples
    * Wikipedia_articles_with_style_issues_from_September_2018
***** Navigation menu *****
**** Personal tools ****
    * Not logged in
    * Talk
    * Contributions
    * Create_account
    * Log_in
**** Namespaces ****
    * Article
    * Talk
⁰
**** Variants ****
**** Views ****
    * Read
    * Edit
    * View_history
⁰
**** More ****
**** Search ****
[Unknown INPUT type][Search][Go]
**** Navigation ****
    * Main_page
    * Contents
    * Featured_content
    * Current_events
    * Random_article
    * Donate_to_Wikipedia
    * Wikipedia_store
**** Interaction ****
    * Help
    * About_Wikipedia
    * Community_portal
    * Recent_changes
    * Contact_page
**** Tools ****
    * What_links_here
    * Related_changes
    * Upload_file
    * Special_pages
    * Permanent_link
    * Page_information
    * Wikidata_item
    * Cite_this_page
**** Print/export ****
    * Create_a_book
    * Download_as_PDF
    * Printable_version
**** Languages ****
    * Ø§ÙØ¹Ø±Ø¨ÙØ©
    * AzÉrbaycanca
    * CatalÃ 
    * ÄeÅ¡tina
    * Dansk
    * Deutsch
    * ÎÎ»Î»Î·Î½Î¹ÎºÎ¬
    * EspaÃ±ol
    * ÙØ§Ø±Ø³Û
    * FranÃ§ais
    * íêµ­ì´
    * ÕÕ¡ÕµÕ¥ÖÕ¥Õ¶
    * Hrvatski
    * Bahasa_Indonesia
    * Italiano
    * ×¢××¨××ª
    * ÐÐ°ÐºÐµÐ´Ð¾Ð½ÑÐºÐ¸
    * Bahasa_Melayu
    * Nederlands
    * æ¥æ¬èª
    * Norsk
    * Polski
    * PortuguÃªs
    * Ð ÑÑÑÐºÐ¸Ð¹
    * Svenska
    * à®¤à®®à®¿à®´à¯
    * TÃ¼rkÃ§e
    * Ð£ÐºÑÐ°ÑÐ½ÑÑÐºÐ°
    * Tiáº¿ng_Viá»t
    * ä¸­æ
Edit_links
    * This page was last edited on 6 August 2019, at 10:32 (UTC).
    * Text is available under the Creative_Commons_Attribution-ShareAlike
      License; additional terms may apply. By using this site, you agree to the
      Terms_of_Use and Privacy_Policy. WikipediaÂ® is a registered trademark of
      the Wikimedia_Foundation,_Inc., a non-profit organization.
    * Privacy_policy
    * About_Wikipedia
    * Disclaimers
    * Contact_Wikipedia
    * Developers
    * Cookie_statement
    * Mobile_view
    * [Wikimedia_Foundation]
    * [Powered_by_MediaWiki]
