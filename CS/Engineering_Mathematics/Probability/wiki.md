The following text has been accessed from https://en.wikipedia.org/wiki/Probability at Fri Aug 9 01:08:24 IST 2019
Creative_Commons_Attribution-ShareAlike_License





















****** Probability ******
From Wikipedia, the free encyclopedia
Jump_to_navigation Jump_to_search
For the mathematical field of probability specifically rather than a general
discussion, see Probability_theory. For other uses, see Probability_
(disambiguation).
Not to be confused with Probably.
Probability
    * Outline
    * Catalog_of_articles
    * Probabilists
    * Glossary
    * Notation
    * Journals
    * Category
    * v
    * t
    * e
Part of a series on
Certainty
    * Approximation
    * Belief
    * Determinism
    * Dogma
    * Doubt
    * Fallibilism
    * Fatalism
    * Hypothesis
    * Justification
    * Nihilism
    * Proof
    * Scientific_theory
    * Skepticism
    * Solipsism
    * Theory
    * Truth
    * Uncertainty
===============================================================================
Related concepts and fundamentals:
    * Agnosticism
    * Epistemology
    * Presupposition
    * Probability
    * v
    * t
    * e
The probabilities of rolling several numbers using two dice.
Probability is a measure quantifying the likelihood that events will occur.[1]
See glossary_of_probability_and_statistics. Probability quantifies as a number
between 0 and 1, where, sparsly speaking,[note_1] 0 indicates impossibility and
1 indicates certainty.[2][3] The higher the probability of an event, the more
likely it is that the event will occur. A simple example is the tossing of a
fair (unbiased) coin. Since the coin is fair, the two outcomes ("heads" and
"tails") are both equally probable; the probability of "heads" equals the
probability of "tails"; and since no other outcomes are possible, the
probability of either "heads" or "tails" is 1/2 (which could also be written as
0.5 or 50%).
These concepts have been given an axiomatic mathematical formalization in
probability_theory, which is used widely in such areas_of_study as mathematics,
statistics, finance, gambling, science (in particular physics), artificial
intelligence/machine_learning, computer_science, game_theory, and philosophy
to, for example, draw inferences about the expected frequency of events.
Probability theory is also used to describe the underlying mechanics and
regularities of complex_systems.[4]
⁰
***** Contents *****
    * 1_Interpretations
    * 2_Etymology
    * 3_History
    * 4_Theory
    * 5_Applications
    * 6_Mathematical_treatment
          o 6.1_Independent_events
          o 6.2_Mutually_exclusive_events
          o 6.3_Not_mutually_exclusive_events
          o 6.4_Conditional_probability
          o 6.5_Inverse_probability
          o 6.6_Summary_of_probabilities
    * 7_Relation_to_randomness_and_probability_in_quantum_mechanics
    * 8_See_also
    * 9_Notes
    * 10_References
    * 11_Bibliography
    * 12_External_links
***** Interpretations[edit] *****
Main article: Probability_interpretations
When dealing with experiments that are random and well-defined in a purely
theoretical setting (like tossing a fair coin), probabilities can be
numerically described by the number of desired outcomes divided by the total
number of all outcomes. For example, tossing a fair coin twice will yield
"head-head", "head-tail", "tail-head", and "tail-tail" outcomes. The
probability of getting an outcome of "head-head" is 1 out of 4 outcomes, or, in
numerical terms, 1/4, 0.25 or 25%. However, when it comes to practical
application, there are two major competing categories of probability
interpretations, whose adherents possess different views about the fundamental
nature of probability:
   1. Objectivists assign numbers to describe some objective or physical state
      of affairs. The most popular version of objective probability is
      frequentist_probability, which claims that the probability of a random
      event denotes the relative frequency of occurrence of an experiment's
      outcome, when repeating the experiment. This interpretation considers
      probability to be the relative frequency "in the long run" of outcomes.
      [5] A modification of this is propensity_probability, which interprets
      probability as the tendency of some experiment to yield a certain
      outcome, even if it is performed only once.
   2. Subjectivists assign numbers per subjective probability, i.e., as a
      degree of belief.[6] The degree of belief has been interpreted as, "the
      price at which you would buy or sell a bet that pays 1 unit of utility if
      E, 0 if not E."[7] The most popular version of subjective probability is
      Bayesian_probability, which includes expert knowledge as well as
      experimental data to produce probabilities. The expert knowledge is
      represented by some (subjective) prior_probability_distribution. These
      data are incorporated in a likelihood_function. The product of the prior
      and the likelihood, normalized, results in a posterior_probability
      distribution that incorporates all the information known to date.[8] By
      Aumann's_agreement_theorem, Bayesian agents whose prior beliefs are
      similar will end up with similar posterior beliefs. However, sufficiently
      different priors can lead to different conclusions regardless of how much
      information the agents share.[9]
***** Etymology[edit] *****
See also: History_of_probability_Â§ Etymology
Further information: Likelihood
The word probability derives from the Latin probabilitas, which can also mean
"probity", a measure of the authority of a witness in a legal_case in Europe,
and often correlated with the witness's nobility. In a sense, this differs much
from the modern meaning of probability, which, in contrast, is a measure of the
weight of empirical_evidence, and is arrived at from inductive_reasoning and
statistical_inference.[10]
***** History[edit] *****
Main article: History_of_probability
The scientific study of probability is a modern development of mathematics.
Gambling shows that there has been an interest in quantifying the ideas of
probability for millennia, but exact mathematical descriptions arose much
later. There are reasons for the slow development of the mathematics of
probability. Whereas games of chance provided the impetus for the mathematical
study of probability, fundamental issues[clarification_needed] are still
obscured by the superstitions of gamblers.[11]
Christiaan Huygens likely published the first book on probability
According to Richard_Jeffrey, "Before the middle of the seventeenth century,
the term 'probable' (Latin probabilis) meant approvable, and was applied in
that sense, unequivocally, to opinion and to action. A probable action or
opinion was one such as sensible people would undertake or hold, in the
circumstances."[12] However, in legal contexts especially, 'probable' could
also apply to propositions for which there was good evidence.[13]
Forms of probability_and_statistics were developed by Arab_mathematicians
studying cryptology between the 8th and 13th centuries. Al-Khalil (717â786)
wrote the Book of Cryptographic Messages which contains the first use of
permutations_and_combinations to list all possible Arabic words with and
without vowels. Al-Kindi (801â873) made the earliest known use of statistical
inference in his work on cryptanalysis and frequency_analysis. An important
contribution of Ibn_Adlan (1187â1268) was on sample_size for use of frequency
analysis.[14]
Gerolamo Cardano
The sixteenth century Italian polymath Gerolamo_Cardano demonstrated the
efficacy of defining odds as the ratio of favourable to unfavourable outcomes
(which implies that the probability of an event is given by the ratio of
favourable outcomes to the total number of possible outcomes[15]). Aside from
the elementary work by Cardano, the doctrine of probabilities dates to the
correspondence of Pierre_de_Fermat and Blaise_Pascal (1654). Christiaan_Huygens
(1657) gave the earliest known scientific treatment of the subject.[16] Jakob
Bernoulli's Ars_Conjectandi (posthumous, 1713) and Abraham_de_Moivre's Doctrine
of_Chances (1718) treated the subject as a branch of mathematics.[17] See Ian
Hacking's The Emergence of Probability[10] and James_Franklin's The Science of
Conjecture[18] for histories of the early development of the very concept of
mathematical probability.
The theory_of_errors may be traced back to Roger_Cotes's Opera Miscellanea
(posthumous, 1722), but a memoir prepared by Thomas_Simpson in 1755 (printed
1756) first applied the theory to the discussion of errors of observation.[19]
The reprint (1757) of this memoir lays down the axioms that positive and
negative errors are equally probable, and that certain assignable limits define
the range of all errors. Simpson also discusses continuous errors and describes
a probability curve.
The first two laws of error that were proposed both originated with Pierre-
Simon_Laplace. The first law was published in 1774 and stated that the
frequency of an error could be expressed as an exponential function of the
numerical magnitude of the error, disregarding sign. The second law of error
was proposed in 1778 by Laplace and stated that the frequency of the error is
an exponential function of the square of the error.[20] The second law of error
is called the normal distribution or the Gauss law. "It is difficult
historically to attribute that law to Gauss, who in spite of his well-known
precocity had probably not made this discovery before he was two years old."
[20]
Daniel_Bernoulli (1778) introduced the principle of the maximum product of the
probabilities of a system of concurrent errors.
Carl Friedrich Gauss
Adrien-Marie_Legendre (1805) developed the method_of_least_squares, and
introduced it in his Nouvelles mÃ©thodes pour la dÃ©termination des orbites des
comÃ¨tes (New Methods for Determining the Orbits of Comets).[21] In ignorance
of Legendre's contribution, an Irish-American writer, Robert_Adrain, editor of
"The Analyst" (1808), first deduced the law of facility of error,
         &#x03D5; ( x ) = c  e  &#x2212;  h  2    x  2     ,   {\displaystyle
      \phi (x)=ce^{-h^{2}x^{2}},}  [\phi (x)=ce^{-h^{2}x^{2}},]
where     h   {\displaystyle h}  [h] is a constant depending on precision of
observation, and     c   {\displaystyle c}  [c] is a scale factor ensuring that
the area under the curve equals 1. He gave two proofs, the second being
essentially the same as John_Herschel's (1850).[citation_needed] Gauss gave the
first proof that seems to have been known in Europe (the third after Adrain's)
in 1809. Further proofs were given by Laplace (1810, 1812), Gauss (1823), James
Ivory (1825, 1826), Hagen (1837), Friedrich_Bessel (1838), W.F._Donkin (1844,
1856), and Morgan_Crofton (1870). Other contributors were Ellis (1844), De
Morgan (1864), Glaisher (1872), and Giovanni_Schiaparelli (1875). Peters's
(1856) formula[clarification_needed] for r, the probable_error of a single
observation, is well known.
In the nineteenth century authors on the general theory included Laplace,
Sylvestre_Lacroix (1816), Littrow (1833), Adolphe_Quetelet (1853), Richard
Dedekind (1860), Helmert (1872), Hermann_Laurent (1873), Liagre, Didion, and
Karl_Pearson. Augustus_De_Morgan and George_Boole improved the exposition of
the theory.
Andrey_Markov introduced[22] the notion of Markov_chains (1906), which played
an important role in stochastic_processes theory and its applications. The
modern theory of probability based on the measure_theory was developed by
Andrey_Kolmogorov (1931).[23]
On the geometric side (see integral_geometry) contributors to The Educational
Times were influential (Miller, Crofton, McColl, Wolstenholme, Watson, and
Artemas_Martin).[citation_needed]
Further information: History_of_statistics
***** Theory[edit] *****
Main article: Probability_theory
Like other theories, the theory_of_probability is a representation of its
concepts in formal termsâthat is, in terms that can be considered separately
from their meaning. These formal terms are manipulated by the rules of
mathematics and logic, and any results are interpreted or translated back into
the problem domain.
There have been at least two successful attempts to formalize probability,
namely the Kolmogorov formulation and the Cox formulation. In Kolmogorov's
formulation (see probability_space), sets are interpreted as events and
probability itself as a measure on a class of sets. In Cox's_theorem,
probability is taken as a primitive (that is, not further analyzed) and the
emphasis is on constructing a consistent assignment of probability values to
propositions. In both cases, the laws_of_probability are the same, except for
technical details.
There are other methods for quantifying uncertainty, such as the
DempsterâShafer_theory or possibility_theory, but those are essentially
different and not compatible with the laws of probability as usually
understood.
***** Applications[edit] *****
Probability theory is applied in everyday life in risk assessment and modeling.
The insurance industry and markets use actuarial_science to determine pricing
and make trading decisions. Governments apply probabilistic methods in
environmental_regulation, entitlement analysis (Reliability_theory_of_aging_and
longevity), and financial_regulation.
A good example of the use of probability theory in equity trading is the effect
of the perceived probability of any widespread Middle East conflict on oil
prices, which have ripple effects in the economy as a whole. An assessment by a
commodity trader that a war is more likely can send that commodity's prices up
or down, and signals other traders of that opinion. Accordingly, the
probabilities are neither assessed independently nor necessarily very
rationally. The theory of behavioral_finance emerged to describe the effect of
such groupthink on pricing, on policy, and on peace and conflict.[24]
In addition to financial assessment, probability can be used to analyze trends
in biology (e.g. disease spread) as well as ecology (e.g. biological Punnett
squares). As with finance, risk assessment can be used as a statistical tool to
calculate the likelihood of undesirable events occurring and can assist with
implementing protocols to avoid encountering such circumstances. Probability is
used to design games_of_chance so that casinos can make a guaranteed profit,
yet provide payouts to players that are frequent enough to encourage continued
play.[25]
The discovery of rigorous methods to assess and combine probability assessments
has changed society.[26][citation_needed]
Another significant application of probability theory in everyday life is
reliability. Many consumer products, such as automobiles and consumer
electronics, use reliability theory in product design to reduce the probability
of failure. Failure probability may influence a manufacturer's decisions on a
product's warranty.[27]
The cache_language_model and other statistical_language_models that are used in
natural_language_processing are also examples of applications of probability
theory.
***** Mathematical treatment[edit] *****
See also: Probability_axioms
Consider an experiment that can produce a number of results. The collection of
all possible results is called the sample space of the experiment. The power
set of the sample space is formed by considering all different collections of
possible results. For example, rolling a dice can produce six possible results.
One collection of possible results gives an odd number on the dice. Thus, the
subset {1,3,5} is an element of the power_set of the sample space of dice
rolls. These collections are called "events". In this case, {1,3,5} is the
event that the dice falls on some odd number. If the results that actually
occur fall in a given event, the event is said to have occurred.
A probability is a way_of_assigning every event a value between zero and one,
with the requirement that the event made up of all possible results (in our
example, the event {1,2,3,4,5,6}) is assigned a value of one. To qualify as a
probability, the assignment of values must satisfy the requirement that if you
look at a collection of mutually exclusive events (events with no common
results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive),
the probability that at least one of the events will occur is given by the sum
of the probabilities of all the individual events.[28]
The probability of an event A is written as     P ( A )   {\displaystyle P(A)}
[P(A)],     p ( A )   {\displaystyle p(A)}  [p(A)], or      Pr  ( A )
{\displaystyle {\text{Pr}}(A)}  [{\text{Pr}}(A)].[29] This mathematical
definition of probability can extend to infinite sample spaces, and even
uncountable sample spaces, using the concept of a measure.
The opposite or complement of an event A is the event [not A] (that is, the
event of A not occurring), often denoted as       A &#x00AF;   ,  A  &#x2201;
, &#x00AC; A   {\displaystyle {\overline {A}},A^{\complement },\neg A}  [
{\displaystyle {\overline {A}},A^{\complement },\neg A}], or      &#x223C;  A
{\displaystyle {\sim }A}  [{\displaystyle {\sim }A}]; its probability is given
by P(not A) = 1 â P(A).[30] As an example, the chance of not rolling a six on
a six-sided die is 1 â (chance of rolling a six)     = 1 &#x2212;    1 6    =
5 6      {\displaystyle =1-{\tfrac {1}{6}}={\tfrac {5}{6}}}  [=1-{\tfrac {1}
{6}}={\tfrac {5}{6}}]. See Complementary_event for a more complete treatment.
If two events A and B occur on a single performance of an experiment, this is
called the intersection or joint_probability of A and B, denoted as     P ( A
&#x2229; B )   {\displaystyle P(A\cap B)}  [P(A\cap B)].
**** Independent events[edit] ****
If two events, A and B are independent then the joint probability is
         P ( A   &#xA0;and&#xA0;   B ) = P ( A &#x2229; B ) = P ( A ) P ( B ) ,
      {\displaystyle P(A{\mbox{ and }}B)=P(A\cap B)=P(A)P(B),\,}  [P(A{\mbox
      { and }}B)=P(A\cap B)=P(A)P(B),\,]
for example, if two coins are flipped the chance of both being heads is
1 2    &#x00D7;    1 2    =    1 4      {\displaystyle {\tfrac {1}{2}}\times
{\tfrac {1}{2}}={\tfrac {1}{4}}}  [{\tfrac {1}{2}}\times {\tfrac {1}{2}}=
{\tfrac {1}{4}}].[31]
**** Mutually exclusive events[edit] ****
If either event A or event B but never both occurs on a single performance of
an experiment, then they are called mutually exclusive events.
If two events are mutually_exclusive then the probability of both occurring is
denoted as     P ( A &#x2229; B )   {\displaystyle P(A\cap B)}  [P(A\cap B)].
         P ( A   &#xA0;and&#xA0;   B ) = P ( A &#x2229; B ) = 0
      {\displaystyle P(A{\mbox{ and }}B)=P(A\cap B)=0}  [{\displaystyle P(A
      {\mbox{ and }}B)=P(A\cap B)=0}]
If two events are mutually_exclusive then the probability of either occurring
is denoted as     P ( A &#x222A; B )   {\displaystyle P(A\cup B)}  [P(A\cup
B)].
         P ( A   &#xA0;or&#xA0;   B ) = P ( A &#x222A; B ) = P ( A ) + P ( B )
      &#x2212; P ( A &#x2229; B ) = P ( A ) + P ( B ) &#x2212; 0 = P ( A ) + P
      ( B )   {\displaystyle P(A{\mbox{ or }}B)=P(A\cup B)=P(A)+P(B)-P(A\cap
      B)=P(A)+P(B)-0=P(A)+P(B)}  [{\displaystyle P(A{\mbox{ or }}B)=P(A\cup
      B)=P(A)+P(B)-P(A\cap B)=P(A)+P(B)-0=P(A)+P(B)}]
For example, the chance of rolling a 1 or 2 on a six-sided die is     P ( 1
&#xA0;or&#xA0;   2 ) = P ( 1 ) + P ( 2 ) =    1 6    +    1 6    =    1 3    .
{\displaystyle P(1{\mbox{ or }}2)=P(1)+P(2)={\tfrac {1}{6}}+{\tfrac {1}{6}}=
{\tfrac {1}{3}}.}  [P(1{\mbox{ or }}2)=P(1)+P(2)={\tfrac {1}{6}}+{\tfrac {1}
{6}}={\tfrac {1}{3}}.]
**** Not mutually exclusive events[edit] ****
If the events are not mutually exclusive then
         P  (  A   &#xA0;or&#xA0;   B  )  = P ( A &#x222A; B ) = P  ( A )  + P
      ( B )  &#x2212; P  (  A   &#xA0;and&#xA0;   B  )  .   {\displaystyle
      P\left(A{\hbox{ or }}B\right)=P(A\cup B)=P\left(A\right)+P\left(B\right)-
      P\left(A{\mbox{ and }}B\right).}  [{\displaystyle P\left(A{\hbox{ or
      }}B\right)=P(A\cup B)=P\left(A\right)+P\left(B\right)-P\left(A{\mbox{ and
      }}B\right).}]
For example, when drawing a single card at random from a regular deck of cards,
the chance of getting a heart or a face card (J,Q,K) (or one that is both) is
13 52    +    12 52    &#x2212;    3 52    =    11 26      {\displaystyle
{\tfrac {13}{52}}+{\tfrac {12}{52}}-{\tfrac {3}{52}}={\tfrac {11}{26}}}  [
{\tfrac {13}{52}}+{\tfrac {12}{52}}-{\tfrac {3}{52}}={\tfrac {11}{26}}],
because of the 52 cards of a deck 13 are hearts, 12 are face cards, and 3 are
both: here the possibilities included in the "3 that are both" are included in
each of the "13 hearts" and the "12 face cards" but should only be counted
once.
**** Conditional probability[edit] ****
Conditional_probability is the probability of some event A, given the
occurrence of some other event B. Conditional probability is written     P ( A
&#x2223; B )   {\displaystyle P(A\mid B)}  [P(A\mid B)], and is read "the
probability of A, given B". It is defined by[32]
         P ( A &#x2223; B ) =    P ( A &#x2229; B )   P ( B )    .
      {\displaystyle P(A\mid B)={\frac {P(A\cap B)}{P(B)}}.\,}  [P(A\mid B)=
      {\frac {P(A\cap B)}{P(B)}}.\,]
If     P ( B ) = 0   {\displaystyle P(B)=0}  [P(B)=0] then     P ( A &#x2223; B
)   {\displaystyle P(A\mid B)}  [P(A\mid B)] is formally undefined by this
expression. However, it is possible to define a conditional probability for
some zero-probability events using a Ï-algebra of such events (such as those
arising from a continuous_random_variable).[citation_needed]
For example, in a bag of 2 red balls and 2 blue balls (4 balls in total), the
probability of taking a red ball is     1  /  2   {\displaystyle 1/2}  [1/2];
however, when taking a second ball, the probability of it being either a red
ball or a blue ball depends on the ball previously taken, such as, if a red
ball was taken, the probability of picking a red ball again would be     1  /
3   {\displaystyle 1/3}  [1/3] since only 1 red and 2 blue balls would have
been remaining.
**** Inverse probability[edit] ****
In probability_theory and applications, Bayes' rule relates the odds of event
A  1     {\displaystyle A_{1}}  [A_{1}] to event      A  2     {\displaystyle
A_{2}}  [A_{2}], before (prior to) and after (posterior to) conditioning on
another event     B   {\displaystyle B}  [B]. The odds on      A  1
{\displaystyle A_{1}}  [A_{1}] to event      A  2     {\displaystyle A_{2}}
[A_{2}] is simply the ratio of the probabilities of the two events. When
arbitrarily many events     A   {\displaystyle A}  [A] are of interest, not
just two, the rule can be rephrased as posterior is proportional to prior times
likelihood,     P ( A  |  B ) &#x221D; P ( A ) P ( B  |  A )   {\displaystyle P
(A|B)\propto P(A)P(B|A)}  [P(A|B)\propto P(A)P(B|A)] where the proportionality
symbol means that the left hand side is proportional to (i.e., equals a
constant times) the right hand side as     A   {\displaystyle A}  [A] varies,
for fixed or given     B   {\displaystyle B}  [B] (Lee, 2012; Bertsch McGrayne,
2012). In this form it goes back to Laplace (1774) and to Cournot (1843); see
Fienberg (2005). See Inverse_probability and Bayes'_rule.
**** Summary of probabilities[edit] ****
                           Summary of probabilities
Event     Probability
      A      P ( A ) &#x2208; [ 0 , 1 ]    {\displaystyle P(A)\in [0,1]\,}  [P
          (A)\in [0,1]\,]
             P (  A  &#x2201;   ) = 1 &#x2212; P ( A )    {\displaystyle P(A^
   not A  {\complement })=1-P(A)\,}  [{\displaystyle P(A^{\complement })=1-P
          (A)\,}]
                 P ( A &#x222A; B )    = P ( A ) + P ( B ) &#x2212; P ( A
          &#x2229; B )     P ( A &#x222A; B )    = P ( A ) + P ( B )    if A
          and B are mutually exclusive         {\displaystyle {\begin{aligned}P
  A or B  (A\cup B)&=P(A)+P(B)-P(A\cap B)\\P(A\cup B)&=P(A)+P(B)\qquad {\mbox
          {if A and B are mutually exclusive}}\\\end{aligned}}}  [{\begin
          {aligned}P(A\cup B)&=P(A)+P(B)-P(A\cap B)\\P(A\cup B)&=P(A)+P
          (B)\qquad {\mbox{if A and B are mutually exclusive}}\\\end{aligned}}]
                 P ( A &#x2229; B )    = P ( A  |  B ) P ( B ) = P ( B  |  A )
          P ( A )     P ( A &#x2229; B )    = P ( A ) P ( B )    if A and B are
          independent         {\displaystyle {\begin{aligned}P(A\cap B)&=P
 A and B  (A|B)P(B)=P(B|A)P(A)\\P(A\cap B)&=P(A)P(B)\qquad {\mbox{if A and B
          are independent}}\\\end{aligned}}}  [{\begin{aligned}P(A\cap B)&=P
          (A|B)P(B)=P(B|A)P(A)\\P(A\cap B)&=P(A)P(B)\qquad {\mbox{if A and B
          are independent}}\\\end{aligned}}]
             P ( A &#x2223; B ) =    P ( A &#x2229; B )   P ( B )    =    P ( B
A given B |  A ) P ( A )   P ( B )       {\displaystyle P(A\mid B)={\frac {P
          (A\cap B)}{P(B)}}={\frac {P(B|A)P(A)}{P(B)}}\,}  [P(A\mid B)={\frac
          {P(A\cap B)}{P(B)}}={\frac {P(B|A)P(A)}{P(B)}}\,]
***** Relation to randomness and probability in quantum mechanics[edit] *****
Main article: Randomness
See also: Quantum_fluctuation_Â§ Interpretations
[[icon]] This section needs expansion. You can help by adding_to_it. (April
         2017)
In a deterministic universe, based on Newtonian concepts, there would be no
probability if all conditions were known (Laplace's_demon), (but there are
situations in which sensitivity_to_initial_conditions exceeds our ability to
measure them, i.e. know them). In the case of a roulette wheel, if the force of
the hand and the period of that force are known, the number on which the ball
will stop would be a certainty (though as a practical matter, this would likely
be true only of a roulette wheel that had not been exactly levelled â as
Thomas A. Bass' Newtonian_Casino revealed). This also assumes knowledge of
inertia and friction of the wheel, weight, smoothness and roundness of the
ball, variations in hand speed during the turning and so forth. A probabilistic
description can thus be more useful than Newtonian mechanics for analyzing the
pattern of outcomes of repeated rolls of a roulette wheel. Physicists face the
same situation in kinetic_theory of gases, where the system, while
deterministic in principle, is so complex (with the number of molecules
typically the order of magnitude of the Avogadro_constant 6.02Ã1023) that only
a statistical description of its properties is feasible.
Probability_theory is required to describe quantum phenomena.[33] A
revolutionary discovery of early 20th century physics was the random character
of all physical processes that occur at sub-atomic scales and are governed by
the laws of quantum_mechanics. The objective wave_function evolves
deterministically but, according to the Copenhagen_interpretation, it deals
with probabilities of observing, the outcome being explained by a wave_function
collapse when an observation is made. However, the loss of determinism for the
sake of instrumentalism did not meet with universal approval. Albert_Einstein
famously remarked in a letter to Max_Born: "I am convinced that God does not
play dice".[34] Like Einstein, Erwin_SchrÃ¶dinger, who discovered the wave
function, believed quantum mechanics is a statistical approximation of an
underlying deterministic reality.[35] In some modern interpretations of the
statistical mechanics of measurement, quantum_decoherence is invoked to account
for the appearance of subjectively probabilistic experimental outcomes.
***** See also[edit] *****
    * [icon]Mathematics_portal
    * [icon]Logic_portal
Main article: Outline_of_probability
    * Chance_(disambiguation)
    * Class_membership_probabilities
    * Contingency
    * Equiprobability
    * Heuristics_in_judgment_and_decision-making
    * Probability_theory
    * Randomness
    * Statistics
    * Estimators
    * Estimation_Theory
    * Probability_density_function
  In Law
    * Balance_of_probabilities
***** Notes[edit] *****
   1. ^ Strictly speaking, a probability of 0 indicates that an event almost
      never takes place, whereas a probability of 1 indicates than an event
      almost_certainly takes place. This is an important distinction when the
      sample_space is infinite. For example, for the continuous_uniform
      distribution on the real interval [5, 10], there are an infinite number
      of possible outcomes, and the probability of any given outcome being
      observed â for instance, exactly 7 â is 0. This means that when we
      make an observation, it will almost surely not be exactly 7. However, it
      does not mean that exactly 7 is impossible. Ultimately some specific
      outcome (with probability 0) will be observed, and one possibility for
      that specific outcome is exactly 7.
***** References[edit] *****
   1. ^ "Probability". Webster's_Revised_Unabridged_Dictionary. G & C Merriam,
      1913.
   2. ^ "Kendall's Advanced Theory of Statistics, Volume 1: Distribution
      Theory", Alan Stuart and Keith Ord, 6th Ed, (2009),
   3. .mw-parser-output cite.citation{font-style:inherit}.mw-parser-output
      .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-
      free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/
      65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:
      right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-
      parser-output .citation .cs1-lock-registration a{background:url("//
      upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/
      9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em
      center}.mw-parser-output .citation .cs1-lock-subscription a{background:
      url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-
      2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right
      .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-
      registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-
      parser-output .cs1-registration span{border-bottom:1px dotted;cursor:
      help}.mw-parser-output .cs1-ws-icon a{background:url("//
      upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/
      12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em
      center}.mw-parser-output code.cs1-code{color:inherit;background:
      inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-
      error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error
      {font-size:100%}.mw-parser-output .cs1-maint{display:none;color:
      #33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-
      output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-
      parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-
      left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-
      wl-right{padding-right:0.2em}
   4. ISBN 978-0-534-24312-8.
   5. ^ William Feller, "An Introduction to Probability Theory and Its
      Applications", (Vol 1), 3rd Ed, (1968), Wiley,
   6. ISBN 0-471-25708-7.
   7. ^ Probability_Theory The Britannica website
   8. ^Hacking,_Ian (1965). The Logic of Statistical Inference. Cambridge
      University Press. ISBN 978-0-521-05165-1.
   9. [page needed]
  10. ^Finetti, Bruno de (1970). "Logical foundations and measurement of
      subjective probability". Acta Psychologica. 34: 129â145. doi:10.1016/
      0001-6918(70)90012-0.
  11. ^HÃ¡jek, Alan (21 October 2002). Edward N. Zalta (ed.). "Interpretations
      of_Probability". The Stanford Encyclopedia of Philosophy (Winter 2012
      ed.). Retrieved 22 April 2013.
  12. ^Hogg, Robert V.; Craig, Allen; McKean, Joseph W. (2004). Introduction to
      Mathematical Statistics (6th ed.). Upper Saddle River: Pearson. ISBN 978-
      0-13-008507-8.
  13. [page needed]
  14. ^Jaynes, E.T. (2003). "Section 5.3 Converging and diverging views". In
      Bretthorst, G. Larry (ed.). Probability Theory: The Logic of Science (1
      ed.). Cambridge University Press. ISBN 978-0-521-59271-0.
  15. ^ a b Hacking,_I. (2006) The Emergence of Probability: A Philosophical
      Study of Early Ideas about Probability, Induction and Statistical
      Inference, Cambridge University Press,
  16. ISBN 978-0-521-68557-3[page needed]
  17. ^ Freund,_John. (1973) Introduction to Probability. Dickenson
  18. ISBN 978-0-8221-0078-2 (p. 1)
  19. ^ Jeffrey, R.C., Probability and the Art of Judgment, Cambridge
      University Press. (1992). pp. 54â55 .
  20. ISBN 0-521-39459-7
  21. ^ Franklin, J. (2001) The Science of Conjecture: Evidence and Probability
      Before Pascal, Johns Hopkins University Press. (pp. 22, 113, 127)
  22. ^Broemeling, Lyle D. (1 November 2011). "An Account of Early Statistical
      Inference in Arab Cryptology". The American Statistician. 65 (4):
      255â257. doi:10.1198/tas.2011.10191.
  23. ^ Some_laws_and_problems_in_classical_probability_and_how_Cardano
      anticipated_them_Gorrochum,_P._Chance_magazine_2012
  24. ^Abrams, William, A_Brief_History_of_Probability, Second Moment,
      retrieved 23 May 2008
  25. ^Ivancevic, Vladimir G.; Ivancevic, Tijana T. (2008). Quantum leap : from
      Dirac and Feynman, across the universe, to human body and mind.
      Singapore ; Hackensack, NJ: World Scientific. p. 16. ISBN 978-981-281-
      927-7.
  26. ^Franklin, James (2001). The Science of Conjecture: Evidence and
      Probability Before Pascal. Johns Hopkins University Press. ISBN 978-0-
      8018-6569-5.
  27. ^Shoesmith, Eddie (November 1985). "Thomas Simpson and the arithmetic
      mean". Historia Mathematica. 12 (4): 352â355. doi:10.1016/0315-0860
      (85)90044-8.
  28. ^ a b Wilson EB (1923) "First and second laws of error". Journal_of_the
      American_Statistical_Association, 18, 143
  29. ^Seneta, Eugene William. ""Adrien-Marie_Legendre"_(version_9)". StatProb:
      The Encyclopedia Sponsored by Statistics and Probability Societies.
      Archived from the_original on 3 February 2016. Retrieved 27 January 2016.
  30. ^Weber, Richard. "Markov_Chains" (PDF). Statistical Laboratory.
      University of Cambridge.
  31. ^Vitanyi, Paul M.B. (1988). "Andrei_Nikolaevich_Kolmogorov". CWI
      Quarterly (1): 3â18. Retrieved 27 January 2016.
  32. ^ Singh, Laurie (2010) "Whither Efficient Markets? Efficient Market
      Theory and Behavioral Finance". The Finance Professionals' Post, 2010.
  33. ^Gao, J.Z.; Fong, D.; Liu, X. (April 2011). "Mathematical analyses of
      casino rebate systems for VIP gambling". International Gambling Studies.
      11 (1): 93â106. doi:10.1080/14459795.2011.552575.
  34. ^"Data:_Data_Analysis,_Probability_and_Statistics,_and_Graphing".
      archon.educ.kent.edu. Retrieved 28 May 2017.
  35. ^Gorman, Michael F. (2010). "Management Insights". Management Science.
      56: ivâvii. doi:10.1287/mnsc.1090.1132.
  36. ^Ross, Sheldon M. (2010). A First course in Probability (8th ed.).
      Pearson Prentice Hall. pp. 26â27. ISBN 9780136033134.
  37. ^ Olofsson (2005) p. 8.
  38. ^ Olofsson (2005), p. 9
  39. ^ Olofsson (2005) p. 35.
  40. ^ Olofsson (2005) p. 29.
  41. ^Burgin, Mark (2010). "Interpretations of Negative Probabilities": 1.
      arXiv:1008.1287v1.
  42. ^ Jedenfalls bin ich Ã¼berzeugt, daÃ der Alte nicht wÃ¼rfelt. Letter to
      Max Born, 4 December 1926, in: Einstein/Born_Briefwechsel_1916â1955.
  43. ^Moore, W.J. (1992). SchrÃ¶dinger: Life and Thought. Cambridge_University
      Press. p. 479. ISBN 978-0-521-43767-7.
***** Bibliography[edit] *****
    * Kallenberg,_O. (2005) Probabilistic Symmetries and Invariance Principles.
      Springer-Verlag, New York. 510 pp. 
ISBN 0-387-25115-4
Kallenberg, O. (2002) Foundations of Modern Probability, 2nd ed. Springer
Series in Statistics. 650 pp. 
ISBN 0-387-95313-2
Olofsson, Peter (2005) Probability, Statistics, and Stochastic Processes,
Wiley-Interscience. 504 pp
ISBN 0-471-67969-0.
***** External links[edit] *****
 Wikiquote has quotations related to: Probability
 Wikibooks has more on the topic of: Probability
 Wikimedia Commons has media related to Probability.
Library_resources about
Probability
===============================================================================
    * Resources_in_your_library
    * Virtual_Laboratories_in_Probability_and_Statistics_(Univ._of_Ala.-
      Huntsville)
    * Probability on In_Our_Time at the BBC
    * Probability_and_Statistics_EBook
    * Edwin_Thompson_Jaynes. Probability Theory: The Logic of Science.
      Preprint: Washington University, (1996). â HTML_index_with_links_to
      PostScript_files and PDF (first three chapters)
    * People_from_the_History_of_Probability_and_Statistics_(Univ._of
      Southampton)
    * Probability_and_Statistics_on_the_Earliest_Uses_Pages_(Univ._of
      Southampton)
    * Earliest_Uses_of_Symbols_in_Probability_and_Statistics on Earliest_Uses
      of_Various_Mathematical_Symbols
    * A_tutorial_on_probability_and_Bayes'_theorem_devised_for_first-year
      Oxford_University_students
    * [1] pdf file of An_Anthology_of_Chance_Operations (1963) at UbuWeb
    * Introduction_to_Probability_â_eBook, by Charles Grinstead, Laurie Snell
      Source (GNU_Free_Documentation_License)
    * (in English) (in Italian) Bruno_de_Finetti, ProbabilitÃ _e_induzione,
      Bologna, CLUEB, 1993.
ISBN 88-8091-176-7 (digital version)
Richard_P._Feynman's_Lecture_on_probability.
    * v
    * t
    * e
Logic
    * Outline
    * History
                * Computer_science
                * Inference
                * Philosophy_of_logic
                * Proof
                * Semantics
                * Syntax
                         * Classical
                         * Informal
Fields                         o Critical_thinking
            Logics             o Reason
                         * Mathematical
                         * Non-classical
                         * Philosophical
                         * Argumentation
            Theories     * Metalogic
                         * Metamathematics
                         * Set
                * Abduction
                * Analytic_and_synthetic_propositions
                * Contradiction
                      o Paradox
                      o Antinomy
                * Deduction
                * Deductive_closure
                * Definition
                * Description
                * Entailment
Foundations           o Linguistic
                * Form
                * Induction
                * Logical_truth
                * Name
                * Necessity_and_sufficiency
                * Probability
                * Reference
                * Statement
                * Substitution
                * Truth
                * Validity
                       * Mathematical_logic
            topics     * Boolean_algebra
                       * Set_theory
Lists                  * Logicians
                       * Rules_of_inference
            other      * Paradoxes
                       * Fallacies
                       * Logic_symbols
    * Portal
    * Category
    * WikiProject (talk)
    * changes
    * v
    * t
    * e
Glossaries of science and engineering
    * Aerospace_engineering
    * Archaeology
    * Architecture
    * Artificial_intelligence
    * Astronomy
    * Biology
    * Botany
    * Calculus
    * Chemistry
    * Civil_engineering
    * Clinical_research
    * Computer_science
    * Ecology
    * Economics
    * Electrical_and_electronics_engineering
    * Engineering
    * Entomology
    * Environmental_science
    * Genetics
    * Geography
    * Geology
    * Machine_vision
    * Mathematics
    * Mechanical_engineering
    * Medicine
    * Meteorology
    * Physics
    * Probability_and_statistics
    * Psychiatry
    * Robotics
    * Speciation
    * Structural_engineering
    * [Fisher_iris_versicolor_sepalwidth.svg]Statistics_portal
    * [Nuvola_apps_edu_mathematics_blue-p.svg]Mathematics_portal
Authority_control [Edit_this_at_Wikidata]     * GND: 4137007-7
                                              * LCCN: sh85107090

Retrieved from "https://en.wikipedia.org/w/
index.php?title=Probability&oldid=909265286"
Categories:
    * Probability
    * Dimensionless_numbers
Hidden categories:
    * Wikipedia_articles_needing_page_number_citations_from_June_2012
    * Use_dmy_dates_from_October_2013
    * Wikipedia_articles_needing_clarification_from_July_2014
    * All_articles_with_unsourced_statements
    * Articles_with_unsourced_statements_from_June_2012
    * Wikipedia_articles_needing_clarification_from_June_2012
    * Articles_with_unsourced_statements_from_February_2012
    * Articles_with_unsourced_statements_from_October_2015
    * Articles_with_unsourced_statements_from_July_2012
    * Articles_to_be_expanded_from_April_2017
    * All_articles_to_be_expanded
    * Articles_using_small_message_boxes
    * Commons_category_link_from_Wikidata
    * Articles_with_Italian-language_external_links
    * Wikipedia_articles_with_GND_identifiers
    * Wikipedia_articles_with_LCCN_identifiers
***** Navigation menu *****
**** Personal tools ****
    * Not logged in
    * Talk
    * Contributions
    * Create_account
    * Log_in
**** Namespaces ****
    * Article
    * Talk
⁰
**** Variants ****
**** Views ****
    * Read
    * Edit
    * View_history
⁰
**** More ****
**** Search ****
[Unknown INPUT type][Search][Go]
**** Navigation ****
    * Main_page
    * Contents
    * Featured_content
    * Current_events
    * Random_article
    * Donate_to_Wikipedia
    * Wikipedia_store
**** Interaction ****
    * Help
    * About_Wikipedia
    * Community_portal
    * Recent_changes
    * Contact_page
**** Tools ****
    * What_links_here
    * Related_changes
    * Upload_file
    * Special_pages
    * Permanent_link
    * Page_information
    * Wikidata_item
    * Cite_this_page
**** In other projects ****
    * Wikimedia_Commons
    * Wikibooks
    * Wikiquote
**** Print/export ****
    * Create_a_book
    * Download_as_PDF
    * Printable_version
**** Languages ****
    * Afrikaans
    * Alemannisch
    * á áá­á
    * Ø§ÙØ¹Ø±Ø¨ÙØ©
    * AragonÃ©s
    * Asturianu
    * Aymar_aru
    * AzÉrbaycanca
    * ØªÛØ±Ú©Ø¬Ù
    * à¦¬à¦¾à¦à¦²à¦¾
    * BÃ¢n-lÃ¢m-gÃº
    * ÐÐ°ÑÒ¡Ð¾ÑÑÑÐ°
    * ÐÐµÐ»Ð°ÑÑÑÐºÐ°Ñ
    * ÐÐµÐ»Ð°ÑÑÑÐºÐ°Ñ_(ÑÐ°ÑÐ°ÑÐºÐµÐ²ÑÑÐ°)â
    * ÐÑÐ»Ð³Ð°ÑÑÐºÐ¸
    * Boarisch
    * Bosanski
    * ÐÑÑÑÐ°Ð´
    * CatalÃ 
    * Ð§ÓÐ²Ð°ÑÐ»Ð°
    * ÄeÅ¡tina
    * Cymraeg
    * Dansk
    * Deutsch
    * Eesti
    * ÎÎ»Î»Î·Î½Î¹ÎºÎ¬
    * EspaÃ±ol
    * Esperanto
    * Euskara
    * ÙØ§Ø±Ø³Û
    * FranÃ§ais
    * Gaeilge
    * Galego
    * è´èª
    * íêµ­ì´
    * ÕÕ¡ÕµÕ¥ÖÕ¥Õ¶
    * à¤¹à¤¿à¤¨à¥à¤¦à¥
    * Hrvatski
    * Ido
    * Ilokano
    * Bahasa_Indonesia
    * Interlingua
    * Ãslenska
    * Italiano
    * ×¢××¨××ª
    * KabÉ©yÉ
    * à²à²¨à³à²¨à²¡
    * á¥áá áá£áá
    * ÒÐ°Ð·Ð°ÒÑÐ°
    * Kiswahili
    * Latina
    * LatvieÅ¡u
    * LÃ«tzebuergesch
    * LietuviÅ³
    * ÐÐ°ÐºÐµÐ´Ð¾Ð½ÑÐºÐ¸
    * à´®à´²à´¯à´¾à´³à´
    * Malti
    * Bahasa_Melayu
    * áá¼ááºáá¬áá¬áá¬
    * Nederlands
    * æ¥æ¬èª
    * ÐÐ¾ÑÑÐ¸Ð¹Ð½
    * Nordfriisk
    * Norsk
    * Occitan
    * OÊ»zbekcha/ÑÐ·Ð±ÐµÐºÑÐ°
    * à¨ªà©°à¨à¨¾à¨¬à©
    * Ù¾ÙØ¬Ø§Ø¨Û
    * Patois
    * Picard
    * PiemontÃ¨is
    * Polski
    * PortuguÃªs
    * RomÃ¢nÄ
    * Ð ÑÑÑÐºÐ¸Ð¹
    * Scots
    * Shqip
    * Sicilianu
    * Simple_English
    * SlovenÄina
    * SlovenÅ¡Äina
    * Soomaaliga
    * Ú©ÙØ±Ø¯Û
    * Ð¡ÑÐ¿ÑÐºÐ¸_/_srpski
    * Srpskohrvatski_/_ÑÑÐ¿ÑÐºÐ¾ÑÑÐ²Ð°ÑÑÐºÐ¸
    * Basa_Sunda
    * Suomi
    * Svenska
    * Tagalog
    * à®¤à®®à®¿à®´à¯
    * Ð¢Ð°ÑÐ°ÑÑÐ°/tatarÃ§a
    * à¹à¸à¸¢
    * TÃ¼rkÃ§e
    * Ð£ÐºÑÐ°ÑÐ½ÑÑÐºÐ°
    * Ø§Ø±Ø¯Ù
    * VÃ¨neto
    * Tiáº¿ng_Viá»t
    * Winaray
    * å´è¯­
    * ××Ö´×××©
    * ç²µèª
    * ä¸­æ
Edit_links
    * This page was last edited on 4 August 2019, at 08:22 (UTC).
    * Text is available under the Creative_Commons_Attribution-ShareAlike
      License; additional terms may apply. By using this site, you agree to the
      Terms_of_Use and Privacy_Policy. WikipediaÂ® is a registered trademark of
      the Wikimedia_Foundation,_Inc., a non-profit organization.
    * Privacy_policy
    * About_Wikipedia
    * Disclaimers
    * Contact_Wikipedia
    * Developers
    * Cookie_statement
    * Mobile_view
    * [Wikimedia_Foundation]
    * [Powered_by_MediaWiki]
